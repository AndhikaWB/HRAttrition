{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV to database\n",
    "import sqlalchemy as sqa\n",
    "\n",
    "# Handle imbalanced data\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Reduce data dimension for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Remove less important features (Pearson correlation)\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from sklearn.preprocessing import TargetEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Mainly intended for dashboard, ML preparation is on separate section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>884</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2061</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>39</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>613</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2062</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>155</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2064</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1023</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2065</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>34</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>628</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2068</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "1465   36        No  Travel_Frequently        884  Research & Development   \n",
       "1466   39        No      Travel_Rarely        613  Research & Development   \n",
       "1467   27        No      Travel_Rarely        155  Research & Development   \n",
       "1468   49        No  Travel_Frequently       1023                   Sales   \n",
       "1469   34        No      Travel_Rarely        628  Research & Development   \n",
       "\n",
       "      DistanceFromHome  Education EducationField  EmployeeCount  \\\n",
       "1465                23          2        Medical              1   \n",
       "1466                 6          1        Medical              1   \n",
       "1467                 4          3  Life Sciences              1   \n",
       "1468                 2          3        Medical              1   \n",
       "1469                 8          3        Medical              1   \n",
       "\n",
       "      EmployeeNumber  ...  RelationshipSatisfaction StandardHours  \\\n",
       "1465            2061  ...                         3            80   \n",
       "1466            2062  ...                         1            80   \n",
       "1467            2064  ...                         2            80   \n",
       "1468            2065  ...                         4            80   \n",
       "1469            2068  ...                         1            80   \n",
       "\n",
       "      StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear  \\\n",
       "1465                 1                 17                      3   \n",
       "1466                 1                  9                      5   \n",
       "1467                 1                  6                      0   \n",
       "1468                 0                 17                      3   \n",
       "1469                 0                  6                      3   \n",
       "\n",
       "     WorkLifeBalance  YearsAtCompany YearsInCurrentRole  \\\n",
       "1465               3               5                  2   \n",
       "1466               3               7                  7   \n",
       "1467               3               6                  2   \n",
       "1468               2               9                  6   \n",
       "1469               4               4                  3   \n",
       "\n",
       "      YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "1465                        0                     3  \n",
       "1466                        1                     7  \n",
       "1467                        0                     3  \n",
       "1468                        0                     8  \n",
       "1469                        1                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('data/employee_data.csv')\n",
    "df = pd.read_csv('data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check Missing Data\n",
    "The original `employee_data.csv` from Dicoding has many missing attrition values, which is the most important one (for checking prediction accuracy, etc)\n",
    "\n",
    "On real world scenario, it's impossible to have missing attrition values since employees that no longer work should be noticed by HR\n",
    "\n",
    "Possible solutions:\n",
    "- Solution A: use algorithm like KNN or PCA or TSNE to fill missing values (may needs data scaling)\n",
    "- **Solution B (choosen)**: use alternative data from [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/) (no missing attrition values but need to re-add employee IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "Attrition                   0\n",
       "BusinessTravel              0\n",
       "DailyRate                   0\n",
       "Department                  0\n",
       "DistanceFromHome            0\n",
       "Education                   0\n",
       "EducationField              0\n",
       "EmployeeCount               0\n",
       "EmployeeNumber              0\n",
       "EnvironmentSatisfaction     0\n",
       "Gender                      0\n",
       "HourlyRate                  0\n",
       "JobInvolvement              0\n",
       "JobLevel                    0\n",
       "JobRole                     0\n",
       "JobSatisfaction             0\n",
       "MaritalStatus               0\n",
       "MonthlyIncome               0\n",
       "MonthlyRate                 0\n",
       "NumCompaniesWorked          0\n",
       "Over18                      0\n",
       "OverTime                    0\n",
       "PercentSalaryHike           0\n",
       "PerformanceRating           0\n",
       "RelationshipSatisfaction    0\n",
       "StandardHours               0\n",
       "StockOptionLevel            0\n",
       "TotalWorkingYears           0\n",
       "TrainingTimesLastYear       0\n",
       "WorkLifeBalance             0\n",
       "YearsAtCompany              0\n",
       "YearsInCurrentRole          0\n",
       "YearsSinceLastPromotion     0\n",
       "YearsWithCurrManager        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-add missing employee ID column (by joining both dataset)\n",
    "\n",
    "Will be joined based on hash since there is no similar primary key on both tables (one has `EmployeeId`, the other has `EmployeeNumber` which is different)\n",
    "\n",
    "**Note:** The original rows with missing attrition value (from Dicoding) will be marked as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to exclude from hashing (for table join operation)\n",
    "# Assuming both dataframes have similar column names\n",
    "skip_col = ['Attrition', 'EmployeeId', 'EmployeeNumber']\n",
    "hash_col = [i for i in df.columns if i not in skip_col]\n",
    "\n",
    "temp = pd.read_csv('data/employee_data.csv')\n",
    "\n",
    "# Make sure to exclude index from hashing too\n",
    "# Otherwise, both tables will have different hashes\n",
    "temp['hash'] = pd.util.hash_pandas_object(temp[hash_col], index = False)\n",
    "df['hash'] = pd.util.hash_pandas_object(df[hash_col], index = False)\n",
    "\n",
    "temp = temp.sort_values('hash').reset_index(drop = True)\n",
    "df = df.sort_values('hash').reset_index(drop = True)\n",
    "\n",
    "# Will give output if there are still differences\n",
    "df[hash_col].compare(temp[hash_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1466</td>\n",
       "      <td>38</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>168</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1467</td>\n",
       "      <td>50</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>813</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1468</td>\n",
       "      <td>28</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1485</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1469</td>\n",
       "      <td>40</td>\n",
       "      <td>No</td>\n",
       "      <td>Non-Travel</td>\n",
       "      <td>458</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1470</td>\n",
       "      <td>19</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>602</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Technical Degree</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  Age Attrition     BusinessTravel  DailyRate  \\\n",
       "1465        1466   38        No      Travel_Rarely        168   \n",
       "1466        1467   50        No      Travel_Rarely        813   \n",
       "1467        1468   28       Yes      Travel_Rarely       1485   \n",
       "1468        1469   40        No         Non-Travel        458   \n",
       "1469        1470   19       Yes  Travel_Frequently        602   \n",
       "\n",
       "                  Department  DistanceFromHome  Education    EducationField  \\\n",
       "1465  Research & Development                 1          3     Life Sciences   \n",
       "1466  Research & Development                17          5     Life Sciences   \n",
       "1467  Research & Development                12          1     Life Sciences   \n",
       "1468  Research & Development                16          2     Life Sciences   \n",
       "1469                   Sales                 1          1  Technical Degree   \n",
       "\n",
       "      EmployeeCount  ...  StandardHours StockOptionLevel  TotalWorkingYears  \\\n",
       "1465              1  ...             80                0                 10   \n",
       "1466              1  ...             80                3                 19   \n",
       "1467              1  ...             80                0                  1   \n",
       "1468              1  ...             80                1                  6   \n",
       "1469              1  ...             80                0                  1   \n",
       "\n",
       "      TrainingTimesLastYear  WorkLifeBalance YearsAtCompany  \\\n",
       "1465                      4                4              1   \n",
       "1466                      3                3             14   \n",
       "1467                      4                2              1   \n",
       "1468                      0                3              4   \n",
       "1469                      5                4              0   \n",
       "\n",
       "      YearsInCurrentRole YearsSinceLastPromotion  YearsWithCurrManager  IsTest  \n",
       "1465                   0                       0                     0       0  \n",
       "1466                  11                       1                    11       1  \n",
       "1467                   1                       0                     0       0  \n",
       "1468                   2                       0                     0       0  \n",
       "1469                   0                       0                     0       0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, temp[['hash', 'EmployeeId']], on = 'hash')\n",
    "df = df.drop(['hash', 'EmployeeNumber'], axis = 1)\n",
    "\n",
    "# Mark rows with no labels (from Dicoding) as test data\n",
    "df['IsTest'] = temp['Attrition'].isnull().astype('int')\n",
    "\n",
    "# Reorder employee id column as first column\n",
    "temp = [i for i in df.columns if i != 'EmployeeId']\n",
    "df = df[['EmployeeId'] + temp]\n",
    "\n",
    "df = df.sort_values('EmployeeId').reset_index(drop = True)\n",
    "# Export back fixed dataframe (no missing attrition value)\n",
    "df.to_csv('data/employee_data_fixed.csv', index = False)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Check/Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmployeeId                   int64\n",
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EmployeeCount                int64\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "Over18                      object\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StandardHours                int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "IsTest                       int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition\n",
      "No     1233\n",
      "Yes     237\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "IsTest\n",
      "0    1058\n",
      "1     412\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "BusinessTravel\n",
      "Travel_Rarely        1043\n",
      "Travel_Frequently     277\n",
      "Non-Travel            150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check possible values for specific column\n",
    "# Add other columns if needed\n",
    "print(df['Attrition'].value_counts(dropna = False), '\\n')\n",
    "print(df['IsTest'].value_counts(dropna = False), '\\n')\n",
    "print(df['BusinessTravel'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace boolean and ordinal values with numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhika\\AppData\\Local\\Temp\\VSCodePortableTemp\\ipykernel_7460\\763175519.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId  Age  Attrition  BusinessTravel  DailyRate  \\\n",
       "0           1   38          0               2       1444   \n",
       "1           2   37          1               1       1141   \n",
       "2           3   51          1               1       1323   \n",
       "3           4   42          0               2        555   \n",
       "4           5   40          0               1       1194   \n",
       "\n",
       "               Department  DistanceFromHome  Education EducationField  \\\n",
       "0         Human Resources                 1          4          Other   \n",
       "1  Research & Development                11          2        Medical   \n",
       "2  Research & Development                 4          4  Life Sciences   \n",
       "3                   Sales                26          3      Marketing   \n",
       "4  Research & Development                 2          4        Medical   \n",
       "\n",
       "   EmployeeCount  ...  StandardHours StockOptionLevel  TotalWorkingYears  \\\n",
       "0              1  ...             80                1                  7   \n",
       "1              1  ...             80                0                 15   \n",
       "2              1  ...             80                3                 18   \n",
       "3              1  ...             80                1                 23   \n",
       "4              1  ...             80                3                 20   \n",
       "\n",
       "   TrainingTimesLastYear  WorkLifeBalance YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                      2                3              6                   2   \n",
       "1                      2                1              1                   0   \n",
       "2                      2                4             10                   0   \n",
       "3                      2                4             20                   4   \n",
       "4                      2                3              5                   3   \n",
       "\n",
       "  YearsSinceLastPromotion  YearsWithCurrManager  IsTest  \n",
       "0                       1                     2       1  \n",
       "1                       0                     0       0  \n",
       "2                       2                     7       0  \n",
       "3                       4                     8       0  \n",
       "4                       0                     2       1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace(\n",
    "    {\n",
    "        'Attrition': {\n",
    "            'Yes': 1,\n",
    "            'No': 0\n",
    "        },\n",
    "        'BusinessTravel': {\n",
    "            'Non-Travel': 0,\n",
    "            'Travel_Rarely': 1,\n",
    "            'Travel_Frequently': 2\n",
    "        },\n",
    "        'Over18': {\n",
    "            'Y': 1,\n",
    "            'N': 0\n",
    "        },\n",
    "        'OverTime': {\n",
    "            'Yes': 1,\n",
    "            'No': 0\n",
    "        }\n",
    "    },\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical values to numeric values (e.g. 0 = single, 1 = married, 2 = divorced)\n",
    "\n",
    "Normally, label encoding is enough for dashboard, but we may use other encoder for ML model later (see comparison [here](https://www.datacamp.com/tutorial/categorical-data) and [there](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder.html))\n",
    "\n",
    "**Alternative:** use \"pd.get_dummies\" (one hot encoder) or Sklearn \"TargetEncoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DepartmentN</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId  Age  Attrition  BusinessTravel  DailyRate  \\\n",
       "0           1   38          0               2       1444   \n",
       "1           2   37          1               1       1141   \n",
       "2           3   51          1               1       1323   \n",
       "3           4   42          0               2        555   \n",
       "4           5   40          0               1       1194   \n",
       "\n",
       "               Department  DepartmentN  DistanceFromHome  Education  \\\n",
       "0         Human Resources            0                 1          4   \n",
       "1  Research & Development            1                11          2   \n",
       "2  Research & Development            1                 4          4   \n",
       "3                   Sales            2                26          3   \n",
       "4  Research & Development            1                 2          4   \n",
       "\n",
       "  EducationField  ...  StandardHours  StockOptionLevel  TotalWorkingYears  \\\n",
       "0          Other  ...             80                 1                  7   \n",
       "1        Medical  ...             80                 0                 15   \n",
       "2  Life Sciences  ...             80                 3                 18   \n",
       "3      Marketing  ...             80                 1                 23   \n",
       "4        Medical  ...             80                 3                 20   \n",
       "\n",
       "  TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                     2                3               6                   2   \n",
       "1                     2                1               1                   0   \n",
       "2                     2                4              10                   0   \n",
       "3                     2                4              20                   4   \n",
       "4                     2                3               5                   3   \n",
       "\n",
       "   YearsSinceLastPromotion YearsWithCurrManager  IsTest  \n",
       "0                        1                    2       1  \n",
       "1                        0                    0       0  \n",
       "2                        2                    7       0  \n",
       "3                        4                    8       0  \n",
       "4                        0                    2       1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical columns to convert\n",
    "cat_col = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
    "\n",
    "for col in cat_col:\n",
    "    # Add new numeric column (column name + N) for each categorical column\n",
    "    df[col + 'N'] = pd.factorize(df[col])[0]\n",
    "\n",
    "# Sort column names to fix newly added columns\n",
    "df = df.reindex(sorted(df.columns), axis = 1)\n",
    "\n",
    "# Place employee id at beginning and is test at the end\n",
    "temp = [i for i in df.columns if i not in ['EmployeeId', 'IsTest']]\n",
    "df = df[['EmployeeId'] + temp + ['IsTest']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck data types\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Remove Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeId is useless (unique values equal row length)\n",
      "EmployeeCount is useless (only 1 possible value)\n",
      "Over18 is useless (only 1 possible value)\n",
      "StandardHours is useless (only 1 possible value)\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].nunique() == 1:\n",
    "        print(i, 'is useless (only 1 possible value)')\n",
    "    if df[i].nunique() == len(df):\n",
    "        print(i, 'is useless (unique values equal row length)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EmployeeId` is useless for dashboard, but still can be used for machine learning prediction (hence not removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis = 1, inplace = True)\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Supabase project first or local PostgreSQL server (for Metabase), and fill required URL in `database.txt` to export the CSV table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to Supabase/local SQL server\n",
    "export_database = False\n",
    "\n",
    "with open('database.txt') as f:\n",
    "    database_url = f.readline()\n",
    "    # print(database_url)\n",
    "    engine = sqa.create_engine(url = database_url)\n",
    "\n",
    "if export_database:\n",
    "    df.to_sql('employee', engine, index = False, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix is very useful before analyzing too deep into the data, and can be used as initial hypothesis\n",
    "\n",
    "It will give a quick insight about the data relationship (bi-directional only), especially those that caused attrition\n",
    "\n",
    "**Note 1:** Correlation does not imply causation, and it works both ways (1 and -1 both means strong correlation, but opposite relationship)\n",
    "\n",
    "**Note 2:** Categorical columns should be ignored (non-standard sort order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_eb37e_row0_col0 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row1_col0 {\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_eb37e_row2_col0 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_eb37e_row3_col0 {\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row4_col0 {\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row5_col0 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row6_col0 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row7_col0 {\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row8_col0 {\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row9_col0 {\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row10_col0 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row11_col0 {\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row12_col0 {\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row13_col0 {\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row14_col0, #T_eb37e_row15_col0, #T_eb37e_row16_col0 {\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row17_col0 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row18_col0 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row19_col0 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row20_col0 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row21_col0 {\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row22_col0, #T_eb37e_row23_col0 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row24_col0, #T_eb37e_row25_col0 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row26_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row27_col0 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row28_col0, #T_eb37e_row29_col0, #T_eb37e_row30_col0 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_eb37e_row31_col0, #T_eb37e_row32_col0 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_eb37e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_eb37e_level0_col0\" class=\"col_heading level0 col0\" >Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row0\" class=\"row_heading level0 row0\" >Attrition</th>\n",
       "      <td id=\"T_eb37e_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row1\" class=\"row_heading level0 row1\" >OverTime</th>\n",
       "      <td id=\"T_eb37e_row1_col0\" class=\"data row1 col0\" >0.246118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row2\" class=\"row_heading level0 row2\" >BusinessTravel</th>\n",
       "      <td id=\"T_eb37e_row2_col0\" class=\"data row2 col0\" >0.127006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row3\" class=\"row_heading level0 row3\" >EducationFieldN</th>\n",
       "      <td id=\"T_eb37e_row3_col0\" class=\"data row3 col0\" >0.094277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row4\" class=\"row_heading level0 row4\" >DistanceFromHome</th>\n",
       "      <td id=\"T_eb37e_row4_col0\" class=\"data row4 col0\" >0.077924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row5\" class=\"row_heading level0 row5\" >DepartmentN</th>\n",
       "      <td id=\"T_eb37e_row5_col0\" class=\"data row5 col0\" >0.063991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row6\" class=\"row_heading level0 row6\" >JobRoleN</th>\n",
       "      <td id=\"T_eb37e_row6_col0\" class=\"data row6 col0\" >0.047330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row7\" class=\"row_heading level0 row7\" >NumCompaniesWorked</th>\n",
       "      <td id=\"T_eb37e_row7_col0\" class=\"data row7 col0\" >0.043494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row8\" class=\"row_heading level0 row8\" >MonthlyRate</th>\n",
       "      <td id=\"T_eb37e_row8_col0\" class=\"data row8 col0\" >0.015170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row9\" class=\"row_heading level0 row9\" >MaritalStatusN</th>\n",
       "      <td id=\"T_eb37e_row9_col0\" class=\"data row9 col0\" >0.011195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row10\" class=\"row_heading level0 row10\" >PerformanceRating</th>\n",
       "      <td id=\"T_eb37e_row10_col0\" class=\"data row10 col0\" >0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row11\" class=\"row_heading level0 row11\" >EmployeeId</th>\n",
       "      <td id=\"T_eb37e_row11_col0\" class=\"data row11 col0\" >-0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row12\" class=\"row_heading level0 row12\" >HourlyRate</th>\n",
       "      <td id=\"T_eb37e_row12_col0\" class=\"data row12 col0\" >-0.006846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row13\" class=\"row_heading level0 row13\" >PercentSalaryHike</th>\n",
       "      <td id=\"T_eb37e_row13_col0\" class=\"data row13 col0\" >-0.013478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row14\" class=\"row_heading level0 row14\" >GenderN</th>\n",
       "      <td id=\"T_eb37e_row14_col0\" class=\"data row14 col0\" >-0.029453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row15\" class=\"row_heading level0 row15\" >Education</th>\n",
       "      <td id=\"T_eb37e_row15_col0\" class=\"data row15 col0\" >-0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row16\" class=\"row_heading level0 row16\" >YearsSinceLastPromotion</th>\n",
       "      <td id=\"T_eb37e_row16_col0\" class=\"data row16 col0\" >-0.033019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row17\" class=\"row_heading level0 row17\" >IsTest</th>\n",
       "      <td id=\"T_eb37e_row17_col0\" class=\"data row17 col0\" >-0.034699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row18\" class=\"row_heading level0 row18\" >RelationshipSatisfaction</th>\n",
       "      <td id=\"T_eb37e_row18_col0\" class=\"data row18 col0\" >-0.045872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row19\" class=\"row_heading level0 row19\" >DailyRate</th>\n",
       "      <td id=\"T_eb37e_row19_col0\" class=\"data row19 col0\" >-0.056652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row20\" class=\"row_heading level0 row20\" >TrainingTimesLastYear</th>\n",
       "      <td id=\"T_eb37e_row20_col0\" class=\"data row20 col0\" >-0.059478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row21\" class=\"row_heading level0 row21\" >WorkLifeBalance</th>\n",
       "      <td id=\"T_eb37e_row21_col0\" class=\"data row21 col0\" >-0.063939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row22\" class=\"row_heading level0 row22\" >EnvironmentSatisfaction</th>\n",
       "      <td id=\"T_eb37e_row22_col0\" class=\"data row22 col0\" >-0.103369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row23\" class=\"row_heading level0 row23\" >JobSatisfaction</th>\n",
       "      <td id=\"T_eb37e_row23_col0\" class=\"data row23 col0\" >-0.103481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row24\" class=\"row_heading level0 row24\" >JobInvolvement</th>\n",
       "      <td id=\"T_eb37e_row24_col0\" class=\"data row24 col0\" >-0.130016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row25\" class=\"row_heading level0 row25\" >YearsAtCompany</th>\n",
       "      <td id=\"T_eb37e_row25_col0\" class=\"data row25 col0\" >-0.134392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row26\" class=\"row_heading level0 row26\" >StockOptionLevel</th>\n",
       "      <td id=\"T_eb37e_row26_col0\" class=\"data row26 col0\" >-0.137145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row27\" class=\"row_heading level0 row27\" >YearsWithCurrManager</th>\n",
       "      <td id=\"T_eb37e_row27_col0\" class=\"data row27 col0\" >-0.156199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row28\" class=\"row_heading level0 row28\" >Age</th>\n",
       "      <td id=\"T_eb37e_row28_col0\" class=\"data row28 col0\" >-0.159205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row29\" class=\"row_heading level0 row29\" >MonthlyIncome</th>\n",
       "      <td id=\"T_eb37e_row29_col0\" class=\"data row29 col0\" >-0.159840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row30\" class=\"row_heading level0 row30\" >YearsInCurrentRole</th>\n",
       "      <td id=\"T_eb37e_row30_col0\" class=\"data row30 col0\" >-0.160545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row31\" class=\"row_heading level0 row31\" >JobLevel</th>\n",
       "      <td id=\"T_eb37e_row31_col0\" class=\"data row31 col0\" >-0.169105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_eb37e_level0_row32\" class=\"row_heading level0 row32\" >TotalWorkingYears</th>\n",
       "      <td id=\"T_eb37e_row32_col0\" class=\"data row32 col0\" >-0.171063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2672b446610>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.corr(numeric_only = True)\n",
    "\n",
    "# Focus correlation matrix on certain column\n",
    "col = 'Attrition'\n",
    "\n",
    "temp = temp[[col]]\n",
    "temp = temp.sort_values(by = col, ascending = False)\n",
    "\n",
    "# https://stackoverflow.com/questions/29432629\n",
    "temp.style.background_gradient(cmap = 'coolwarm', axis = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also describe the min, max, mean, etc from each column\n",
    "\n",
    "May be needed for `width_bucket` (e.g. age, salary) or other special SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EmployeeId</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>735.500000</td>\n",
       "      <td>424.496761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>368.25</td>\n",
       "      <td>735.5</td>\n",
       "      <td>1102.75</td>\n",
       "      <td>1470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>36.923810</td>\n",
       "      <td>9.135373</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43.00</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attrition</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.161224</td>\n",
       "      <td>0.367863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessTravel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.086395</td>\n",
       "      <td>0.532170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DailyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>802.485714</td>\n",
       "      <td>403.509100</td>\n",
       "      <td>102.0</td>\n",
       "      <td>465.00</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1157.00</td>\n",
       "      <td>1499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepartmentN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.260544</td>\n",
       "      <td>0.527792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>9.192517</td>\n",
       "      <td>8.106864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.912925</td>\n",
       "      <td>1.024165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EducationFieldN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.915646</td>\n",
       "      <td>1.079401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.721769</td>\n",
       "      <td>1.093082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenderN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.490065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HourlyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>65.891156</td>\n",
       "      <td>20.329428</td>\n",
       "      <td>30.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>83.75</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobInvolvement</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.729932</td>\n",
       "      <td>0.711561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobLevel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.063946</td>\n",
       "      <td>1.106940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobRoleN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>3.778912</td>\n",
       "      <td>2.126872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.728571</td>\n",
       "      <td>1.102846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalStatusN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.764626</td>\n",
       "      <td>0.790757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>6502.931293</td>\n",
       "      <td>4707.956783</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2911.00</td>\n",
       "      <td>4919.0</td>\n",
       "      <td>8379.00</td>\n",
       "      <td>19999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>14313.103401</td>\n",
       "      <td>7117.786044</td>\n",
       "      <td>2094.0</td>\n",
       "      <td>8047.00</td>\n",
       "      <td>14235.5</td>\n",
       "      <td>20461.50</td>\n",
       "      <td>26999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.693197</td>\n",
       "      <td>2.498009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OverTime</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.282993</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>15.209524</td>\n",
       "      <td>3.659938</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerformanceRating</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>3.153741</td>\n",
       "      <td>0.360824</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.712245</td>\n",
       "      <td>1.081209</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.793878</td>\n",
       "      <td>0.852077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>11.279592</td>\n",
       "      <td>7.780782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.799320</td>\n",
       "      <td>1.289271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.761224</td>\n",
       "      <td>0.706476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>7.008163</td>\n",
       "      <td>6.126525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>4.229252</td>\n",
       "      <td>3.623137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.187755</td>\n",
       "      <td>3.222430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>4.123129</td>\n",
       "      <td>3.568136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsTest</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.280272</td>\n",
       "      <td>0.449285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count          mean          std     min      25%  \\\n",
       "EmployeeId                1470.0    735.500000   424.496761     1.0   368.25   \n",
       "Age                       1470.0     36.923810     9.135373    18.0    30.00   \n",
       "Attrition                 1470.0      0.161224     0.367863     0.0     0.00   \n",
       "BusinessTravel            1470.0      1.086395     0.532170     0.0     1.00   \n",
       "DailyRate                 1470.0    802.485714   403.509100   102.0   465.00   \n",
       "DepartmentN               1470.0      1.260544     0.527792     0.0     1.00   \n",
       "DistanceFromHome          1470.0      9.192517     8.106864     1.0     2.00   \n",
       "Education                 1470.0      2.912925     1.024165     1.0     2.00   \n",
       "EducationFieldN           1470.0      1.915646     1.079401     0.0     1.00   \n",
       "EnvironmentSatisfaction   1470.0      2.721769     1.093082     1.0     2.00   \n",
       "GenderN                   1470.0      0.400000     0.490065     0.0     0.00   \n",
       "HourlyRate                1470.0     65.891156    20.329428    30.0    48.00   \n",
       "JobInvolvement            1470.0      2.729932     0.711561     1.0     2.00   \n",
       "JobLevel                  1470.0      2.063946     1.106940     1.0     1.00   \n",
       "JobRoleN                  1470.0      3.778912     2.126872     0.0     2.00   \n",
       "JobSatisfaction           1470.0      2.728571     1.102846     1.0     2.00   \n",
       "MaritalStatusN            1470.0      0.764626     0.790757     0.0     0.00   \n",
       "MonthlyIncome             1470.0   6502.931293  4707.956783  1009.0  2911.00   \n",
       "MonthlyRate               1470.0  14313.103401  7117.786044  2094.0  8047.00   \n",
       "NumCompaniesWorked        1470.0      2.693197     2.498009     0.0     1.00   \n",
       "OverTime                  1470.0      0.282993     0.450606     0.0     0.00   \n",
       "PercentSalaryHike         1470.0     15.209524     3.659938    11.0    12.00   \n",
       "PerformanceRating         1470.0      3.153741     0.360824     3.0     3.00   \n",
       "RelationshipSatisfaction  1470.0      2.712245     1.081209     1.0     2.00   \n",
       "StockOptionLevel          1470.0      0.793878     0.852077     0.0     0.00   \n",
       "TotalWorkingYears         1470.0     11.279592     7.780782     0.0     6.00   \n",
       "TrainingTimesLastYear     1470.0      2.799320     1.289271     0.0     2.00   \n",
       "WorkLifeBalance           1470.0      2.761224     0.706476     1.0     2.00   \n",
       "YearsAtCompany            1470.0      7.008163     6.126525     0.0     3.00   \n",
       "YearsInCurrentRole        1470.0      4.229252     3.623137     0.0     2.00   \n",
       "YearsSinceLastPromotion   1470.0      2.187755     3.222430     0.0     0.00   \n",
       "YearsWithCurrManager      1470.0      4.123129     3.568136     0.0     2.00   \n",
       "IsTest                    1470.0      0.280272     0.449285     0.0     0.00   \n",
       "\n",
       "                              50%       75%      max  \n",
       "EmployeeId                  735.5   1102.75   1470.0  \n",
       "Age                          36.0     43.00     60.0  \n",
       "Attrition                     0.0      0.00      1.0  \n",
       "BusinessTravel                1.0      1.00      2.0  \n",
       "DailyRate                   802.0   1157.00   1499.0  \n",
       "DepartmentN                   1.0      2.00      2.0  \n",
       "DistanceFromHome              7.0     14.00     29.0  \n",
       "Education                     3.0      4.00      5.0  \n",
       "EducationFieldN               2.0      2.00      5.0  \n",
       "EnvironmentSatisfaction       3.0      4.00      4.0  \n",
       "GenderN                       0.0      1.00      1.0  \n",
       "HourlyRate                   66.0     83.75    100.0  \n",
       "JobInvolvement                3.0      3.00      4.0  \n",
       "JobLevel                      2.0      3.00      5.0  \n",
       "JobRoleN                      3.0      5.00      8.0  \n",
       "JobSatisfaction               3.0      4.00      4.0  \n",
       "MaritalStatusN                1.0      1.00      2.0  \n",
       "MonthlyIncome              4919.0   8379.00  19999.0  \n",
       "MonthlyRate               14235.5  20461.50  26999.0  \n",
       "NumCompaniesWorked            2.0      4.00      9.0  \n",
       "OverTime                      0.0      1.00      1.0  \n",
       "PercentSalaryHike            14.0     18.00     25.0  \n",
       "PerformanceRating             3.0      3.00      4.0  \n",
       "RelationshipSatisfaction      3.0      4.00      4.0  \n",
       "StockOptionLevel              1.0      1.00      3.0  \n",
       "TotalWorkingYears            10.0     15.00     40.0  \n",
       "TrainingTimesLastYear         3.0      3.00      6.0  \n",
       "WorkLifeBalance               3.0      3.00      4.0  \n",
       "YearsAtCompany                5.0      9.00     40.0  \n",
       "YearsInCurrentRole            3.0      7.00     18.0  \n",
       "YearsSinceLastPromotion       1.0      3.00     15.0  \n",
       "YearsWithCurrManager          3.0      7.00     17.0  \n",
       "IsTest                        0.0      1.00      1.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring Metabase\n",
    "\n",
    "### 2.1. First Time Setup\n",
    "1. Run `run_metabase.sh`\n",
    "2. Open `http://localhost:3000` (Metabase URL)\n",
    "3. Setup username and password with something easy (e.g. `admin@example.com`, `admin456`)\n",
    "3. Fill other things in setup and connect to database (see Supabase - Project Settings - Database)\n",
    "4. Edit more through Metabase admin settings if necessary\n",
    "\n",
    "### 2.2. Add Model as Data Source\n",
    "Metabase model is an derived database table that should be used as main data source for questions and visualizations\n",
    "\n",
    "You can join tables, filter columns, or add engineered columns (e.g. binned age & income) without changing the database itself\n",
    "\n",
    "1. To add a new model, click New - Model - Use a native query\n",
    "2. Select your database, write your SQL query, and save when done\n",
    "3. A single model can (and should) be used by multiple questions and visualizations, try creating a broad one (contains many columns) if possible\n",
    "4. By using a single model in a dashboard, you can create filters (e.g. by age) that can be applied to all visualizations at once\n",
    "5. If you use more than 1 model, the filter won't be applied if there's no common column across models (I learned from my mistake)\n",
    "\n",
    "### 2.3. Add Question/Visualization from a Model\n",
    "1. Click New - Question, and add your previously created model\n",
    "2. Click the metrics you want to see (e.g. count of) and the column(s) to group by (e.g. age and attrition)\n",
    "3. Click Visualize - Visualization, to customize the graph (e.g. line chart)\n",
    "4. If the graph won't show due to ambiguous columns, click the gear icon and customize the axis manually\n",
    "5. Add a dashboard to show the visualization, and create more questions if necessary (e.g. gender and attrition)\n",
    "6. Remember that 1 model can be used by multiple questions, so creating 1 model for each question is very discouraged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For quick check of data and faster mass visualizations\n",
    "\n",
    "# filter = df['Attrition'] == 1\n",
    "\n",
    "# _ = df[filter].hist(\n",
    "#     [\n",
    "#         i for i in df.columns\n",
    "#         if i not in cat_col + ['EmployeeId', 'IsTest', 'Attrition']\n",
    "#         # Remove categorical columns too\n",
    "#         and i[-1] != 'N'\n",
    "#     ],\n",
    "#     figsize = (20, 20)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. ML Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Replace Outlier\n",
    "Outlier can be replaced using IQR method (or just skip this and use robust scaler later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>18711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>17123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>17639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>18824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>16885.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MonthlyIncome         \n",
       "              self    other\n",
       "7          16581.0  19859.0\n",
       "17         16581.0  19406.0\n",
       "19         16581.0  18711.0\n",
       "27         16581.0  17123.0\n",
       "29         16581.0  19658.0\n",
       "...            ...      ...\n",
       "1373       16581.0  17639.0\n",
       "1414       16581.0  19665.0\n",
       "1425       16581.0  19094.0\n",
       "1444       16581.0  18824.0\n",
       "1460       16581.0  16885.0\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# Only normalize rate and income\n",
    "# Normalizing other columns may cause weird issues\n",
    "col = [i for i in df.columns if 'Rate' in i or 'Income' in i]\n",
    "\n",
    "for c in col:\n",
    "    q1 = df[c].quantile(0.25)\n",
    "    q3 = df[c].quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    iqr1 = q1 - 1.5 * iqr\n",
    "    iqr3 = q3 + 1.5 * iqr\n",
    "    # print(c, q1, q3, iqr, iqr1, iqr3)\n",
    "    \n",
    "    df.loc[ df[c] < iqr1, c ] = 0 if iqr1 < 0 else int(iqr1)\n",
    "    df.loc[ df[c] > iqr3, c ] = int(iqr3)\n",
    "\n",
    "display(df.compare(df_copy))\n",
    "del df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See more details from Metabase\n",
    "# df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning is must be applied first to group continuous values so that it has limited range (e.g. age, salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pd.cut to use inclusive range\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\n",
    "bin_arg = {'right': True, 'include_lowest': True}\n",
    "\n",
    "# Bins/groups feature range first\n",
    "# https://stackoverflow.com/questions/45273731\n",
    "\n",
    "temp = [18] + [i * 5 + 20 for i in range(9)] # 18, 20, 25, 30, ..., 60\n",
    "df['Age'] = pd.cut(df['Age'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 5 for i in range(7)] # 0, 5, 10, 15, ..., 30\n",
    "df['DistanceFromHome'] = pd.cut(df['DistanceFromHome'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 2000 for i in range(16)] # 0, 2000, 4000, 6000, ..., 30000\n",
    "df['MonthlyIncome'] = pd.cut(df['MonthlyIncome'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['MonthlyRate'] = pd.cut(df['MonthlyRate'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [0, 1, 2, 5, 10]\n",
    "df['NumCompaniesWorked'] = pd.cut(df['NumCompaniesWorked'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 5 for i in range(6)] # 0, 5, 10, 15, ..., 25\n",
    "df['PercentSalaryHike'] = pd.cut(df['PercentSalaryHike'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [0, 2] + [ (i + 1) * 5 for i in range(9)] # 0, 2, 5, 10, 15, ..., 45\n",
    "df['YearsAtCompany'] = pd.cut(df['YearsAtCompany'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsInCurrentRole'] = pd.cut(df['YearsInCurrentRole'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsSinceLastPromotion'] = pd.cut(df['YearsSinceLastPromotion'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsWithCurrManager'] = pd.cut(df['YearsWithCurrManager'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['TotalWorkingYears'] = pd.cut(df['TotalWorkingYears'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some readings, it seems that WOE and target encoder can also be used for non-categorical features\n",
    "\n",
    "This will stimulate relation between those features and target variable (attrition) that may be understood by ML model (see [reference](https://www.datacamp.com/tutorial/categorical-data))\n",
    "\n",
    "For example, we can also apply this on ordinal values (WLB, job satisfaction, etc) or even continuous numerical values (age, salary, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeN</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>BusinessTravelN</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DepartmentN</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>...</th>\n",
       "      <th>WorkLifeBalanceN</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsAtCompanyN</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsInCurrentRoleN</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsSinceLastPromotionN</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>YearsWithCurrManagerN</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>0.189956</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>2</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>2</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>2</td>\n",
       "      <td>0.121966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0.206156</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101520</td>\n",
       "      <td>2</td>\n",
       "      <td>0.121966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116487</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId Age      AgeN  Attrition  BusinessTravel  BusinessTravelN  \\\n",
       "0           1   4  0.090735          0               2         0.248661   \n",
       "1           2   4  0.090735          1               1         0.149579   \n",
       "2           3   7  0.104574          1               1         0.149579   \n",
       "3           4   5  0.093970          0               2         0.248661   \n",
       "4           5   4  0.090735          0               1         0.149579   \n",
       "\n",
       "   DailyRate              Department  DepartmentN DistanceFromHome  ...  \\\n",
       "0       1444         Human Resources     0.189956                0  ...   \n",
       "1       1141  Research & Development     0.138418                2  ...   \n",
       "2       1323  Research & Development     0.138418                0  ...   \n",
       "3        555                   Sales     0.206156                5  ...   \n",
       "4       1194  Research & Development     0.138418                0  ...   \n",
       "\n",
       "   WorkLifeBalanceN  YearsAtCompany YearsAtCompanyN  YearsInCurrentRole  \\\n",
       "0          0.142236               2        0.122836                   0   \n",
       "1          0.309554               0        0.297628                   0   \n",
       "2          0.176364               2        0.122836                   0   \n",
       "3          0.176364               4        0.070050                   1   \n",
       "4          0.142236               1        0.138295                   1   \n",
       "\n",
       "   YearsInCurrentRoleN  YearsSinceLastPromotion  YearsSinceLastPromotionN  \\\n",
       "0             0.225730                        0                  0.169545   \n",
       "1             0.225730                        0                  0.169545   \n",
       "2             0.225730                        0                  0.169545   \n",
       "3             0.116487                        1                  0.101520   \n",
       "4             0.116487                        0                  0.169545   \n",
       "\n",
       "  YearsWithCurrManager  YearsWithCurrManagerN  IsTest  \n",
       "0                    0               0.213667       1  \n",
       "1                    0               0.213667       0  \n",
       "2                    2               0.121966       0  \n",
       "3                    2               0.121966       0  \n",
       "4                    0               0.213667       1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See previous cell\n",
    "bin_col = [\n",
    "    'Age','DistanceFromHome','MonthlyIncome','MonthlyRate','NumCompaniesWorked','PercentSalaryHike',\n",
    "    'YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','TotalWorkingYears'\n",
    "]\n",
    "\n",
    "# Previously it was encoded only using label encoder\n",
    "cat_col = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
    "\n",
    "# Other columns, mostly ordinal (if not all)\n",
    "other_col = [\n",
    "    'BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction',\n",
    "    'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear',\n",
    "    'WorkLifeBalance'\n",
    "]\n",
    "\n",
    "# Columns to be encoded\n",
    "enc_col = bin_col + cat_col + other_col\n",
    "cat_encoder = TargetEncoder()\n",
    "\n",
    "new_col = [i + 'N' for i in enc_col]\n",
    "cat_encoder.fit(df[enc_col], df['Attrition'])\n",
    "df[new_col] = cat_encoder.transform(df[enc_col])\n",
    "\n",
    "# Sort column names to fix newly added columns\n",
    "df = df.reindex(sorted(df.columns), axis = 1)\n",
    "\n",
    "# Place employee id at beginning and is test at the end\n",
    "temp = [i for i in df.columns if i not in ['EmployeeId', 'IsTest']]\n",
    "df = df[['EmployeeId'] + temp + ['IsTest']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Feature Removal/Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove original columns that have been converted to numerical or processed using encoder\n",
    "\n",
    "Most ML models can't process non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'EducationField', 'Education', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
      "Kept: ['AgeN', 'BusinessTravelN', 'DepartmentN', 'DistanceFromHomeN', 'EducationFieldN', 'EducationN', 'EnvironmentSatisfactionN', 'GenderN', 'JobInvolvementN', 'JobLevelN', 'JobRoleN', 'JobSatisfactionN', 'MaritalStatusN', 'MonthlyIncomeN', 'MonthlyRateN', 'NumCompaniesWorkedN', 'OverTimeN', 'PercentSalaryHikeN', 'PerformanceRatingN', 'RelationshipSatisfactionN', 'StockOptionLevelN', 'TotalWorkingYearsN', 'TrainingTimesLastYearN', 'WorkLifeBalanceN', 'YearsAtCompanyN', 'YearsInCurrentRoleN', 'YearsSinceLastPromotionN', 'YearsWithCurrManagerN']\n"
     ]
    }
   ],
   "source": [
    "temp = [i[:-1] for i in df.columns if i[-1] == 'N']\n",
    "print('Removed:', temp)\n",
    "print('Kept:', [i + 'N' for i in temp])\n",
    "\n",
    "df.drop(temp, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unimportant features that have little effect or too dependent on each other (e.g. hourly/daily/monthly rate -> monthly income)\n",
    "\n",
    "We should prioritize strong/independent features first, and filter those that are less relevant (see [here](https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80#Feature-Engineering) and [there](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/))\n",
    "\n",
    "**Note 1:** The first reference applied SMOTE on test data, which is a bad practice and inflated the F1 score\n",
    "\n",
    "**Note 2:** Some feature selection methods may only work for numerical/categorical data, not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2dbf8_row0_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row1_col1 {\n",
       "  background-color: #b50927;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row2_col1 {\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row3_col1 {\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row4_col1 {\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row5_col1 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row6_col1 {\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row7_col1 {\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row8_col1 {\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row9_col1 {\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row10_col1 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row11_col1 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row12_col1 {\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row13_col1 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row14_col1, #T_2dbf8_row15_col1 {\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row16_col1 {\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row17_col1 {\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row18_col1 {\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row19_col1 {\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row20_col1 {\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row21_col1 {\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row22_col1 {\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row23_col1 {\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row24_col1 {\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row25_col1 {\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row26_col1 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_2dbf8_row27_col1 {\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row28_col1 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row29_col1 {\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row30_col1 {\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_2dbf8_row31_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2dbf8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2dbf8_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_2dbf8_level0_col1\" class=\"col_heading level0 col1\" >importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_2dbf8_row0_col0\" class=\"data row0 col0\" >TotalWorkingYearsN</td>\n",
       "      <td id=\"T_2dbf8_row0_col1\" class=\"data row0 col1\" >0.247316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_2dbf8_row1_col0\" class=\"data row1 col0\" >OverTimeN</td>\n",
       "      <td id=\"T_2dbf8_row1_col1\" class=\"data row1 col1\" >0.246118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_2dbf8_row2_col0\" class=\"data row2 col0\" >JobRoleN</td>\n",
       "      <td id=\"T_2dbf8_row2_col1\" class=\"data row2 col1\" >0.242134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_2dbf8_row3_col0\" class=\"data row3 col0\" >MonthlyIncomeN</td>\n",
       "      <td id=\"T_2dbf8_row3_col1\" class=\"data row3 col1\" >0.239267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_2dbf8_row4_col0\" class=\"data row4 col0\" >AgeN</td>\n",
       "      <td id=\"T_2dbf8_row4_col1\" class=\"data row4 col1\" >0.223413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_2dbf8_row5_col0\" class=\"data row5 col0\" >JobLevelN</td>\n",
       "      <td id=\"T_2dbf8_row5_col1\" class=\"data row5 col1\" >0.222125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_2dbf8_row6_col0\" class=\"data row6 col0\" >YearsAtCompanyN</td>\n",
       "      <td id=\"T_2dbf8_row6_col1\" class=\"data row6 col1\" >0.217988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_2dbf8_row7_col0\" class=\"data row7 col0\" >StockOptionLevelN</td>\n",
       "      <td id=\"T_2dbf8_row7_col1\" class=\"data row7 col1\" >0.203035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_2dbf8_row8_col0\" class=\"data row8 col0\" >MaritalStatusN</td>\n",
       "      <td id=\"T_2dbf8_row8_col1\" class=\"data row8 col1\" >0.177211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_2dbf8_row9_col0\" class=\"data row9 col0\" >YearsInCurrentRoleN</td>\n",
       "      <td id=\"T_2dbf8_row9_col1\" class=\"data row9 col1\" >0.165077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_2dbf8_row10_col0\" class=\"data row10 col0\" >YearsWithCurrManagerN</td>\n",
       "      <td id=\"T_2dbf8_row10_col1\" class=\"data row10 col1\" >0.141636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_2dbf8_row11_col0\" class=\"data row11 col0\" >JobInvolvementN</td>\n",
       "      <td id=\"T_2dbf8_row11_col1\" class=\"data row11 col1\" >0.139217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_2dbf8_row12_col0\" class=\"data row12 col0\" >BusinessTravelN</td>\n",
       "      <td id=\"T_2dbf8_row12_col1\" class=\"data row12 col1\" >0.128260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_2dbf8_row13_col0\" class=\"data row13 col0\" >EnvironmentSatisfactionN</td>\n",
       "      <td id=\"T_2dbf8_row13_col1\" class=\"data row13 col1\" >0.123729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_2dbf8_row14_col0\" class=\"data row14 col0\" >DistanceFromHomeN</td>\n",
       "      <td id=\"T_2dbf8_row14_col1\" class=\"data row14 col1\" >0.109274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_2dbf8_row15_col0\" class=\"data row15 col0\" >JobSatisfactionN</td>\n",
       "      <td id=\"T_2dbf8_row15_col1\" class=\"data row15 col1\" >0.109125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_2dbf8_row16_col0\" class=\"data row16 col0\" >WorkLifeBalanceN</td>\n",
       "      <td id=\"T_2dbf8_row16_col1\" class=\"data row16 col1\" >0.105381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_2dbf8_row17_col0\" class=\"data row17 col0\" >EducationFieldN</td>\n",
       "      <td id=\"T_2dbf8_row17_col1\" class=\"data row17 col1\" >0.104399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_2dbf8_row18_col0\" class=\"data row18 col0\" >TrainingTimesLastYearN</td>\n",
       "      <td id=\"T_2dbf8_row18_col1\" class=\"data row18 col1\" >0.101502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_2dbf8_row19_col0\" class=\"data row19 col0\" >MonthlyRateN</td>\n",
       "      <td id=\"T_2dbf8_row19_col1\" class=\"data row19 col1\" >0.094489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_2dbf8_row20_col0\" class=\"data row20 col0\" >DepartmentN</td>\n",
       "      <td id=\"T_2dbf8_row20_col1\" class=\"data row20 col1\" >0.085698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_2dbf8_row21_col0\" class=\"data row21 col0\" >NumCompaniesWorkedN</td>\n",
       "      <td id=\"T_2dbf8_row21_col1\" class=\"data row21 col1\" >0.077756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_2dbf8_row22_col0\" class=\"data row22 col0\" >YearsSinceLastPromotionN</td>\n",
       "      <td id=\"T_2dbf8_row22_col1\" class=\"data row22 col1\" >0.063805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_2dbf8_row23_col0\" class=\"data row23 col0\" >RelationshipSatisfactionN</td>\n",
       "      <td id=\"T_2dbf8_row23_col1\" class=\"data row23 col1\" >0.059711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_2dbf8_row24_col0\" class=\"data row24 col0\" >EducationN</td>\n",
       "      <td id=\"T_2dbf8_row24_col1\" class=\"data row24 col1\" >0.045728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_2dbf8_row25_col0\" class=\"data row25 col0\" >GenderN</td>\n",
       "      <td id=\"T_2dbf8_row25_col1\" class=\"data row25 col1\" >0.029453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_2dbf8_row26_col0\" class=\"data row26 col0\" >PercentSalaryHikeN</td>\n",
       "      <td id=\"T_2dbf8_row26_col1\" class=\"data row26 col1\" >0.020810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_2dbf8_row27_col0\" class=\"data row27 col0\" >PerformanceRatingN</td>\n",
       "      <td id=\"T_2dbf8_row27_col1\" class=\"data row27 col1\" >0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_2dbf8_row28_col0\" class=\"data row28 col0\" >EmployeeId</td>\n",
       "      <td id=\"T_2dbf8_row28_col1\" class=\"data row28 col1\" >-0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_2dbf8_row29_col0\" class=\"data row29 col0\" >HourlyRate</td>\n",
       "      <td id=\"T_2dbf8_row29_col1\" class=\"data row29 col1\" >-0.006846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_2dbf8_row30_col0\" class=\"data row30 col0\" >IsTest</td>\n",
       "      <td id=\"T_2dbf8_row30_col1\" class=\"data row30 col1\" >-0.034699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2dbf8_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_2dbf8_row31_col0\" class=\"data row31 col0\" >DailyRate</td>\n",
       "      <td id=\"T_2dbf8_row31_col1\" class=\"data row31 col1\" >-0.056652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2672b34b710>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = r_regression(\n",
    "    X = df[[i for i in df.columns if i != 'Attrition']],\n",
    "    y = df['Attrition']\n",
    ")\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    'column': [i for i in df.columns if i != 'Attrition'],\n",
    "    'importance': importance\n",
    "})\n",
    "\n",
    "temp = temp.sort_values(by = 'importance', ascending = False)\n",
    "temp = temp.reset_index(drop = True)\n",
    "temp.style.background_gradient(cmap = 'coolwarm', axis = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that target encoder can change the data relationship, especially those nearing the range -1 to the opposite side aka 1 (see previous correlation matrix for comparison)\n",
    "\n",
    "The affected columns still have strong correlation (either near 1 or -1 originally), but now always move the same way as attrition (more XXX = more attrition chance), as opposed of before, where some column may have this kind of relationship: more XXX = less attrition chance\n",
    "\n",
    "**Note:** Correlation matrix above can't be used directly as conclusion since each possible values are replaced/scored based on the amount of attrition (not sorted in a standard way). It's meant to be used by ML model only unless you know what you're doing\n",
    "\n",
    "Now, we will only keep columns with importance more than 0.1 (or less than -0.1), without removing `EmployeeId`, `IsTest`, and `Attrition` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>TotalWorkingYearsN</th>\n",
       "      <th>OverTimeN</th>\n",
       "      <th>JobRoleN</th>\n",
       "      <th>MonthlyIncomeN</th>\n",
       "      <th>AgeN</th>\n",
       "      <th>JobLevelN</th>\n",
       "      <th>YearsAtCompanyN</th>\n",
       "      <th>StockOptionLevelN</th>\n",
       "      <th>MaritalStatusN</th>\n",
       "      <th>...</th>\n",
       "      <th>JobInvolvementN</th>\n",
       "      <th>BusinessTravelN</th>\n",
       "      <th>EnvironmentSatisfactionN</th>\n",
       "      <th>DistanceFromHomeN</th>\n",
       "      <th>JobSatisfactionN</th>\n",
       "      <th>WorkLifeBalanceN</th>\n",
       "      <th>EducationFieldN</th>\n",
       "      <th>TrainingTimesLastYearN</th>\n",
       "      <th>IsTest</th>\n",
       "      <th>Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.229057</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.134427</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.125805</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.069035</td>\n",
       "      <td>0.113303</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.253069</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.309554</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.253069</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.174802</td>\n",
       "      <td>0.113871</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0.047526</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.149551</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.219659</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1466</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.103084</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.146850</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.255039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1467</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.113871</td>\n",
       "      <td>0.123310</td>\n",
       "      <td>0.146850</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.101041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189249</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.228072</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.140567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1468</td>\n",
       "      <td>0.434971</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.238978</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.212685</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.113367</td>\n",
       "      <td>0.168582</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1469</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.101041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.274661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1470</td>\n",
       "      <td>0.434971</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.392652</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.546509</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.255039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.228072</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.241597</td>\n",
       "      <td>0.117926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1470 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  TotalWorkingYearsN  OverTimeN  JobRoleN  MonthlyIncomeN  \\\n",
       "0              1            0.149935   0.304747  0.229057        0.233147   \n",
       "1              2            0.125805   0.104402  0.069035        0.113303   \n",
       "2              3            0.100942   0.304747  0.160960        0.233147   \n",
       "3              4            0.082971   0.104402  0.174802        0.113871   \n",
       "4              5            0.100942   0.104402  0.160960        0.233147   \n",
       "...          ...                 ...        ...       ...             ...   \n",
       "1465        1466            0.149935   0.304747  0.069267        0.103084   \n",
       "1466        1467            0.100942   0.104402  0.025306        0.113871   \n",
       "1467        1468            0.434971   0.304747  0.238978        0.233147   \n",
       "1468        1469            0.149935   0.104402  0.160960        0.233147   \n",
       "1469        1470            0.434971   0.104402  0.392652        0.233147   \n",
       "\n",
       "          AgeN  JobLevelN  YearsAtCompanyN  StockOptionLevelN  MaritalStatusN  \\\n",
       "0     0.090735   0.263083         0.122836           0.094031        0.124858   \n",
       "1     0.090735   0.097456         0.297628           0.243878        0.124858   \n",
       "2     0.104574   0.263083         0.122836           0.176280        0.124858   \n",
       "3     0.093970   0.047526         0.070050           0.094031        0.124858   \n",
       "4     0.090735   0.263083         0.138295           0.176280        0.124858   \n",
       "...        ...        ...              ...                ...             ...   \n",
       "1465  0.090735   0.146850         0.297628           0.243878        0.255039   \n",
       "1466  0.123310   0.146850         0.065213           0.176280        0.101041   \n",
       "1467  0.212685   0.263083         0.297628           0.243878        0.124858   \n",
       "1468  0.090735   0.263083         0.138295           0.094031        0.101041   \n",
       "1469  0.546509   0.263083         0.297628           0.243878        0.255039   \n",
       "\n",
       "      ...  JobInvolvementN  BusinessTravelN  EnvironmentSatisfactionN  \\\n",
       "0     ...         0.144027         0.248661                  0.134581   \n",
       "1     ...         0.333910         0.149579                  0.253069   \n",
       "2     ...         0.144027         0.149579                  0.253069   \n",
       "3     ...         0.144027         0.248661                  0.136912   \n",
       "4     ...         0.144027         0.149579                  0.136912   \n",
       "...   ...              ...              ...                       ...   \n",
       "1465  ...         0.144027         0.149579                  0.136912   \n",
       "1466  ...         0.189249         0.149579                  0.134581   \n",
       "1467  ...         0.144027         0.149579                  0.136912   \n",
       "1468  ...         0.144027         0.080294                  0.136912   \n",
       "1469  ...         0.333910         0.248661                  0.136912   \n",
       "\n",
       "      DistanceFromHomeN  JobSatisfactionN  WorkLifeBalanceN  EducationFieldN  \\\n",
       "0              0.137691          0.164275          0.142236         0.134427   \n",
       "1              0.216783          0.164275          0.309554         0.135823   \n",
       "2              0.137691          0.165149          0.176364         0.146887   \n",
       "3              0.149551          0.164275          0.176364         0.219659   \n",
       "4              0.137691          0.165149          0.142236         0.135823   \n",
       "...                 ...               ...               ...              ...   \n",
       "1465           0.137691          0.165149          0.176364         0.146887   \n",
       "1466           0.183799          0.228072          0.142236         0.146887   \n",
       "1467           0.216783          0.113367          0.168582         0.146887   \n",
       "1468           0.183799          0.165149          0.142236         0.146887   \n",
       "1469           0.137691          0.228072          0.176364         0.241597   \n",
       "\n",
       "      TrainingTimesLastYearN  IsTest  Attrition  \n",
       "0                   0.179123       1          0  \n",
       "1                   0.179123       0          1  \n",
       "2                   0.179123       0          1  \n",
       "3                   0.179123       0          0  \n",
       "4                   0.179123       1          0  \n",
       "...                      ...     ...        ...  \n",
       "1465                0.210884       0          0  \n",
       "1466                0.140567       1          0  \n",
       "1467                0.210884       0          1  \n",
       "1468                0.274661       0          0  \n",
       "1469                0.117926       0          1  \n",
       "\n",
       "[1470 rows x 22 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = temp[(temp['importance'] >= 0.1) | (temp['importance'] <= -0.1)]['column']\n",
    "col = ['EmployeeId'] + list(col) + ['IsTest', 'Attrition']\n",
    "\n",
    "df = df[col]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Data Scaling/Normalization and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data using standard/min-max scaler\n",
    "\n",
    "**Note:** Columns that are affected by target encoder can also be ignored (range should already between 0-1)\n",
    "\n",
    "**Alternative:** use robust scaler if not using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [i for i in df.columns if i != 'EmployeeId' and i != 'IsTest']\n",
    "x_col = [i for i in col if i != 'Attrition']\n",
    "y_col = ['Attrition']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Use \"inverse_transform\" to inverse later\n",
    "df[x_col] = scaler.fit_transform(df[x_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for training (x = feature, y = label)\n",
    "x_train = df.loc[df['IsTest'] == 0, x_col]\n",
    "y_train = df.loc[df['IsTest'] == 0, ['Attrition']]\n",
    "\n",
    "# Data for testing\n",
    "x_test = df.loc[df['IsTest'] == 1, x_col]\n",
    "y_test = df.loc[df['IsTest'] == 1, ['Attrition']]\n",
    "\n",
    "len(x_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [SMOTE](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/) and set class weight to workaround imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 2, random_state = 12)\n",
    "# x_pcay = pca.fit_transform(x_train[y_train['Attrition'] == 0])\n",
    "# x_pcan = pca.fit_transform(x_train[y_train['Attrition'] == 1])\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n",
    "# ax1.scatter(x_pcay[:,0], x_pcay[:,1], label = '0')\n",
    "# ax1.scatter(x_pcan[:,0], x_pcan[:,1], label = '1')\n",
    "# ax1.legend(loc = 'upper left')\n",
    "# ax1.set_title('Before using SMOTE')\n",
    "\n",
    "# #  ====================\n",
    "\n",
    "# smote = SMOTETomek(random_state = 12)\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# print('After SMOTE:', y_train.value_counts())\n",
    "\n",
    "# #  ====================\n",
    "\n",
    "# x_pcay = pca.fit_transform(x_train[y_train['Attrition'] == 0])\n",
    "# x_pcan = pca.fit_transform(x_train[y_train['Attrition'] == 1])\n",
    "\n",
    "# ax2.scatter(x_pcay[:,0], x_pcay[:,1], label = '0')\n",
    "# ax2.scatter(x_pcan[:,0], x_pcan[:,1], label = '1')\n",
    "# ax2.legend(loc = 'upper left')\n",
    "# ax2.set_title('After using SMOTE')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, it seems that SMOTE may actually make things worse\n",
    "\n",
    "So we can just skip it and apply only class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 4.910614525139665}\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/66501676\n",
    "# Alternatively, use Sklearn \"compute_class_weight\"\n",
    "\n",
    "class_weights = {}\n",
    "samples = list(y_train.value_counts())\n",
    "sample_max = np.max(samples)\n",
    "\n",
    "for i in range (len(samples)):\n",
    "    class_weights[i] = sample_max / samples[i]\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple testings, I decided to use neural network (NN) [ensembling](https://www.tensorflow.org/decision_forests/tutorials/model_composition_colab) (since TFDF v1.8.1 only support Linux)\n",
    "\n",
    "**Alternative:** use Sklearn [ensembling](https://scikit-learn.org/stable/modules/ensemble.html) (non-NN approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)       [(None, 14)]                 0         []                            \n",
      "                                                                                                  \n",
      " dense_85 (Dense)            (None, 128)                  1920      ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " dense_89 (Dense)            (None, 128)                  1920      ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_66 (PReLU)          (None, 128)                  128       ['dense_85[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_69 (PReLU)          (None, 128)                  128       ['dense_89[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 128)                  512       ['p_re_lu_66[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 128)                  0         ['p_re_lu_69[0][0]']          \n",
      "                                                                                                  \n",
      " dense_86 (Dense)            (None, 64)                   8256      ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_90 (Dense)            (None, 64)                   8256      ['dropout_15[0][0]']          \n",
      "                                                                                                  \n",
      " dense_93 (Dense)            (None, 128)                  1920      ['input_14[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_67 (PReLU)          (None, 64)                   64        ['dense_86[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_70 (PReLU)          (None, 64)                   64        ['dense_90[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_72 (PReLU)          (None, 128)                  128       ['dense_93[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 64)                   256       ['p_re_lu_67[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 64)                   0         ['p_re_lu_70[0][0]']          \n",
      "                                                                                                  \n",
      " dense_94 (Dense)            (None, 64)                   8256      ['p_re_lu_72[0][0]']          \n",
      "                                                                                                  \n",
      " dense_87 (Dense)            (None, 32)                   2080      ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_91 (Dense)            (None, 32)                   2080      ['dropout_16[0][0]']          \n",
      "                                                                                                  \n",
      " p_re_lu_73 (PReLU)          (None, 64)                   64        ['dense_94[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_68 (PReLU)          (None, 32)                   32        ['dense_87[0][0]']            \n",
      "                                                                                                  \n",
      " p_re_lu_71 (PReLU)          (None, 32)                   32        ['dense_91[0][0]']            \n",
      "                                                                                                  \n",
      " dense_95 (Dense)            (None, 32)                   2080      ['p_re_lu_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 32)                   128       ['p_re_lu_68[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 32)                   0         ['p_re_lu_71[0][0]']          \n",
      "                                                                                                  \n",
      " p_re_lu_74 (PReLU)          (None, 32)                   32        ['dense_95[0][0]']            \n",
      "                                                                                                  \n",
      " dense_88 (Dense)            (None, 1)                    33        ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_92 (Dense)            (None, 1)                    33        ['dropout_17[0][0]']          \n",
      "                                                                                                  \n",
      " dense_96 (Dense)            (None, 1)                    33        ['p_re_lu_74[0][0]']          \n",
      "                                                                                                  \n",
      " tf.stack_6 (TFOpLambda)     (3, None, 1)                 0         ['dense_88[0][0]',            \n",
      "                                                                     'dense_92[0][0]',            \n",
      "                                                                     'dense_96[0][0]']            \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean_6 (TFO  (None, 1)                    0         ['tf.stack_6[0][0]']          \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38435 (150.14 KB)\n",
      "Trainable params: 37987 (148.39 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# All model inputs should refer to the same object\n",
    "model_input = keras.layers.Input(shape = (len(x_col),))\n",
    "\n",
    "model1 = keras.models.Sequential([\n",
    "    model_input,\n",
    "\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model2 = keras.models.Sequential([\n",
    "    model_input,\n",
    "\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.1),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.1),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model3 = keras.models.Sequential([\n",
    "    model_input,\n",
    "\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model_output = tf.reduce_mean(\n",
    "    tf.stack(\n",
    "        [ \n",
    "            model1.layers[-1].output,\n",
    "            model2.layers[-1].output,\n",
    "            model3.layers[-1].output\n",
    "        ],\n",
    "        axis = 0\n",
    "    ),\n",
    "    axis = 0\n",
    ")\n",
    "\n",
    "model_ensemble = keras.models.Model(\n",
    "    inputs = model_input,\n",
    "    outputs = model_output\n",
    ")\n",
    "\n",
    "model_ensemble.summary(expand_nested = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/17 [=====================>........] - ETA: 0s - loss: 1.0727 - acc: 0.5757 - auc: 0.7176 \n",
      "Epoch 1: val_auc improved from -inf to 0.73958, saving model to model\\model.keras\n",
      "17/17 [==============================] - 3s 52ms/step - loss: 1.0596 - acc: 0.5756 - auc: 0.7165 - val_loss: 0.7194 - val_acc: 0.1748 - val_auc: 0.7396 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.9444 - acc: 0.6815 - auc: 0.8392\n",
      "Epoch 2: val_auc improved from 0.73958 to 0.74886, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.9554 - acc: 0.6815 - auc: 0.8345 - val_loss: 0.7000 - val_acc: 0.4879 - val_auc: 0.7489 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.9270 - acc: 0.7254 - auc: 0.8511\n",
      "Epoch 3: val_auc improved from 0.74886 to 0.75614, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.9320 - acc: 0.7250 - auc: 0.8467 - val_loss: 0.6942 - val_acc: 0.5340 - val_auc: 0.7561 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.9228 - acc: 0.7406 - auc: 0.8564\n",
      "Epoch 4: val_auc improved from 0.75614 to 0.76649, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.9177 - acc: 0.7372 - auc: 0.8546 - val_loss: 0.6879 - val_acc: 0.5510 - val_auc: 0.7665 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.8896 - acc: 0.7617 - auc: 0.8686\n",
      "Epoch 5: val_auc did not improve from 0.76649\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.8897 - acc: 0.7665 - auc: 0.8731 - val_loss: 0.6518 - val_acc: 0.6723 - val_auc: 0.7651 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.8735 - acc: 0.7892 - auc: 0.8794\n",
      "Epoch 6: val_auc improved from 0.76649 to 0.76948, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.8735 - acc: 0.7892 - auc: 0.8794 - val_loss: 0.6317 - val_acc: 0.7233 - val_auc: 0.7695 - lr: 0.0100\n",
      "Epoch 7/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.8738 - acc: 0.8281 - auc: 0.8766\n",
      "Epoch 7: val_auc improved from 0.76948 to 0.77218, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 0.8604 - acc: 0.7836 - auc: 0.8799 - val_loss: 0.6056 - val_acc: 0.7791 - val_auc: 0.7722 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.8044 - acc: 0.8047 - auc: 0.8880\n",
      "Epoch 8: val_auc did not improve from 0.77218\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.8437 - acc: 0.8081 - auc: 0.8879 - val_loss: 0.5854 - val_acc: 0.8010 - val_auc: 0.7721 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.8449 - acc: 0.7885 - auc: 0.8794\n",
      "Epoch 9: val_auc did not improve from 0.77218\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.8336 - acc: 0.7911 - auc: 0.8854 - val_loss: 0.6045 - val_acc: 0.7209 - val_auc: 0.7719 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.8316 - acc: 0.8153 - auc: 0.8894\n",
      "Epoch 10: val_auc did not improve from 0.77218\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.8124 - acc: 0.8119 - auc: 0.8916 - val_loss: 0.5698 - val_acc: 0.7791 - val_auc: 0.7711 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.7934 - acc: 0.8185 - auc: 0.8974\n",
      "Epoch 11: val_auc improved from 0.77218 to 0.77408, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.7934 - acc: 0.8185 - auc: 0.8974 - val_loss: 0.5670 - val_acc: 0.7791 - val_auc: 0.7741 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.8025 - acc: 0.8197 - auc: 0.8959\n",
      "Epoch 12: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.7845 - acc: 0.8204 - auc: 0.8975 - val_loss: 0.5466 - val_acc: 0.7864 - val_auc: 0.7740 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.8156 - acc: 0.8042 - auc: 0.8729\n",
      "Epoch 13: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.8009 - acc: 0.8072 - auc: 0.8755 - val_loss: 0.5303 - val_acc: 0.8010 - val_auc: 0.7648 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.7604 - acc: 0.8224 - auc: 0.9037\n",
      "Epoch 14: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.7500 - acc: 0.8251 - auc: 0.9049 - val_loss: 0.5002 - val_acc: 0.8228 - val_auc: 0.7728 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7267 - acc: 0.8477 - auc: 0.9116\n",
      "Epoch 15: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.7544 - acc: 0.8336 - auc: 0.9017 - val_loss: 0.5306 - val_acc: 0.7913 - val_auc: 0.7721 - lr: 0.0100\n",
      "Epoch 16/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.7506 - acc: 0.8125 - auc: 0.8961\n",
      "Epoch 16: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.7466 - acc: 0.8195 - auc: 0.8996 - val_loss: 0.5020 - val_acc: 0.8034 - val_auc: 0.7712 - lr: 0.0100\n",
      "Epoch 17/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.7512 - acc: 0.8125 - auc: 0.8944\n",
      "Epoch 17: val_auc did not improve from 0.77408\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.7539 - acc: 0.8034 - auc: 0.8896 - val_loss: 0.4755 - val_acc: 0.8180 - val_auc: 0.7697 - lr: 0.0100\n",
      "Epoch 18/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.7305 - acc: 0.8232 - auc: 0.8993\n",
      "Epoch 18: val_auc improved from 0.77408 to 0.77640, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.7336 - acc: 0.8204 - auc: 0.8984 - val_loss: 0.4796 - val_acc: 0.8155 - val_auc: 0.7764 - lr: 0.0100\n",
      "Epoch 19/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.7370 - acc: 0.8229 - auc: 0.8998\n",
      "Epoch 19: val_auc did not improve from 0.77640\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.7179 - acc: 0.8233 - auc: 0.9040 - val_loss: 0.4386 - val_acc: 0.8398 - val_auc: 0.7642 - lr: 0.0100\n",
      "Epoch 20/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6916 - acc: 0.8099 - auc: 0.9075\n",
      "Epoch 20: val_auc did not improve from 0.77640\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.7122 - acc: 0.8062 - auc: 0.9008 - val_loss: 0.4551 - val_acc: 0.8374 - val_auc: 0.7672 - lr: 0.0100\n",
      "Epoch 21/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.8464 - acc: 0.7812 - auc: 0.8558\n",
      "Epoch 21: val_auc improved from 0.77640 to 0.78193, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.7152 - acc: 0.8129 - auc: 0.8975 - val_loss: 0.4616 - val_acc: 0.8301 - val_auc: 0.7819 - lr: 0.0100\n",
      "Epoch 22/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.6494 - acc: 0.8453 - auc: 0.9223\n",
      "Epoch 22: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6877 - acc: 0.8384 - auc: 0.9107 - val_loss: 0.5044 - val_acc: 0.8058 - val_auc: 0.7740 - lr: 0.0100\n",
      "Epoch 23/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.6782 - acc: 0.8210 - auc: 0.9125\n",
      "Epoch 23: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6766 - acc: 0.8299 - auc: 0.9110 - val_loss: 0.4166 - val_acc: 0.8471 - val_auc: 0.7758 - lr: 0.0100\n",
      "Epoch 24/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.7328 - acc: 0.8438 - auc: 0.8998\n",
      "Epoch 24: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6673 - acc: 0.8261 - auc: 0.9140 - val_loss: 0.4506 - val_acc: 0.8325 - val_auc: 0.7774 - lr: 0.0100\n",
      "Epoch 25/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6671 - acc: 0.8471 - auc: 0.9142\n",
      "Epoch 25: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6680 - acc: 0.8422 - auc: 0.9114 - val_loss: 0.4817 - val_acc: 0.8034 - val_auc: 0.7726 - lr: 0.0100\n",
      "Epoch 26/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.6510 - acc: 0.8395 - auc: 0.9198\n",
      "Epoch 26: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6386 - acc: 0.8384 - auc: 0.9236 - val_loss: 0.4609 - val_acc: 0.8180 - val_auc: 0.7775 - lr: 0.0100\n",
      "Epoch 27/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.6459 - acc: 0.8233 - auc: 0.9179\n",
      "Epoch 27: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6545 - acc: 0.8233 - auc: 0.9119 - val_loss: 0.4559 - val_acc: 0.8058 - val_auc: 0.7730 - lr: 0.0100\n",
      "Epoch 28/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.6614 - acc: 0.8438 - auc: 0.9186\n",
      "Epoch 28: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6553 - acc: 0.8308 - auc: 0.9125 - val_loss: 0.4610 - val_acc: 0.8277 - val_auc: 0.7747 - lr: 0.0100\n",
      "Epoch 29/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6359 - acc: 0.8417 - auc: 0.9200\n",
      "Epoch 29: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6404 - acc: 0.8384 - auc: 0.9178 - val_loss: 0.5134 - val_acc: 0.7840 - val_auc: 0.7791 - lr: 0.0100\n",
      "Epoch 30/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6500 - acc: 0.8294 - auc: 0.9088\n",
      "Epoch 30: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6641 - acc: 0.8308 - auc: 0.9087 - val_loss: 0.5325 - val_acc: 0.7718 - val_auc: 0.7787 - lr: 0.0100\n",
      "Epoch 31/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.6346 - acc: 0.8422 - auc: 0.9184\n",
      "Epoch 31: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.6346 - acc: 0.8422 - auc: 0.9184 - val_loss: 0.5563 - val_acc: 0.7451 - val_auc: 0.7760 - lr: 0.0100\n",
      "Epoch 32/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6333 - acc: 0.8174 - auc: 0.9131\n",
      "Epoch 32: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6488 - acc: 0.8147 - auc: 0.9083 - val_loss: 0.4780 - val_acc: 0.8180 - val_auc: 0.7742 - lr: 0.0100\n",
      "Epoch 33/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6418 - acc: 0.8156 - auc: 0.9139\n",
      "Epoch 33: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.6466 - acc: 0.8166 - auc: 0.9118 - val_loss: 0.4707 - val_acc: 0.8131 - val_auc: 0.7756 - lr: 0.0100\n",
      "Epoch 34/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.6384 - acc: 0.8311 - auc: 0.9147\n",
      "Epoch 34: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6313 - acc: 0.8308 - auc: 0.9158 - val_loss: 0.4761 - val_acc: 0.8204 - val_auc: 0.7693 - lr: 0.0100\n",
      "Epoch 35/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.6097 - acc: 0.8382 - auc: 0.9285\n",
      "Epoch 35: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6209 - acc: 0.8318 - auc: 0.9222 - val_loss: 0.4570 - val_acc: 0.8374 - val_auc: 0.7697 - lr: 0.0100\n",
      "Epoch 36/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6103 - acc: 0.8438 - auc: 0.9268\n",
      "Epoch 36: val_auc did not improve from 0.78193\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6085 - acc: 0.8403 - auc: 0.9249 - val_loss: 0.5005 - val_acc: 0.8034 - val_auc: 0.7728 - lr: 0.0100\n",
      "Epoch 37/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5781 - acc: 0.8377 - auc: 0.9351\n",
      "Epoch 37: val_auc improved from 0.78193 to 0.78232, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5904 - acc: 0.8384 - auc: 0.9282 - val_loss: 0.4710 - val_acc: 0.8155 - val_auc: 0.7823 - lr: 0.0100\n",
      "Epoch 38/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5758 - acc: 0.8580 - auc: 0.9314\n",
      "Epoch 38: val_auc did not improve from 0.78232\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5977 - acc: 0.8507 - auc: 0.9292 - val_loss: 0.4761 - val_acc: 0.8155 - val_auc: 0.7707 - lr: 0.0100\n",
      "Epoch 39/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5756 - acc: 0.8580 - auc: 0.9291\n",
      "Epoch 39: val_auc did not improve from 0.78232\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6136 - acc: 0.8526 - auc: 0.9247 - val_loss: 0.5099 - val_acc: 0.7961 - val_auc: 0.7741 - lr: 0.0100\n",
      "Epoch 40/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6002 - acc: 0.8240 - auc: 0.9247\n",
      "Epoch 40: val_auc did not improve from 0.78232\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5948 - acc: 0.8289 - auc: 0.9250 - val_loss: 0.4676 - val_acc: 0.8252 - val_auc: 0.7610 - lr: 0.0100\n",
      "Epoch 41/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.6114 - acc: 0.8521 - auc: 0.9263\n",
      "Epoch 41: val_auc improved from 0.78232 to 0.78268, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6043 - acc: 0.8516 - auc: 0.9255 - val_loss: 0.4983 - val_acc: 0.8058 - val_auc: 0.7827 - lr: 0.0100\n",
      "Epoch 42/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5709 - acc: 0.8529 - auc: 0.9372\n",
      "Epoch 42: val_auc improved from 0.78268 to 0.78436, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5730 - acc: 0.8497 - auc: 0.9331 - val_loss: 0.4293 - val_acc: 0.8252 - val_auc: 0.7844 - lr: 0.0100\n",
      "Epoch 43/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5910 - acc: 0.8570 - auc: 0.9287\n",
      "Epoch 43: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5825 - acc: 0.8535 - auc: 0.9306 - val_loss: 0.5107 - val_acc: 0.7961 - val_auc: 0.7808 - lr: 0.0100\n",
      "Epoch 44/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.6200 - acc: 0.8111 - auc: 0.9122\n",
      "Epoch 44: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6029 - acc: 0.8327 - auc: 0.9225 - val_loss: 0.4643 - val_acc: 0.8252 - val_auc: 0.7781 - lr: 0.0100\n",
      "Epoch 45/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5793 - acc: 0.8371 - auc: 0.9295\n",
      "Epoch 45: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5665 - acc: 0.8422 - auc: 0.9315 - val_loss: 0.4504 - val_acc: 0.8495 - val_auc: 0.7706 - lr: 0.0100\n",
      "Epoch 46/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5442 - acc: 0.8750 - auc: 0.9399\n",
      "Epoch 46: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5804 - acc: 0.8620 - auc: 0.9320 - val_loss: 0.5476 - val_acc: 0.7621 - val_auc: 0.7788 - lr: 0.0100\n",
      "Epoch 47/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.6094 - acc: 0.7956 - auc: 0.9192\n",
      "Epoch 47: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6014 - acc: 0.8110 - auc: 0.9201 - val_loss: 0.4267 - val_acc: 0.8301 - val_auc: 0.7805 - lr: 0.0100\n",
      "Epoch 48/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5794 - acc: 0.8197 - auc: 0.9220\n",
      "Epoch 48: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6087 - acc: 0.8289 - auc: 0.9173 - val_loss: 0.4354 - val_acc: 0.8374 - val_auc: 0.7776 - lr: 0.0100\n",
      "Epoch 49/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5561 - acc: 0.8479 - auc: 0.9324\n",
      "Epoch 49: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5623 - acc: 0.8544 - auc: 0.9336 - val_loss: 0.4825 - val_acc: 0.8131 - val_auc: 0.7787 - lr: 0.0100\n",
      "Epoch 50/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5909 - acc: 0.8068 - auc: 0.9214\n",
      "Epoch 50: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5995 - acc: 0.8214 - auc: 0.9194 - val_loss: 0.4678 - val_acc: 0.8107 - val_auc: 0.7756 - lr: 0.0100\n",
      "Epoch 51/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5537 - acc: 0.8401 - auc: 0.9367\n",
      "Epoch 51: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5705 - acc: 0.8403 - auc: 0.9295 - val_loss: 0.4385 - val_acc: 0.8592 - val_auc: 0.7652 - lr: 0.0100\n",
      "Epoch 52/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5763 - acc: 0.8620 - auc: 0.9382\n",
      "Epoch 52: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5831 - acc: 0.8412 - auc: 0.9262 - val_loss: 0.5062 - val_acc: 0.8083 - val_auc: 0.7728 - lr: 0.0100\n",
      "Epoch 53/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5584 - acc: 0.8450 - auc: 0.9330\n",
      "Epoch 53: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5584 - acc: 0.8450 - auc: 0.9330 - val_loss: 0.4994 - val_acc: 0.8252 - val_auc: 0.7751 - lr: 0.0100\n",
      "Epoch 54/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5529 - acc: 0.8311 - auc: 0.9309\n",
      "Epoch 54: val_auc did not improve from 0.78436\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5617 - acc: 0.8318 - auc: 0.9300 - val_loss: 0.4760 - val_acc: 0.8252 - val_auc: 0.7839 - lr: 0.0100\n",
      "Epoch 55/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5646 - acc: 0.8385 - auc: 0.9280\n",
      "Epoch 55: val_auc improved from 0.78436 to 0.78451, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5560 - acc: 0.8393 - auc: 0.9312 - val_loss: 0.4619 - val_acc: 0.8350 - val_auc: 0.7845 - lr: 0.0100\n",
      "Epoch 56/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5048 - acc: 0.8372 - auc: 0.9476\n",
      "Epoch 56: val_auc did not improve from 0.78451\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5413 - acc: 0.8365 - auc: 0.9357 - val_loss: 0.4375 - val_acc: 0.8471 - val_auc: 0.7789 - lr: 0.0100\n",
      "Epoch 57/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5707 - acc: 0.8466 - auc: 0.9287\n",
      "Epoch 57: val_auc did not improve from 0.78451\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5637 - acc: 0.8469 - auc: 0.9317 - val_loss: 0.5110 - val_acc: 0.8083 - val_auc: 0.7750 - lr: 0.0100\n",
      "Epoch 58/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5220 - acc: 0.8636 - auc: 0.9462\n",
      "Epoch 58: val_auc did not improve from 0.78451\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5386 - acc: 0.8497 - auc: 0.9376 - val_loss: 0.4894 - val_acc: 0.8180 - val_auc: 0.7835 - lr: 0.0100\n",
      "Epoch 59/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5650 - acc: 0.8415 - auc: 0.9314\n",
      "Epoch 59: val_auc did not improve from 0.78451\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.5393 - acc: 0.8497 - auc: 0.9389 - val_loss: 0.5181 - val_acc: 0.8083 - val_auc: 0.7832 - lr: 0.0100\n",
      "Epoch 60/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5632 - acc: 0.8292 - auc: 0.9315\n",
      "Epoch 60: val_auc improved from 0.78451 to 0.78738, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.5551 - acc: 0.8336 - auc: 0.9314 - val_loss: 0.4291 - val_acc: 0.8471 - val_auc: 0.7874 - lr: 0.0100\n",
      "Epoch 61/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5331 - acc: 0.8620 - auc: 0.9402\n",
      "Epoch 61: val_auc improved from 0.78738 to 0.79038, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5430 - acc: 0.8526 - auc: 0.9368 - val_loss: 0.4836 - val_acc: 0.8204 - val_auc: 0.7904 - lr: 0.0100\n",
      "Epoch 62/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5219 - acc: 0.8494 - auc: 0.9438\n",
      "Epoch 62: val_auc did not improve from 0.79038\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5236 - acc: 0.8403 - auc: 0.9373 - val_loss: 0.4243 - val_acc: 0.8519 - val_auc: 0.7841 - lr: 0.0100\n",
      "Epoch 63/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5408 - acc: 0.8789 - auc: 0.9412\n",
      "Epoch 63: val_auc did not improve from 0.79038\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5281 - acc: 0.8800 - auc: 0.9443 - val_loss: 0.5639 - val_acc: 0.7864 - val_auc: 0.7756 - lr: 0.0100\n",
      "Epoch 64/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5341 - acc: 0.8111 - auc: 0.9361\n",
      "Epoch 64: val_auc improved from 0.79038 to 0.79500, saving model to model\\model.keras\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.5580 - acc: 0.8318 - auc: 0.9287 - val_loss: 0.4967 - val_acc: 0.8180 - val_auc: 0.7950 - lr: 0.0100\n",
      "Epoch 65/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5397 - acc: 0.8320 - auc: 0.9377\n",
      "Epoch 65: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5285 - acc: 0.8374 - auc: 0.9381 - val_loss: 0.4539 - val_acc: 0.8350 - val_auc: 0.7901 - lr: 0.0100\n",
      "Epoch 66/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5687 - acc: 0.8426 - auc: 0.9315\n",
      "Epoch 66: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.5438 - acc: 0.8478 - auc: 0.9350 - val_loss: 0.4456 - val_acc: 0.8398 - val_auc: 0.7860 - lr: 0.0100\n",
      "Epoch 67/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5470 - acc: 0.8464 - auc: 0.9407\n",
      "Epoch 67: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.5270 - acc: 0.8450 - auc: 0.9405 - val_loss: 0.4190 - val_acc: 0.8495 - val_auc: 0.7789 - lr: 0.0100\n",
      "Epoch 68/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5517 - acc: 0.8583 - auc: 0.9337\n",
      "Epoch 68: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5405 - acc: 0.8535 - auc: 0.9354 - val_loss: 0.5150 - val_acc: 0.8083 - val_auc: 0.7764 - lr: 0.0100\n",
      "Epoch 69/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5360 - acc: 0.8438 - auc: 0.9364\n",
      "Epoch 69: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5382 - acc: 0.8459 - auc: 0.9355 - val_loss: 0.4533 - val_acc: 0.8374 - val_auc: 0.7810 - lr: 0.0100\n",
      "Epoch 70/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.5472 - acc: 0.8516 - auc: 0.9355\n",
      "Epoch 70: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5472 - acc: 0.8516 - auc: 0.9355 - val_loss: 0.4712 - val_acc: 0.8325 - val_auc: 0.7774 - lr: 0.0100\n",
      "Epoch 71/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5447 - acc: 0.8377 - auc: 0.9347\n",
      "Epoch 71: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5292 - acc: 0.8497 - auc: 0.9390 - val_loss: 0.4138 - val_acc: 0.8519 - val_auc: 0.7866 - lr: 0.0100\n",
      "Epoch 72/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5146 - acc: 0.8562 - auc: 0.9429\n",
      "Epoch 72: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5175 - acc: 0.8601 - auc: 0.9433 - val_loss: 0.4808 - val_acc: 0.8277 - val_auc: 0.7801 - lr: 0.0100\n",
      "Epoch 73/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.4639 - acc: 0.8484 - auc: 0.9575\n",
      "Epoch 73: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5212 - acc: 0.8346 - auc: 0.9390 - val_loss: 0.4353 - val_acc: 0.8471 - val_auc: 0.7834 - lr: 0.0100\n",
      "Epoch 74/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5033 - acc: 0.8583 - auc: 0.9469\n",
      "Epoch 74: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5191 - acc: 0.8488 - auc: 0.9418 - val_loss: 0.5136 - val_acc: 0.8228 - val_auc: 0.7807 - lr: 0.0100\n",
      "Epoch 75/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4767 - acc: 0.8594 - auc: 0.9511\n",
      "Epoch 75: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5050 - acc: 0.8554 - auc: 0.9447 - val_loss: 0.4802 - val_acc: 0.8325 - val_auc: 0.7835 - lr: 0.0100\n",
      "Epoch 76/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5141 - acc: 0.8462 - auc: 0.9428\n",
      "Epoch 76: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5181 - acc: 0.8535 - auc: 0.9423 - val_loss: 0.4592 - val_acc: 0.8325 - val_auc: 0.7833 - lr: 0.0100\n",
      "Epoch 77/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.5341 - acc: 0.8438 - auc: 0.9381\n",
      "Epoch 77: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5251 - acc: 0.8459 - auc: 0.9386 - val_loss: 0.4580 - val_acc: 0.8471 - val_auc: 0.7825 - lr: 0.0100\n",
      "Epoch 78/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.5241 - acc: 0.8549 - auc: 0.9414\n",
      "Epoch 78: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5422 - acc: 0.8422 - auc: 0.9349 - val_loss: 0.5000 - val_acc: 0.8034 - val_auc: 0.7835 - lr: 0.0100\n",
      "Epoch 79/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.5181 - acc: 0.8458 - auc: 0.9420\n",
      "Epoch 79: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5269 - acc: 0.8440 - auc: 0.9387 - val_loss: 0.5268 - val_acc: 0.8010 - val_auc: 0.7829 - lr: 0.0100\n",
      "Epoch 80/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4975 - acc: 0.8381 - auc: 0.9418\n",
      "Epoch 80: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5273 - acc: 0.8355 - auc: 0.9362 - val_loss: 0.4739 - val_acc: 0.8277 - val_auc: 0.7817 - lr: 0.0100\n",
      "Epoch 81/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4883 - acc: 0.8562 - auc: 0.9453\n",
      "Epoch 81: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5112 - acc: 0.8554 - auc: 0.9412 - val_loss: 0.5032 - val_acc: 0.8083 - val_auc: 0.7810 - lr: 0.0100\n",
      "Epoch 82/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5078 - acc: 0.8324 - auc: 0.9427\n",
      "Epoch 82: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5064 - acc: 0.8346 - auc: 0.9415 - val_loss: 0.4728 - val_acc: 0.8350 - val_auc: 0.7766 - lr: 0.0100\n",
      "Epoch 83/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.5058 - acc: 0.8494 - auc: 0.9460\n",
      "Epoch 83: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5075 - acc: 0.8488 - auc: 0.9440 - val_loss: 0.4890 - val_acc: 0.8301 - val_auc: 0.7734 - lr: 0.0100\n",
      "Epoch 84/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4545 - acc: 0.8711 - auc: 0.9545\n",
      "Epoch 84: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4827 - acc: 0.8535 - auc: 0.9480 - val_loss: 0.5056 - val_acc: 0.8180 - val_auc: 0.7766 - lr: 0.0100\n",
      "Epoch 85/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4783 - acc: 0.8594 - auc: 0.9476\n",
      "Epoch 85: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4840 - acc: 0.8658 - auc: 0.9495 - val_loss: 0.5025 - val_acc: 0.8252 - val_auc: 0.7708 - lr: 0.0100\n",
      "Epoch 86/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4816 - acc: 0.8568 - auc: 0.9484\n",
      "Epoch 86: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4826 - acc: 0.8497 - auc: 0.9462 - val_loss: 0.4878 - val_acc: 0.8204 - val_auc: 0.7717 - lr: 0.0100\n",
      "Epoch 87/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4491 - acc: 0.8423 - auc: 0.9593\n",
      "Epoch 87: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4679 - acc: 0.8516 - auc: 0.9528 - val_loss: 0.4830 - val_acc: 0.8422 - val_auc: 0.7769 - lr: 0.0100\n",
      "Epoch 88/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.4699 - acc: 0.8906 - auc: 0.9408\n",
      "Epoch 88: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.4750 - acc: 0.8677 - auc: 0.9515 - val_loss: 0.5106 - val_acc: 0.8107 - val_auc: 0.7767 - lr: 0.0100\n",
      "Epoch 89/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.2043 - acc: 0.8906 - auc: 1.0000\n",
      "Epoch 89: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.4816 - acc: 0.8459 - auc: 0.9488 - val_loss: 0.5290 - val_acc: 0.8204 - val_auc: 0.7798 - lr: 0.0100\n",
      "Epoch 90/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4143 - acc: 0.8594 - auc: 0.9635\n",
      "Epoch 90: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4731 - acc: 0.8469 - auc: 0.9493 - val_loss: 0.4765 - val_acc: 0.8422 - val_auc: 0.7772 - lr: 0.0100\n",
      "Epoch 91/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4549 - acc: 0.8711 - auc: 0.9554\n",
      "Epoch 91: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4809 - acc: 0.8620 - auc: 0.9478 - val_loss: 0.4874 - val_acc: 0.8350 - val_auc: 0.7803 - lr: 0.0100\n",
      "Epoch 92/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.4909 - acc: 0.8403 - auc: 0.9447\n",
      "Epoch 92: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4943 - acc: 0.8497 - auc: 0.9454 - val_loss: 0.5000 - val_acc: 0.8301 - val_auc: 0.7751 - lr: 0.0100\n",
      "Epoch 93/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4837 - acc: 0.8510 - auc: 0.9463\n",
      "Epoch 93: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4764 - acc: 0.8544 - auc: 0.9495 - val_loss: 0.5529 - val_acc: 0.8131 - val_auc: 0.7808 - lr: 0.0100\n",
      "Epoch 94/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4848 - acc: 0.8438 - auc: 0.9444\n",
      "Epoch 94: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4775 - acc: 0.8535 - auc: 0.9483 - val_loss: 0.5334 - val_acc: 0.8277 - val_auc: 0.7873 - lr: 0.0100\n",
      "Epoch 95/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4528 - acc: 0.8707 - auc: 0.9530\n",
      "Epoch 95: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4818 - acc: 0.8648 - auc: 0.9488 - val_loss: 0.5473 - val_acc: 0.8083 - val_auc: 0.7913 - lr: 0.0100\n",
      "Epoch 96/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5220 - acc: 0.8486 - auc: 0.9411\n",
      "Epoch 96: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5192 - acc: 0.8450 - auc: 0.9381 - val_loss: 0.5574 - val_acc: 0.8180 - val_auc: 0.7779 - lr: 0.0100\n",
      "Epoch 97/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4953 - acc: 0.8685 - auc: 0.9445\n",
      "Epoch 97: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5076 - acc: 0.8620 - auc: 0.9437 - val_loss: 0.5688 - val_acc: 0.8010 - val_auc: 0.7744 - lr: 0.0100\n",
      "Epoch 98/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4858 - acc: 0.8565 - auc: 0.9517\n",
      "Epoch 98: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4997 - acc: 0.8469 - auc: 0.9439 - val_loss: 0.4588 - val_acc: 0.8544 - val_auc: 0.7744 - lr: 0.0100\n",
      "Epoch 99/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5065 - acc: 0.8738 - auc: 0.9483\n",
      "Epoch 99: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5003 - acc: 0.8620 - auc: 0.9437 - val_loss: 0.4927 - val_acc: 0.8277 - val_auc: 0.7783 - lr: 0.0100\n",
      "Epoch 100/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4749 - acc: 0.8705 - auc: 0.9522\n",
      "Epoch 100: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4863 - acc: 0.8667 - auc: 0.9509 - val_loss: 0.5258 - val_acc: 0.8034 - val_auc: 0.7841 - lr: 0.0100\n",
      "Epoch 101/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4877 - acc: 0.8503 - auc: 0.9451\n",
      "Epoch 101: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4815 - acc: 0.8526 - auc: 0.9483 - val_loss: 0.4846 - val_acc: 0.8277 - val_auc: 0.7814 - lr: 0.0100\n",
      "Epoch 102/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4806 - acc: 0.8582 - auc: 0.9474\n",
      "Epoch 102: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4806 - acc: 0.8582 - auc: 0.9474 - val_loss: 0.4682 - val_acc: 0.8447 - val_auc: 0.7789 - lr: 0.0100\n",
      "Epoch 103/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5048 - acc: 0.8377 - auc: 0.9421\n",
      "Epoch 103: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.5084 - acc: 0.8431 - auc: 0.9413 - val_loss: 0.4096 - val_acc: 0.8495 - val_auc: 0.7748 - lr: 0.0100\n",
      "Epoch 104/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.5386 - acc: 0.8555 - auc: 0.9351\n",
      "Epoch 104: val_auc did not improve from 0.79500\n",
      "\n",
      "Epoch 104: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5211 - acc: 0.8573 - auc: 0.9394 - val_loss: 0.4359 - val_acc: 0.8471 - val_auc: 0.7617 - lr: 0.0100\n",
      "Epoch 105/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.5430 - acc: 0.8715 - auc: 0.9351\n",
      "Epoch 105: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.5052 - acc: 0.8733 - auc: 0.9454 - val_loss: 0.4929 - val_acc: 0.8301 - val_auc: 0.7726 - lr: 0.0050\n",
      "Epoch 106/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.5162 - acc: 0.8558 - auc: 0.9389\n",
      "Epoch 106: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.5060 - acc: 0.8544 - auc: 0.9417 - val_loss: 0.5059 - val_acc: 0.8350 - val_auc: 0.7789 - lr: 0.0050\n",
      "Epoch 107/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4696 - acc: 0.8594 - auc: 0.9438\n",
      "Epoch 107: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4910 - acc: 0.8582 - auc: 0.9464 - val_loss: 0.5264 - val_acc: 0.8252 - val_auc: 0.7766 - lr: 0.0050\n",
      "Epoch 108/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4741 - acc: 0.8448 - auc: 0.9502\n",
      "Epoch 108: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4822 - acc: 0.8440 - auc: 0.9481 - val_loss: 0.5260 - val_acc: 0.8398 - val_auc: 0.7672 - lr: 0.0050\n",
      "Epoch 109/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4471 - acc: 0.8729 - auc: 0.9572\n",
      "Epoch 109: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4580 - acc: 0.8724 - auc: 0.9555 - val_loss: 0.5413 - val_acc: 0.8325 - val_auc: 0.7659 - lr: 0.0050\n",
      "Epoch 110/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4809 - acc: 0.8490 - auc: 0.9481\n",
      "Epoch 110: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4542 - acc: 0.8573 - auc: 0.9550 - val_loss: 0.5305 - val_acc: 0.8277 - val_auc: 0.7670 - lr: 0.0050\n",
      "Epoch 111/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4496 - acc: 0.8652 - auc: 0.9567\n",
      "Epoch 111: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4539 - acc: 0.8620 - auc: 0.9553 - val_loss: 0.5527 - val_acc: 0.8228 - val_auc: 0.7682 - lr: 0.0050\n",
      "Epoch 112/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4620 - acc: 0.8736 - auc: 0.9565\n",
      "Epoch 112: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4446 - acc: 0.8667 - auc: 0.9570 - val_loss: 0.5297 - val_acc: 0.8422 - val_auc: 0.7663 - lr: 0.0050\n",
      "Epoch 113/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4644 - acc: 0.8666 - auc: 0.9559\n",
      "Epoch 113: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4500 - acc: 0.8573 - auc: 0.9555 - val_loss: 0.5321 - val_acc: 0.8422 - val_auc: 0.7673 - lr: 0.0050\n",
      "Epoch 114/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4603 - acc: 0.8764 - auc: 0.9536\n",
      "Epoch 114: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4510 - acc: 0.8733 - auc: 0.9549 - val_loss: 0.5357 - val_acc: 0.8422 - val_auc: 0.7684 - lr: 0.0050\n",
      "Epoch 115/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4412 - acc: 0.8629 - auc: 0.9570\n",
      "Epoch 115: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4412 - acc: 0.8629 - auc: 0.9570 - val_loss: 0.5392 - val_acc: 0.8350 - val_auc: 0.7664 - lr: 0.0050\n",
      "Epoch 116/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4669 - acc: 0.8555 - auc: 0.9501\n",
      "Epoch 116: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4623 - acc: 0.8573 - auc: 0.9514 - val_loss: 0.5274 - val_acc: 0.8350 - val_auc: 0.7671 - lr: 0.0050\n",
      "Epoch 117/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4400 - acc: 0.8677 - auc: 0.9570\n",
      "Epoch 117: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4416 - acc: 0.8705 - auc: 0.9582 - val_loss: 0.5249 - val_acc: 0.8301 - val_auc: 0.7705 - lr: 0.0050\n",
      "Epoch 118/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4791 - acc: 0.8555 - auc: 0.9510\n",
      "Epoch 118: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4545 - acc: 0.8554 - auc: 0.9534 - val_loss: 0.5041 - val_acc: 0.8495 - val_auc: 0.7719 - lr: 0.0050\n",
      "Epoch 119/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4427 - acc: 0.8818 - auc: 0.9570\n",
      "Epoch 119: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4439 - acc: 0.8809 - auc: 0.9579 - val_loss: 0.5269 - val_acc: 0.8374 - val_auc: 0.7704 - lr: 0.0050\n",
      "Epoch 120/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3975 - acc: 0.8920 - auc: 0.9627\n",
      "Epoch 120: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4324 - acc: 0.8819 - auc: 0.9593 - val_loss: 0.5526 - val_acc: 0.8350 - val_auc: 0.7732 - lr: 0.0050\n",
      "Epoch 121/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4346 - acc: 0.8555 - auc: 0.9589\n",
      "Epoch 121: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4354 - acc: 0.8507 - auc: 0.9574 - val_loss: 0.5372 - val_acc: 0.8374 - val_auc: 0.7685 - lr: 0.0050\n",
      "Epoch 122/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4346 - acc: 0.8740 - auc: 0.9576\n",
      "Epoch 122: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4327 - acc: 0.8715 - auc: 0.9581 - val_loss: 0.5203 - val_acc: 0.8398 - val_auc: 0.7652 - lr: 0.0050\n",
      "Epoch 123/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4031 - acc: 0.8750 - auc: 0.9615\n",
      "Epoch 123: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4296 - acc: 0.8790 - auc: 0.9595 - val_loss: 0.5277 - val_acc: 0.8398 - val_auc: 0.7735 - lr: 0.0050\n",
      "Epoch 124/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4230 - acc: 0.8667 - auc: 0.9619\n",
      "Epoch 124: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4234 - acc: 0.8667 - auc: 0.9614 - val_loss: 0.5317 - val_acc: 0.8398 - val_auc: 0.7669 - lr: 0.0050\n",
      "Epoch 125/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4464 - acc: 0.8605 - auc: 0.9544\n",
      "Epoch 125: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4258 - acc: 0.8686 - auc: 0.9593 - val_loss: 0.5275 - val_acc: 0.8495 - val_auc: 0.7665 - lr: 0.0050\n",
      "Epoch 126/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4182 - acc: 0.8678 - auc: 0.9605\n",
      "Epoch 126: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4248 - acc: 0.8696 - auc: 0.9596 - val_loss: 0.5287 - val_acc: 0.8422 - val_auc: 0.7676 - lr: 0.0050\n",
      "Epoch 127/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4436 - acc: 0.8740 - auc: 0.9579\n",
      "Epoch 127: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.4417 - acc: 0.8724 - auc: 0.9580 - val_loss: 0.5483 - val_acc: 0.8252 - val_auc: 0.7726 - lr: 0.0050\n",
      "Epoch 128/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4395 - acc: 0.8639 - auc: 0.9563\n",
      "Epoch 128: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4395 - acc: 0.8639 - auc: 0.9563 - val_loss: 0.5066 - val_acc: 0.8398 - val_auc: 0.7601 - lr: 0.0050\n",
      "Epoch 129/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4094 - acc: 0.8724 - auc: 0.9649\n",
      "Epoch 129: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4354 - acc: 0.8658 - auc: 0.9587 - val_loss: 0.5191 - val_acc: 0.8398 - val_auc: 0.7619 - lr: 0.0050\n",
      "Epoch 130/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.4181 - acc: 0.8875 - auc: 0.9657\n",
      "Epoch 130: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4360 - acc: 0.8743 - auc: 0.9584 - val_loss: 0.5223 - val_acc: 0.8422 - val_auc: 0.7704 - lr: 0.0050\n",
      "Epoch 131/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4015 - acc: 0.8625 - auc: 0.9633\n",
      "Epoch 131: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4353 - acc: 0.8573 - auc: 0.9560 - val_loss: 0.5256 - val_acc: 0.8374 - val_auc: 0.7681 - lr: 0.0050\n",
      "Epoch 132/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.4327 - acc: 0.8734 - auc: 0.9607\n",
      "Epoch 132: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4343 - acc: 0.8733 - auc: 0.9581 - val_loss: 0.5262 - val_acc: 0.8422 - val_auc: 0.7645 - lr: 0.0050\n",
      "Epoch 133/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4267 - acc: 0.8760 - auc: 0.9605\n",
      "Epoch 133: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4326 - acc: 0.8715 - auc: 0.9584 - val_loss: 0.5617 - val_acc: 0.8277 - val_auc: 0.7635 - lr: 0.0050\n",
      "Epoch 134/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4277 - acc: 0.8620 - auc: 0.9596\n",
      "Epoch 134: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4277 - acc: 0.8620 - auc: 0.9596 - val_loss: 0.5283 - val_acc: 0.8325 - val_auc: 0.7580 - lr: 0.0050\n",
      "Epoch 135/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4070 - acc: 0.8774 - auc: 0.9643\n",
      "Epoch 135: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4218 - acc: 0.8724 - auc: 0.9606 - val_loss: 0.5402 - val_acc: 0.8398 - val_auc: 0.7622 - lr: 0.0050\n",
      "Epoch 136/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4217 - acc: 0.8683 - auc: 0.9611\n",
      "Epoch 136: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4181 - acc: 0.8705 - auc: 0.9598 - val_loss: 0.5387 - val_acc: 0.8374 - val_auc: 0.7638 - lr: 0.0050\n",
      "Epoch 137/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4047 - acc: 0.8761 - auc: 0.9641\n",
      "Epoch 137: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4184 - acc: 0.8781 - auc: 0.9624 - val_loss: 0.5394 - val_acc: 0.8398 - val_auc: 0.7602 - lr: 0.0050\n",
      "Epoch 138/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4281 - acc: 0.8438 - auc: 0.9587\n",
      "Epoch 138: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4286 - acc: 0.8497 - auc: 0.9565 - val_loss: 0.5118 - val_acc: 0.8495 - val_auc: 0.7659 - lr: 0.0050\n",
      "Epoch 139/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4303 - acc: 0.8875 - auc: 0.9615\n",
      "Epoch 139: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4251 - acc: 0.8837 - auc: 0.9604 - val_loss: 0.5838 - val_acc: 0.8228 - val_auc: 0.7706 - lr: 0.0050\n",
      "Epoch 140/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4265 - acc: 0.8474 - auc: 0.9573\n",
      "Epoch 140: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4320 - acc: 0.8554 - auc: 0.9559 - val_loss: 0.5399 - val_acc: 0.8350 - val_auc: 0.7659 - lr: 0.0050\n",
      "Epoch 141/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4107 - acc: 0.8698 - auc: 0.9630\n",
      "Epoch 141: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4203 - acc: 0.8724 - auc: 0.9618 - val_loss: 0.5486 - val_acc: 0.8374 - val_auc: 0.7629 - lr: 0.0050\n",
      "Epoch 142/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3612 - acc: 0.8774 - auc: 0.9709\n",
      "Epoch 142: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4120 - acc: 0.8771 - auc: 0.9615 - val_loss: 0.5319 - val_acc: 0.8398 - val_auc: 0.7628 - lr: 0.0050\n",
      "Epoch 143/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4228 - acc: 0.8789 - auc: 0.9641\n",
      "Epoch 143: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4259 - acc: 0.8629 - auc: 0.9593 - val_loss: 0.5588 - val_acc: 0.8374 - val_auc: 0.7575 - lr: 0.0050\n",
      "Epoch 144/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4105 - acc: 0.8835 - auc: 0.9663\n",
      "Epoch 144: val_auc did not improve from 0.79500\n",
      "\n",
      "Epoch 144: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4206 - acc: 0.8800 - auc: 0.9615 - val_loss: 0.5075 - val_acc: 0.8398 - val_auc: 0.7641 - lr: 0.0050\n",
      "Epoch 145/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4297 - acc: 0.8772 - auc: 0.9589\n",
      "Epoch 145: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4100 - acc: 0.8856 - auc: 0.9633 - val_loss: 0.5235 - val_acc: 0.8398 - val_auc: 0.7679 - lr: 0.0025\n",
      "Epoch 146/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3983 - acc: 0.8892 - auc: 0.9646\n",
      "Epoch 146: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4069 - acc: 0.8800 - auc: 0.9640 - val_loss: 0.5461 - val_acc: 0.8422 - val_auc: 0.7685 - lr: 0.0025\n",
      "Epoch 147/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3995 - acc: 0.8750 - auc: 0.9638\n",
      "Epoch 147: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4004 - acc: 0.8752 - auc: 0.9641 - val_loss: 0.5388 - val_acc: 0.8398 - val_auc: 0.7705 - lr: 0.0025\n",
      "Epoch 148/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4110 - acc: 0.8726 - auc: 0.9629\n",
      "Epoch 148: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4104 - acc: 0.8752 - auc: 0.9629 - val_loss: 0.5462 - val_acc: 0.8398 - val_auc: 0.7689 - lr: 0.0025\n",
      "Epoch 149/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3965 - acc: 0.8760 - auc: 0.9655\n",
      "Epoch 149: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4043 - acc: 0.8724 - auc: 0.9639 - val_loss: 0.5373 - val_acc: 0.8422 - val_auc: 0.7693 - lr: 0.0025\n",
      "Epoch 150/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4231 - acc: 0.8726 - auc: 0.9584\n",
      "Epoch 150: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4118 - acc: 0.8743 - auc: 0.9613 - val_loss: 0.5533 - val_acc: 0.8350 - val_auc: 0.7681 - lr: 0.0025\n",
      "Epoch 151/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4053 - acc: 0.8650 - auc: 0.9621\n",
      "Epoch 151: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4016 - acc: 0.8686 - auc: 0.9637 - val_loss: 0.5385 - val_acc: 0.8422 - val_auc: 0.7682 - lr: 0.0025\n",
      "Epoch 152/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4214 - acc: 0.8646 - auc: 0.9605\n",
      "Epoch 152: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4042 - acc: 0.8715 - auc: 0.9636 - val_loss: 0.5465 - val_acc: 0.8398 - val_auc: 0.7662 - lr: 0.0025\n",
      "Epoch 153/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3767 - acc: 0.8949 - auc: 0.9687\n",
      "Epoch 153: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4045 - acc: 0.8743 - auc: 0.9617 - val_loss: 0.5426 - val_acc: 0.8398 - val_auc: 0.7622 - lr: 0.0025\n",
      "Epoch 154/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3890 - acc: 0.8865 - auc: 0.9672\n",
      "Epoch 154: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3913 - acc: 0.8866 - auc: 0.9664 - val_loss: 0.5338 - val_acc: 0.8398 - val_auc: 0.7683 - lr: 0.0025\n",
      "Epoch 155/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3788 - acc: 0.8781 - auc: 0.9657\n",
      "Epoch 155: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4069 - acc: 0.8762 - auc: 0.9625 - val_loss: 0.5304 - val_acc: 0.8398 - val_auc: 0.7677 - lr: 0.0025\n",
      "Epoch 156/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4009 - acc: 0.8726 - auc: 0.9627\n",
      "Epoch 156: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4098 - acc: 0.8724 - auc: 0.9604 - val_loss: 0.5318 - val_acc: 0.8422 - val_auc: 0.7660 - lr: 0.0025\n",
      "Epoch 157/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3841 - acc: 0.8750 - auc: 0.9667\n",
      "Epoch 157: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4090 - acc: 0.8762 - auc: 0.9633 - val_loss: 0.5559 - val_acc: 0.8350 - val_auc: 0.7647 - lr: 0.0025\n",
      "Epoch 158/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.4024 - acc: 0.8847 - auc: 0.9648\n",
      "Epoch 158: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4024 - acc: 0.8847 - auc: 0.9648 - val_loss: 0.5561 - val_acc: 0.8325 - val_auc: 0.7587 - lr: 0.0025\n",
      "Epoch 159/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3984 - acc: 0.8730 - auc: 0.9653\n",
      "Epoch 159: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3964 - acc: 0.8733 - auc: 0.9654 - val_loss: 0.5626 - val_acc: 0.8350 - val_auc: 0.7586 - lr: 0.0025\n",
      "Epoch 160/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3895 - acc: 0.8906 - auc: 0.9680\n",
      "Epoch 160: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.4000 - acc: 0.8771 - auc: 0.9644 - val_loss: 0.5668 - val_acc: 0.8325 - val_auc: 0.7570 - lr: 0.0025\n",
      "Epoch 161/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3970 - acc: 0.8894 - auc: 0.9633\n",
      "Epoch 161: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4028 - acc: 0.8819 - auc: 0.9634 - val_loss: 0.5640 - val_acc: 0.8325 - val_auc: 0.7569 - lr: 0.0025\n",
      "Epoch 162/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.4188 - acc: 0.8641 - auc: 0.9605\n",
      "Epoch 162: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4042 - acc: 0.8696 - auc: 0.9619 - val_loss: 0.5570 - val_acc: 0.8350 - val_auc: 0.7594 - lr: 0.0025\n",
      "Epoch 163/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3878 - acc: 0.8781 - auc: 0.9625\n",
      "Epoch 163: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3968 - acc: 0.8828 - auc: 0.9650 - val_loss: 0.5609 - val_acc: 0.8374 - val_auc: 0.7601 - lr: 0.0025\n",
      "Epoch 164/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4080 - acc: 0.8711 - auc: 0.9625\n",
      "Epoch 164: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4080 - acc: 0.8611 - auc: 0.9625 - val_loss: 0.5592 - val_acc: 0.8350 - val_auc: 0.7603 - lr: 0.0025\n",
      "Epoch 165/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3974 - acc: 0.8841 - auc: 0.9655\n",
      "Epoch 165: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4038 - acc: 0.8819 - auc: 0.9634 - val_loss: 0.5579 - val_acc: 0.8398 - val_auc: 0.7612 - lr: 0.0025\n",
      "Epoch 166/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3958 - acc: 0.8779 - auc: 0.9624\n",
      "Epoch 166: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4035 - acc: 0.8752 - auc: 0.9612 - val_loss: 0.5768 - val_acc: 0.8447 - val_auc: 0.7594 - lr: 0.0025\n",
      "Epoch 167/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4053 - acc: 0.8763 - auc: 0.9645\n",
      "Epoch 167: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4050 - acc: 0.8762 - auc: 0.9636 - val_loss: 0.5729 - val_acc: 0.8350 - val_auc: 0.7615 - lr: 0.0025\n",
      "Epoch 168/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3740 - acc: 0.8835 - auc: 0.9695\n",
      "Epoch 168: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3992 - acc: 0.8790 - auc: 0.9636 - val_loss: 0.5624 - val_acc: 0.8422 - val_auc: 0.7632 - lr: 0.0025\n",
      "Epoch 169/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4064 - acc: 0.8781 - auc: 0.9634\n",
      "Epoch 169: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4003 - acc: 0.8790 - auc: 0.9648 - val_loss: 0.5616 - val_acc: 0.8398 - val_auc: 0.7643 - lr: 0.0025\n",
      "Epoch 170/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4007 - acc: 0.8691 - auc: 0.9622\n",
      "Epoch 170: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3977 - acc: 0.8715 - auc: 0.9627 - val_loss: 0.5564 - val_acc: 0.8447 - val_auc: 0.7670 - lr: 0.0025\n",
      "Epoch 171/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4148 - acc: 0.8750 - auc: 0.9619\n",
      "Epoch 171: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.4007 - acc: 0.8781 - auc: 0.9638 - val_loss: 0.5593 - val_acc: 0.8422 - val_auc: 0.7642 - lr: 0.0025\n",
      "Epoch 172/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.6095 - acc: 0.8594 - auc: 0.9556\n",
      "Epoch 172: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3962 - acc: 0.8809 - auc: 0.9645 - val_loss: 0.5557 - val_acc: 0.8398 - val_auc: 0.7648 - lr: 0.0025\n",
      "Epoch 173/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.4019 - acc: 0.8779 - auc: 0.9647\n",
      "Epoch 173: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3945 - acc: 0.8800 - auc: 0.9662 - val_loss: 0.5682 - val_acc: 0.8398 - val_auc: 0.7691 - lr: 0.0025\n",
      "Epoch 174/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.4033 - acc: 0.8750 - auc: 0.9626\n",
      "Epoch 174: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3999 - acc: 0.8771 - auc: 0.9629 - val_loss: 0.5680 - val_acc: 0.8398 - val_auc: 0.7691 - lr: 0.0025\n",
      "Epoch 175/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4167 - acc: 0.8650 - auc: 0.9593\n",
      "Epoch 175: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3972 - acc: 0.8762 - auc: 0.9644 - val_loss: 0.5517 - val_acc: 0.8398 - val_auc: 0.7694 - lr: 0.0025\n",
      "Epoch 176/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3928 - acc: 0.8932 - auc: 0.9687\n",
      "Epoch 176: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.4015 - acc: 0.8819 - auc: 0.9629 - val_loss: 0.5639 - val_acc: 0.8398 - val_auc: 0.7690 - lr: 0.0025\n",
      "Epoch 177/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3985 - acc: 0.8781 - auc: 0.9651\n",
      "Epoch 177: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3891 - acc: 0.8790 - auc: 0.9662 - val_loss: 0.5566 - val_acc: 0.8398 - val_auc: 0.7685 - lr: 0.0025\n",
      "Epoch 178/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4009 - acc: 0.8778 - auc: 0.9627\n",
      "Epoch 178: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.4020 - acc: 0.8819 - auc: 0.9630 - val_loss: 0.5413 - val_acc: 0.8374 - val_auc: 0.7660 - lr: 0.0025\n",
      "Epoch 179/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3604 - acc: 0.8892 - auc: 0.9712\n",
      "Epoch 179: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3927 - acc: 0.8752 - auc: 0.9652 - val_loss: 0.5498 - val_acc: 0.8398 - val_auc: 0.7681 - lr: 0.0025\n",
      "Epoch 180/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3922 - acc: 0.8651 - auc: 0.9634\n",
      "Epoch 180: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3876 - acc: 0.8743 - auc: 0.9662 - val_loss: 0.5537 - val_acc: 0.8374 - val_auc: 0.7702 - lr: 0.0025\n",
      "Epoch 181/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4011 - acc: 0.8864 - auc: 0.9663\n",
      "Epoch 181: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3908 - acc: 0.8828 - auc: 0.9653 - val_loss: 0.5513 - val_acc: 0.8374 - val_auc: 0.7723 - lr: 0.0025\n",
      "Epoch 182/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4018 - acc: 0.8878 - auc: 0.9677\n",
      "Epoch 182: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3964 - acc: 0.8819 - auc: 0.9650 - val_loss: 0.5521 - val_acc: 0.8398 - val_auc: 0.7675 - lr: 0.0025\n",
      "Epoch 183/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4011 - acc: 0.8892 - auc: 0.9622\n",
      "Epoch 183: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3916 - acc: 0.8922 - auc: 0.9655 - val_loss: 0.5547 - val_acc: 0.8374 - val_auc: 0.7675 - lr: 0.0025\n",
      "Epoch 184/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3936 - acc: 0.8685 - auc: 0.9664\n",
      "Epoch 184: val_auc did not improve from 0.79500\n",
      "\n",
      "Epoch 184: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3941 - acc: 0.8715 - auc: 0.9642 - val_loss: 0.5507 - val_acc: 0.8398 - val_auc: 0.7689 - lr: 0.0025\n",
      "Epoch 185/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4308 - acc: 0.8722 - auc: 0.9595\n",
      "Epoch 185: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3910 - acc: 0.8837 - auc: 0.9652 - val_loss: 0.5494 - val_acc: 0.8374 - val_auc: 0.7737 - lr: 0.0012\n",
      "Epoch 186/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3845 - acc: 0.8834 - auc: 0.9666\n",
      "Epoch 186: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3883 - acc: 0.8866 - auc: 0.9665 - val_loss: 0.5484 - val_acc: 0.8350 - val_auc: 0.7699 - lr: 0.0012\n",
      "Epoch 187/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3854 - acc: 0.8818 - auc: 0.9654\n",
      "Epoch 187: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3889 - acc: 0.8828 - auc: 0.9651 - val_loss: 0.5503 - val_acc: 0.8374 - val_auc: 0.7666 - lr: 0.0012\n",
      "Epoch 188/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3776 - acc: 0.8793 - auc: 0.9675\n",
      "Epoch 188: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3865 - acc: 0.8781 - auc: 0.9660 - val_loss: 0.5449 - val_acc: 0.8398 - val_auc: 0.7663 - lr: 0.0012\n",
      "Epoch 189/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3866 - acc: 0.8823 - auc: 0.9660\n",
      "Epoch 189: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3907 - acc: 0.8828 - auc: 0.9652 - val_loss: 0.5465 - val_acc: 0.8374 - val_auc: 0.7667 - lr: 0.0012\n",
      "Epoch 190/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.2941 - acc: 0.9531 - auc: 0.9695\n",
      "Epoch 190: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.4036 - acc: 0.8819 - auc: 0.9637 - val_loss: 0.5479 - val_acc: 0.8350 - val_auc: 0.7639 - lr: 0.0012\n",
      "Epoch 191/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4006 - acc: 0.8806 - auc: 0.9630\n",
      "Epoch 191: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3921 - acc: 0.8837 - auc: 0.9642 - val_loss: 0.5510 - val_acc: 0.8325 - val_auc: 0.7636 - lr: 0.0012\n",
      "Epoch 192/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3849 - acc: 0.8818 - auc: 0.9654\n",
      "Epoch 192: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3891 - acc: 0.8790 - auc: 0.9640 - val_loss: 0.5452 - val_acc: 0.8350 - val_auc: 0.7661 - lr: 0.0012\n",
      "Epoch 193/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3813 - acc: 0.8927 - auc: 0.9681\n",
      "Epoch 193: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3927 - acc: 0.8885 - auc: 0.9667 - val_loss: 0.5457 - val_acc: 0.8350 - val_auc: 0.7670 - lr: 0.0012\n",
      "Epoch 194/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3903 - acc: 0.8781 - auc: 0.9660\n",
      "Epoch 194: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3904 - acc: 0.8837 - auc: 0.9660 - val_loss: 0.5555 - val_acc: 0.8350 - val_auc: 0.7673 - lr: 0.0012\n",
      "Epoch 195/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3699 - acc: 0.8906 - auc: 0.9677\n",
      "Epoch 195: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3933 - acc: 0.8837 - auc: 0.9662 - val_loss: 0.5603 - val_acc: 0.8398 - val_auc: 0.7706 - lr: 0.0012\n",
      "Epoch 196/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.4051 - acc: 0.8817 - auc: 0.9648\n",
      "Epoch 196: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3926 - acc: 0.8837 - auc: 0.9659 - val_loss: 0.5619 - val_acc: 0.8398 - val_auc: 0.7717 - lr: 0.0012\n",
      "Epoch 197/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4016 - acc: 0.8919 - auc: 0.9632\n",
      "Epoch 197: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3913 - acc: 0.8885 - auc: 0.9657 - val_loss: 0.5596 - val_acc: 0.8374 - val_auc: 0.7713 - lr: 0.0012\n",
      "Epoch 198/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3733 - acc: 0.8797 - auc: 0.9670\n",
      "Epoch 198: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3866 - acc: 0.8847 - auc: 0.9661 - val_loss: 0.5604 - val_acc: 0.8398 - val_auc: 0.7654 - lr: 0.0012\n",
      "Epoch 199/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3996 - acc: 0.8778 - auc: 0.9639\n",
      "Epoch 199: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3896 - acc: 0.8809 - auc: 0.9662 - val_loss: 0.5632 - val_acc: 0.8398 - val_auc: 0.7706 - lr: 0.0012\n",
      "Epoch 200/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4151 - acc: 0.8789 - auc: 0.9572\n",
      "Epoch 200: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3867 - acc: 0.8866 - auc: 0.9650 - val_loss: 0.5617 - val_acc: 0.8398 - val_auc: 0.7686 - lr: 0.0012\n",
      "Epoch 201/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3886 - acc: 0.8802 - auc: 0.9657\n",
      "Epoch 201: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3872 - acc: 0.8809 - auc: 0.9655 - val_loss: 0.5598 - val_acc: 0.8374 - val_auc: 0.7653 - lr: 0.0012\n",
      "Epoch 202/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3835 - acc: 0.8906 - auc: 0.9693\n",
      "Epoch 202: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3948 - acc: 0.8819 - auc: 0.9644 - val_loss: 0.5575 - val_acc: 0.8422 - val_auc: 0.7671 - lr: 0.0012\n",
      "Epoch 203/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3873 - acc: 0.8835 - auc: 0.9630\n",
      "Epoch 203: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3899 - acc: 0.8875 - auc: 0.9651 - val_loss: 0.5549 - val_acc: 0.8398 - val_auc: 0.7650 - lr: 0.0012\n",
      "Epoch 204/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.3907 - acc: 0.8819 - auc: 0.9641\n",
      "Epoch 204: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3841 - acc: 0.8885 - auc: 0.9666 - val_loss: 0.5564 - val_acc: 0.8447 - val_auc: 0.7645 - lr: 0.0012\n",
      "Epoch 205/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3798 - acc: 0.8865 - auc: 0.9653\n",
      "Epoch 205: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3916 - acc: 0.8856 - auc: 0.9652 - val_loss: 0.5596 - val_acc: 0.8447 - val_auc: 0.7632 - lr: 0.0012\n",
      "Epoch 206/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3981 - acc: 0.8817 - auc: 0.9653\n",
      "Epoch 206: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3890 - acc: 0.8771 - auc: 0.9645 - val_loss: 0.5639 - val_acc: 0.8422 - val_auc: 0.7662 - lr: 0.0012\n",
      "Epoch 207/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3935 - acc: 0.8867 - auc: 0.9639\n",
      "Epoch 207: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3899 - acc: 0.8885 - auc: 0.9649 - val_loss: 0.5516 - val_acc: 0.8374 - val_auc: 0.7653 - lr: 0.0012\n",
      "Epoch 208/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3846 - acc: 0.8880 - auc: 0.9645\n",
      "Epoch 208: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3828 - acc: 0.8922 - auc: 0.9689 - val_loss: 0.5552 - val_acc: 0.8350 - val_auc: 0.7653 - lr: 0.0012\n",
      "Epoch 209/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.4174 - acc: 0.8659 - auc: 0.9623\n",
      "Epoch 209: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3867 - acc: 0.8790 - auc: 0.9662 - val_loss: 0.5690 - val_acc: 0.8398 - val_auc: 0.7679 - lr: 0.0012\n",
      "Epoch 210/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3845 - acc: 0.8848 - auc: 0.9666\n",
      "Epoch 210: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3797 - acc: 0.8856 - auc: 0.9677 - val_loss: 0.5607 - val_acc: 0.8398 - val_auc: 0.7664 - lr: 0.0012\n",
      "Epoch 211/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3842 - acc: 0.8938 - auc: 0.9679\n",
      "Epoch 211: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3908 - acc: 0.8866 - auc: 0.9650 - val_loss: 0.5571 - val_acc: 0.8374 - val_auc: 0.7650 - lr: 0.0012\n",
      "Epoch 212/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3758 - acc: 0.8971 - auc: 0.9664\n",
      "Epoch 212: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3813 - acc: 0.8913 - auc: 0.9673 - val_loss: 0.5478 - val_acc: 0.8350 - val_auc: 0.7665 - lr: 0.0012\n",
      "Epoch 213/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3897 - acc: 0.8854 - auc: 0.9659\n",
      "Epoch 213: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3927 - acc: 0.8847 - auc: 0.9659 - val_loss: 0.5559 - val_acc: 0.8350 - val_auc: 0.7651 - lr: 0.0012\n",
      "Epoch 214/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3911 - acc: 0.8867 - auc: 0.9649\n",
      "Epoch 214: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3881 - acc: 0.8875 - auc: 0.9659 - val_loss: 0.5546 - val_acc: 0.8374 - val_auc: 0.7675 - lr: 0.0012\n",
      "Epoch 215/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3853 - acc: 0.8763 - auc: 0.9659\n",
      "Epoch 215: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3939 - acc: 0.8762 - auc: 0.9636 - val_loss: 0.5611 - val_acc: 0.8374 - val_auc: 0.7657 - lr: 0.0012\n",
      "Epoch 216/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.4060 - acc: 0.8882 - auc: 0.9635\n",
      "Epoch 216: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3839 - acc: 0.8904 - auc: 0.9678 - val_loss: 0.5537 - val_acc: 0.8374 - val_auc: 0.7673 - lr: 0.0012\n",
      "Epoch 217/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3824 - acc: 0.8882 - auc: 0.9669\n",
      "Epoch 217: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3826 - acc: 0.8866 - auc: 0.9665 - val_loss: 0.5558 - val_acc: 0.8374 - val_auc: 0.7657 - lr: 0.0012\n",
      "Epoch 218/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3738 - acc: 0.8766 - auc: 0.9679\n",
      "Epoch 218: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3930 - acc: 0.8819 - auc: 0.9658 - val_loss: 0.5514 - val_acc: 0.8374 - val_auc: 0.7657 - lr: 0.0012\n",
      "Epoch 219/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.3555 - acc: 0.8906 - auc: 0.9827\n",
      "Epoch 219: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3756 - acc: 0.8922 - auc: 0.9685 - val_loss: 0.5612 - val_acc: 0.8374 - val_auc: 0.7644 - lr: 0.0012\n",
      "Epoch 220/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3893 - acc: 0.8942 - auc: 0.9651\n",
      "Epoch 220: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3928 - acc: 0.8847 - auc: 0.9639 - val_loss: 0.5602 - val_acc: 0.8398 - val_auc: 0.7617 - lr: 0.0012\n",
      "Epoch 221/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3874 - acc: 0.8904 - auc: 0.9674\n",
      "Epoch 221: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3874 - acc: 0.8904 - auc: 0.9674 - val_loss: 0.5583 - val_acc: 0.8398 - val_auc: 0.7640 - lr: 0.0012\n",
      "Epoch 222/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4000 - acc: 0.8878 - auc: 0.9645\n",
      "Epoch 222: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3825 - acc: 0.8847 - auc: 0.9665 - val_loss: 0.5590 - val_acc: 0.8398 - val_auc: 0.7629 - lr: 0.0012\n",
      "Epoch 223/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3474 - acc: 0.8938 - auc: 0.9757\n",
      "Epoch 223: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3819 - acc: 0.8828 - auc: 0.9659 - val_loss: 0.5534 - val_acc: 0.8374 - val_auc: 0.7643 - lr: 0.0012\n",
      "Epoch 224/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3794 - acc: 0.8875 - auc: 0.9681\n",
      "Epoch 224: val_auc did not improve from 0.79500\n",
      "\n",
      "Epoch 224: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3842 - acc: 0.8913 - auc: 0.9672 - val_loss: 0.5580 - val_acc: 0.8374 - val_auc: 0.7646 - lr: 0.0012\n",
      "Epoch 225/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3877 - acc: 0.8833 - auc: 0.9673\n",
      "Epoch 225: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3805 - acc: 0.8856 - auc: 0.9677 - val_loss: 0.5632 - val_acc: 0.8447 - val_auc: 0.7629 - lr: 6.2500e-04\n",
      "Epoch 226/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.4060 - acc: 0.8837 - auc: 0.9645\n",
      "Epoch 226: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3923 - acc: 0.8809 - auc: 0.9635 - val_loss: 0.5589 - val_acc: 0.8422 - val_auc: 0.7636 - lr: 6.2500e-04\n",
      "Epoch 227/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3948 - acc: 0.8792 - auc: 0.9645\n",
      "Epoch 227: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3858 - acc: 0.8819 - auc: 0.9666 - val_loss: 0.5619 - val_acc: 0.8398 - val_auc: 0.7649 - lr: 6.2500e-04\n",
      "Epoch 228/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3910 - acc: 0.8880 - auc: 0.9653\n",
      "Epoch 228: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3790 - acc: 0.8885 - auc: 0.9683 - val_loss: 0.5640 - val_acc: 0.8422 - val_auc: 0.7641 - lr: 6.2500e-04\n",
      "Epoch 229/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3924 - acc: 0.8882 - auc: 0.9644\n",
      "Epoch 229: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3841 - acc: 0.8819 - auc: 0.9652 - val_loss: 0.5620 - val_acc: 0.8398 - val_auc: 0.7649 - lr: 6.2500e-04\n",
      "Epoch 230/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3730 - acc: 0.8862 - auc: 0.9666\n",
      "Epoch 230: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3870 - acc: 0.8847 - auc: 0.9651 - val_loss: 0.5622 - val_acc: 0.8398 - val_auc: 0.7653 - lr: 6.2500e-04\n",
      "Epoch 231/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3939 - acc: 0.8867 - auc: 0.9648\n",
      "Epoch 231: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3836 - acc: 0.8828 - auc: 0.9665 - val_loss: 0.5599 - val_acc: 0.8398 - val_auc: 0.7655 - lr: 6.2500e-04\n",
      "Epoch 232/500\n",
      " 9/17 [==============>...............] - ETA: 0s - loss: 0.3832 - acc: 0.8958 - auc: 0.9674\n",
      "Epoch 232: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3825 - acc: 0.8913 - auc: 0.9668 - val_loss: 0.5614 - val_acc: 0.8398 - val_auc: 0.7654 - lr: 6.2500e-04\n",
      "Epoch 233/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3818 - acc: 0.8833 - auc: 0.9677\n",
      "Epoch 233: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3883 - acc: 0.8800 - auc: 0.9664 - val_loss: 0.5631 - val_acc: 0.8398 - val_auc: 0.7660 - lr: 6.2500e-04\n",
      "Epoch 234/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3795 - acc: 0.8854 - auc: 0.9678\n",
      "Epoch 234: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3804 - acc: 0.8856 - auc: 0.9672 - val_loss: 0.5594 - val_acc: 0.8374 - val_auc: 0.7675 - lr: 6.2500e-04\n",
      "Epoch 235/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3796 - acc: 0.8887 - auc: 0.9688\n",
      "Epoch 235: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3840 - acc: 0.8885 - auc: 0.9686 - val_loss: 0.5592 - val_acc: 0.8350 - val_auc: 0.7672 - lr: 6.2500e-04\n",
      "Epoch 236/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3986 - acc: 0.8858 - auc: 0.9645\n",
      "Epoch 236: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3885 - acc: 0.8866 - auc: 0.9654 - val_loss: 0.5655 - val_acc: 0.8374 - val_auc: 0.7664 - lr: 6.2500e-04\n",
      "Epoch 237/500\n",
      " 8/17 [=============>................] - ETA: 0s - loss: 0.4325 - acc: 0.8867 - auc: 0.9599\n",
      "Epoch 237: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3882 - acc: 0.8894 - auc: 0.9645 - val_loss: 0.5648 - val_acc: 0.8398 - val_auc: 0.7667 - lr: 6.2500e-04\n",
      "Epoch 238/500\n",
      " 1/17 [>.............................] - ETA: 0s - loss: 0.2568 - acc: 0.9062 - auc: 0.9879\n",
      "Epoch 238: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3744 - acc: 0.8941 - auc: 0.9681 - val_loss: 0.5611 - val_acc: 0.8374 - val_auc: 0.7668 - lr: 6.2500e-04\n",
      "Epoch 239/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3586 - acc: 0.8978 - auc: 0.9725\n",
      "Epoch 239: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3799 - acc: 0.8904 - auc: 0.9664 - val_loss: 0.5620 - val_acc: 0.8398 - val_auc: 0.7664 - lr: 6.2500e-04\n",
      "Epoch 240/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3842 - acc: 0.8895 - auc: 0.9676\n",
      "Epoch 240: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3810 - acc: 0.8866 - auc: 0.9676 - val_loss: 0.5598 - val_acc: 0.8374 - val_auc: 0.7663 - lr: 6.2500e-04\n",
      "Epoch 241/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3716 - acc: 0.8984 - auc: 0.9703\n",
      "Epoch 241: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3792 - acc: 0.8932 - auc: 0.9676 - val_loss: 0.5633 - val_acc: 0.8422 - val_auc: 0.7665 - lr: 6.2500e-04\n",
      "Epoch 242/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3825 - acc: 0.8815 - auc: 0.9671\n",
      "Epoch 242: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3831 - acc: 0.8885 - auc: 0.9665 - val_loss: 0.5594 - val_acc: 0.8398 - val_auc: 0.7663 - lr: 6.2500e-04\n",
      "Epoch 243/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3775 - acc: 0.8941 - auc: 0.9671\n",
      "Epoch 243: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3775 - acc: 0.8941 - auc: 0.9671 - val_loss: 0.5578 - val_acc: 0.8374 - val_auc: 0.7664 - lr: 6.2500e-04\n",
      "Epoch 244/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3793 - acc: 0.8917 - auc: 0.9691\n",
      "Epoch 244: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3825 - acc: 0.8904 - auc: 0.9670 - val_loss: 0.5585 - val_acc: 0.8398 - val_auc: 0.7657 - lr: 6.2500e-04\n",
      "Epoch 245/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3777 - acc: 0.8838 - auc: 0.9686\n",
      "Epoch 245: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3811 - acc: 0.8847 - auc: 0.9678 - val_loss: 0.5616 - val_acc: 0.8374 - val_auc: 0.7662 - lr: 6.2500e-04\n",
      "Epoch 246/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3918 - acc: 0.8919 - auc: 0.9660\n",
      "Epoch 246: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3788 - acc: 0.8904 - auc: 0.9679 - val_loss: 0.5637 - val_acc: 0.8374 - val_auc: 0.7651 - lr: 6.2500e-04\n",
      "Epoch 247/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3725 - acc: 0.8906 - auc: 0.9686\n",
      "Epoch 247: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3765 - acc: 0.8904 - auc: 0.9686 - val_loss: 0.5631 - val_acc: 0.8374 - val_auc: 0.7645 - lr: 6.2500e-04\n",
      "Epoch 248/500\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 0.3745 - acc: 0.8926 - auc: 0.9669\n",
      "Epoch 248: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3777 - acc: 0.8932 - auc: 0.9669 - val_loss: 0.5668 - val_acc: 0.8398 - val_auc: 0.7658 - lr: 6.2500e-04\n",
      "Epoch 249/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3667 - acc: 0.8951 - auc: 0.9702\n",
      "Epoch 249: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3847 - acc: 0.8913 - auc: 0.9674 - val_loss: 0.5650 - val_acc: 0.8374 - val_auc: 0.7656 - lr: 6.2500e-04\n",
      "Epoch 250/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3618 - acc: 0.9020 - auc: 0.9727\n",
      "Epoch 250: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3795 - acc: 0.8875 - auc: 0.9680 - val_loss: 0.5653 - val_acc: 0.8398 - val_auc: 0.7658 - lr: 6.2500e-04\n",
      "Epoch 251/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3761 - acc: 0.8906 - auc: 0.9677\n",
      "Epoch 251: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3799 - acc: 0.8904 - auc: 0.9671 - val_loss: 0.5657 - val_acc: 0.8398 - val_auc: 0.7658 - lr: 6.2500e-04\n",
      "Epoch 252/500\n",
      "10/17 [================>.............] - ETA: 0s - loss: 0.3837 - acc: 0.8875 - auc: 0.9730\n",
      "Epoch 252: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3815 - acc: 0.8781 - auc: 0.9672 - val_loss: 0.5657 - val_acc: 0.8422 - val_auc: 0.7653 - lr: 6.2500e-04\n",
      "Epoch 253/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3768 - acc: 0.8894 - auc: 0.9673\n",
      "Epoch 253: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 0.3768 - acc: 0.8894 - auc: 0.9673 - val_loss: 0.5566 - val_acc: 0.8374 - val_auc: 0.7646 - lr: 6.2500e-04\n",
      "Epoch 254/500\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 0.3795 - acc: 0.8906 - auc: 0.9689\n",
      "Epoch 254: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.3919 - acc: 0.8866 - auc: 0.9655 - val_loss: 0.5576 - val_acc: 0.8374 - val_auc: 0.7654 - lr: 6.2500e-04\n",
      "Epoch 255/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3919 - acc: 0.8867 - auc: 0.9648\n",
      "Epoch 255: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3806 - acc: 0.8885 - auc: 0.9670 - val_loss: 0.5581 - val_acc: 0.8422 - val_auc: 0.7657 - lr: 6.2500e-04\n",
      "Epoch 256/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3745 - acc: 0.8940 - auc: 0.9682\n",
      "Epoch 256: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3759 - acc: 0.8913 - auc: 0.9674 - val_loss: 0.5493 - val_acc: 0.8374 - val_auc: 0.7657 - lr: 6.2500e-04\n",
      "Epoch 257/500\n",
      "12/17 [====================>.........] - ETA: 0s - loss: 0.3880 - acc: 0.8958 - auc: 0.9673\n",
      "Epoch 257: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3726 - acc: 0.8970 - auc: 0.9701 - val_loss: 0.5515 - val_acc: 0.8374 - val_auc: 0.7668 - lr: 6.2500e-04\n",
      "Epoch 258/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3895 - acc: 0.8822 - auc: 0.9641\n",
      "Epoch 258: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3724 - acc: 0.8894 - auc: 0.9688 - val_loss: 0.5530 - val_acc: 0.8374 - val_auc: 0.7664 - lr: 6.2500e-04\n",
      "Epoch 259/500\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.3773 - acc: 0.8866 - auc: 0.9668\n",
      "Epoch 259: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3773 - acc: 0.8866 - auc: 0.9668 - val_loss: 0.5540 - val_acc: 0.8374 - val_auc: 0.7668 - lr: 6.2500e-04\n",
      "Epoch 260/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3747 - acc: 0.8954 - auc: 0.9682\n",
      "Epoch 260: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3795 - acc: 0.8960 - auc: 0.9673 - val_loss: 0.5592 - val_acc: 0.8374 - val_auc: 0.7660 - lr: 6.2500e-04\n",
      "Epoch 261/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.3580 - acc: 0.8849 - auc: 0.9697\n",
      "Epoch 261: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3839 - acc: 0.8894 - auc: 0.9678 - val_loss: 0.5631 - val_acc: 0.8374 - val_auc: 0.7655 - lr: 6.2500e-04\n",
      "Epoch 262/500\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 0.3900 - acc: 0.8873 - auc: 0.9659\n",
      "Epoch 262: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.3761 - acc: 0.8913 - auc: 0.9672 - val_loss: 0.5641 - val_acc: 0.8422 - val_auc: 0.7656 - lr: 6.2500e-04\n",
      "Epoch 263/500\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 0.4021 - acc: 0.8906 - auc: 0.9640\n",
      "Epoch 263: val_auc did not improve from 0.79500\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3826 - acc: 0.8922 - auc: 0.9654 - val_loss: 0.5671 - val_acc: 0.8422 - val_auc: 0.7648 - lr: 6.2500e-04\n",
      "Epoch 264/500\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 0.3676 - acc: 0.8858 - auc: 0.9704\n",
      "Epoch 264: val_auc did not improve from 0.79500\n",
      "\n",
      "Epoch 264: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "Restoring model weights from the end of the best epoch: 64.\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3752 - acc: 0.8885 - auc: 0.9680 - val_loss: 0.5663 - val_acc: 0.8398 - val_auc: 0.7660 - lr: 6.2500e-04\n",
      "Epoch 264: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_ensemble.compile(\n",
    "    # SGD may generalize better than Adam, especially on CNN\n",
    "    # https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer\n",
    "    optimizer = keras.optimizers.SGD(momentum = 0.9, nesterov = True),\n",
    "    # False if already using activation (e.g. sigmoid), true if no activation\n",
    "    # May call sigmoid activation internally if true\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "    metrics = [\n",
    "        keras.metrics.BinaryAccuracy(name = 'acc'),\n",
    "        # Often used in binary classification since accuracy is a flawed metric\n",
    "        keras.metrics.AUC(name = 'auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  ====================\n",
    "\n",
    "# Reduce LR is usually not needed for Adam (but useful for SGD)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    factor = 0.5,\n",
    "    patience = 40,\n",
    "    min_lr = 0.0001,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    patience = 200,\n",
    "    min_delta = 0,\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    restore_best_weights = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "logger = keras.callbacks.CSVLogger(\n",
    "    filename = 'model/model_training_hist.csv',\n",
    "    append = False\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = f'model/model_ensemble.keras',\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "history = model_ensemble.fit(\n",
    "    x_train,\n",
    "    y_train.values,\n",
    "    epochs = 500,\n",
    "    # Bigger size to include more minority class for the model to learn\n",
    "    batch_size = 64,\n",
    "    class_weight = class_weights,\n",
    "    validation_data = (x_test, y_test),\n",
    "    callbacks = [checkpoint, logger, reduce_lr, early_stop],\n",
    ")\n",
    "\n",
    "# Save also individual models that composed ensemble model\n",
    "for i, model in enumerate([model1, model2, model3]):\n",
    "    model.save(f'model/model{i + 1}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Visualize layer output to debug model performance (see [reference](https://www.kaggle.com/code/kirillka95/keras-nn-layer-activations-ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>Pred_Attrition</th>\n",
       "      <th>Pred_Percent</th>\n",
       "      <th>Pred_IsTrue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.281128</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.578981</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.752460</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921624</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.964939</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>548</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.832790</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>97.859238</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.330963</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.393311</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99.815231</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  Attrition  Pred_Attrition  Pred_Percent  Pred_IsTrue\n",
       "0              1          0               0     36.281128         True\n",
       "953          954          0               0      4.578981         True\n",
       "952          953          0               0     29.752460         True\n",
       "949          950          0               0      0.921624         True\n",
       "945          946          0               0      5.964939         True\n",
       "...          ...        ...             ...           ...          ...\n",
       "547          548          1               0     36.832790        False\n",
       "1103        1104          1               1     97.859238         True\n",
       "540          541          1               1     92.330963         True\n",
       "864          865          1               1     77.393311         True\n",
       "697          698          1               1     99.815231         True\n",
       "\n",
       "[412 rows x 5 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_ensemble.predict(x_test)\n",
    "\n",
    "df_pred = y_test.copy()\n",
    "df_pred['EmployeeId'] = df.loc[df['IsTest'] == 1, 'EmployeeId']\n",
    "df_pred['Pred_Percent'] = y_pred * 100\n",
    "\n",
    "# If more than 50% chance, it's most likely attrition = true\n",
    "df_pred.loc[df_pred['Pred_Percent'] > 50, 'Pred_Attrition'] = 1\n",
    "df_pred.loc[df_pred['Pred_Percent'] <= 50, 'Pred_Attrition'] = 0\n",
    "\n",
    "# https://stackoverflow.com/a/71797755\n",
    "df_pred['Pred_IsTrue'] = df_pred['Attrition'] == df_pred['Pred_Attrition']\n",
    "\n",
    "df_pred['Pred_Attrition'] = df_pred['Pred_Attrition'].astype('int')\n",
    "df_pred = df_pred[['EmployeeId', 'Attrition', 'Pred_Attrition', 'Pred_Percent', 'Pred_IsTrue']]\n",
    "df_pred.sort_values('Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition\n",
      "0    354\n",
      "1     58\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Pred_IsTrue\n",
      "True     337\n",
      "False     75\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_pred['Attrition'].value_counts(), '\\n')\n",
    "print(df_pred['Pred_IsTrue'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check F1 score for both attrition 0 and 1 (usually score will be quite bad on minority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.84      0.89       354\n",
      "           1       0.41      0.69      0.52        58\n",
      "\n",
      "    accuracy                           0.82       412\n",
      "   macro avg       0.68      0.76      0.70       412\n",
      "weighted avg       0.87      0.82      0.84       412\n",
      "\n",
      "Confusion matrix:\n",
      " [[297  57]\n",
      " [ 18  40]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_pred['Attrition'], df_pred['Pred_Attrition']))\n",
    "print('Confusion matrix:\\n', confusion_matrix(df_pred['Attrition'], df_pred['Pred_Attrition']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "Changelog:\n",
    "- v1: Initial\n",
    "- v2: Switch from decision tree to neural network. Around 80-90% accuracy but bad F1 score for attrition = true (0.51)\n",
    "- v3: Use SMOTE, F1 score varies but generally worse than without using it. Also, SMOTE random seed doesn't work correctly\n",
    "- v4: Remove SMOTE. Changed label encoder to target encoder, added feature selection using Pearson method. F1 score doesn't seem to improve much even after following this [reference](https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80) (reference has data leakage)\n",
    "- v5: Use model ensembling (3 NN models), F1 score is still the same but prediction is somewhat better (more true positives)\n",
    "- v6: Used label encoder again before uploading data to Supabase (it will still be encoded using target encoder, but in later steps)\n",
    "\n",
    "Reference and/or todo list:\n",
    "- https://www.kaggle.com/competitions/playground-series-s3e3 (main competition)\n",
    "- https://www.kaggle.com/competitions/cat-in-the-dat-ii (categorical feature encoding)\n",
    "- https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80 (good EDA, recommended)\n",
    "- https://www.kaggle.com/code/kirillka95/keras-nn-layer-activations-ensemble (ensemble NN, NN visualization, recommended)\n",
    "- https://www.kaggle.com/code/robertobonilla/explained-100-advanced-classification-beginners (multiple normalization/pre-processing)\n",
    "- https://www.kaggle.com/code/ashishkumarak/employee-retention-workforce-reduction-prediction (calculating feature importance NN)\n",
    "- https://www.kaggle.com/code/samuelcortinhas/ps-s3e3-hill-climbing-like-a-gm (hill climbing)\n",
    "- https://www.kaggle.com/code/zuberrr/feature-engineering-w-xgboost-87-auc-in-10-mins (feature engineering)\n",
    "- https://www.kaggle.com/code/cv13j0/pss-3-episode-3-lama (feature engineering)\n",
    "- https://www.kaggle.com/code/kirillka95/ps-s03e03-eda-16-models-test-0-94 (ensemble + model test)\n",
    "- https://www.kaggle.com/code/chunweishen/ps-s03e03-ensembling (ensemble + feature importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
