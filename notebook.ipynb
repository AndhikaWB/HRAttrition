{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HR Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CSV to database\n",
    "import sqlalchemy as sqa\n",
    "\n",
    "# Handle imbalanced data\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Reduce data dimension for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Remove less important features (Pearson correlation)\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "from sklearn.preprocessing import TargetEncoder, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "Mainly intended for dashboard, ML preparation is on separate section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>36</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>884</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2061</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>39</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>613</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2062</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>155</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2064</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1023</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2065</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>34</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>628</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>2068</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "1465   36        No  Travel_Frequently        884  Research & Development   \n",
       "1466   39        No      Travel_Rarely        613  Research & Development   \n",
       "1467   27        No      Travel_Rarely        155  Research & Development   \n",
       "1468   49        No  Travel_Frequently       1023                   Sales   \n",
       "1469   34        No      Travel_Rarely        628  Research & Development   \n",
       "\n",
       "      DistanceFromHome  Education EducationField  EmployeeCount  \\\n",
       "1465                23          2        Medical              1   \n",
       "1466                 6          1        Medical              1   \n",
       "1467                 4          3  Life Sciences              1   \n",
       "1468                 2          3        Medical              1   \n",
       "1469                 8          3        Medical              1   \n",
       "\n",
       "      EmployeeNumber  ...  RelationshipSatisfaction StandardHours  \\\n",
       "1465            2061  ...                         3            80   \n",
       "1466            2062  ...                         1            80   \n",
       "1467            2064  ...                         2            80   \n",
       "1468            2065  ...                         4            80   \n",
       "1469            2068  ...                         1            80   \n",
       "\n",
       "      StockOptionLevel  TotalWorkingYears  TrainingTimesLastYear  \\\n",
       "1465                 1                 17                      3   \n",
       "1466                 1                  9                      5   \n",
       "1467                 1                  6                      0   \n",
       "1468                 0                 17                      3   \n",
       "1469                 0                  6                      3   \n",
       "\n",
       "     WorkLifeBalance  YearsAtCompany YearsInCurrentRole  \\\n",
       "1465               3               5                  2   \n",
       "1466               3               7                  7   \n",
       "1467               3               6                  2   \n",
       "1468               2               9                  6   \n",
       "1469               4               4                  3   \n",
       "\n",
       "      YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "1465                        0                     3  \n",
       "1466                        1                     7  \n",
       "1467                        0                     3  \n",
       "1468                        0                     8  \n",
       "1469                        1                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('data/employee_data.csv')\n",
    "df = pd.read_csv('data/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Check Missing Data\n",
    "The original `employee_data.csv` from Dicoding has many missing attrition values, which is the most important one (for checking prediction accuracy, etc)\n",
    "\n",
    "On real world scenario, it's impossible to have missing attrition values since employees that no longer work should be noticed by HR\n",
    "\n",
    "Possible solutions:\n",
    "- Solution A: use algorithm like KNN or PCA or TSNE to fill missing values (may needs data scaling)\n",
    "- **Solution B (choosen)**: use alternative data from [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/) (no missing attrition values but need to re-add employee IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0\n",
       "Attrition                   0\n",
       "BusinessTravel              0\n",
       "DailyRate                   0\n",
       "Department                  0\n",
       "DistanceFromHome            0\n",
       "Education                   0\n",
       "EducationField              0\n",
       "EmployeeCount               0\n",
       "EmployeeNumber              0\n",
       "EnvironmentSatisfaction     0\n",
       "Gender                      0\n",
       "HourlyRate                  0\n",
       "JobInvolvement              0\n",
       "JobLevel                    0\n",
       "JobRole                     0\n",
       "JobSatisfaction             0\n",
       "MaritalStatus               0\n",
       "MonthlyIncome               0\n",
       "MonthlyRate                 0\n",
       "NumCompaniesWorked          0\n",
       "Over18                      0\n",
       "OverTime                    0\n",
       "PercentSalaryHike           0\n",
       "PerformanceRating           0\n",
       "RelationshipSatisfaction    0\n",
       "StandardHours               0\n",
       "StockOptionLevel            0\n",
       "TotalWorkingYears           0\n",
       "TrainingTimesLastYear       0\n",
       "WorkLifeBalance             0\n",
       "YearsAtCompany              0\n",
       "YearsInCurrentRole          0\n",
       "YearsSinceLastPromotion     0\n",
       "YearsWithCurrManager        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.info()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-add missing employee ID column (by joining both dataset)\n",
    "\n",
    "Will be joined based on hash since there is no similar primary key on both tables (one has `EmployeeId`, the other has `EmployeeNumber` which is different)\n",
    "\n",
    "**Note:** The original rows with missing attrition value (from Dicoding) will be marked as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns to exclude from hashing (for table join operation)\n",
    "# Assuming both dataframes have similar column names\n",
    "skip_col = ['Attrition', 'EmployeeId', 'EmployeeNumber']\n",
    "hash_col = [i for i in df.columns if i not in skip_col]\n",
    "\n",
    "temp = pd.read_csv('data/employee_data.csv')\n",
    "\n",
    "# Make sure to exclude index from hashing too\n",
    "# Otherwise, both tables will have different hashes\n",
    "temp['hash'] = pd.util.hash_pandas_object(temp[hash_col], index = False)\n",
    "df['hash'] = pd.util.hash_pandas_object(df[hash_col], index = False)\n",
    "\n",
    "temp = temp.sort_values('hash').reset_index(drop = True)\n",
    "df = df.sort_values('hash').reset_index(drop = True)\n",
    "\n",
    "# Will give output if there are still differences\n",
    "df[hash_col].compare(temp[hash_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1466</td>\n",
       "      <td>38</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>168</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1467</td>\n",
       "      <td>50</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>813</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1468</td>\n",
       "      <td>28</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1485</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1469</td>\n",
       "      <td>40</td>\n",
       "      <td>No</td>\n",
       "      <td>Non-Travel</td>\n",
       "      <td>458</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1470</td>\n",
       "      <td>19</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>602</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Technical Degree</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  Age Attrition     BusinessTravel  DailyRate  \\\n",
       "1465        1466   38        No      Travel_Rarely        168   \n",
       "1466        1467   50        No      Travel_Rarely        813   \n",
       "1467        1468   28       Yes      Travel_Rarely       1485   \n",
       "1468        1469   40        No         Non-Travel        458   \n",
       "1469        1470   19       Yes  Travel_Frequently        602   \n",
       "\n",
       "                  Department  DistanceFromHome  Education    EducationField  \\\n",
       "1465  Research & Development                 1          3     Life Sciences   \n",
       "1466  Research & Development                17          5     Life Sciences   \n",
       "1467  Research & Development                12          1     Life Sciences   \n",
       "1468  Research & Development                16          2     Life Sciences   \n",
       "1469                   Sales                 1          1  Technical Degree   \n",
       "\n",
       "      EmployeeCount  ...  StandardHours StockOptionLevel  TotalWorkingYears  \\\n",
       "1465              1  ...             80                0                 10   \n",
       "1466              1  ...             80                3                 19   \n",
       "1467              1  ...             80                0                  1   \n",
       "1468              1  ...             80                1                  6   \n",
       "1469              1  ...             80                0                  1   \n",
       "\n",
       "      TrainingTimesLastYear  WorkLifeBalance YearsAtCompany  \\\n",
       "1465                      4                4              1   \n",
       "1466                      3                3             14   \n",
       "1467                      4                2              1   \n",
       "1468                      0                3              4   \n",
       "1469                      5                4              0   \n",
       "\n",
       "      YearsInCurrentRole YearsSinceLastPromotion  YearsWithCurrManager  IsTest  \n",
       "1465                   0                       0                     0       0  \n",
       "1466                  11                       1                    11       1  \n",
       "1467                   1                       0                     0       0  \n",
       "1468                   2                       0                     0       0  \n",
       "1469                   0                       0                     0       0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(df, temp[['hash', 'EmployeeId']], on = 'hash')\n",
    "df = df.drop(['hash', 'EmployeeNumber'], axis = 1)\n",
    "\n",
    "# Mark rows with no labels (from Dicoding) as test data\n",
    "df['IsTest'] = temp['Attrition'].isnull().astype('int')\n",
    "\n",
    "# Reorder employee id column as first column\n",
    "temp = [i for i in df.columns if i != 'EmployeeId']\n",
    "df = df[['EmployeeId'] + temp]\n",
    "\n",
    "df = df.sort_values('EmployeeId').reset_index(drop = True)\n",
    "# Export back fixed dataframe (no missing attrition value)\n",
    "df.to_csv('data/employee_data_fixed.csv', index = False)\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Check/Convert Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmployeeId                   int64\n",
       "Age                          int64\n",
       "Attrition                   object\n",
       "BusinessTravel              object\n",
       "DailyRate                    int64\n",
       "Department                  object\n",
       "DistanceFromHome             int64\n",
       "Education                    int64\n",
       "EducationField              object\n",
       "EmployeeCount                int64\n",
       "EnvironmentSatisfaction      int64\n",
       "Gender                      object\n",
       "HourlyRate                   int64\n",
       "JobInvolvement               int64\n",
       "JobLevel                     int64\n",
       "JobRole                     object\n",
       "JobSatisfaction              int64\n",
       "MaritalStatus               object\n",
       "MonthlyIncome                int64\n",
       "MonthlyRate                  int64\n",
       "NumCompaniesWorked           int64\n",
       "Over18                      object\n",
       "OverTime                    object\n",
       "PercentSalaryHike            int64\n",
       "PerformanceRating            int64\n",
       "RelationshipSatisfaction     int64\n",
       "StandardHours                int64\n",
       "StockOptionLevel             int64\n",
       "TotalWorkingYears            int64\n",
       "TrainingTimesLastYear        int64\n",
       "WorkLifeBalance              int64\n",
       "YearsAtCompany               int64\n",
       "YearsInCurrentRole           int64\n",
       "YearsSinceLastPromotion      int64\n",
       "YearsWithCurrManager         int64\n",
       "IsTest                       int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition\n",
      "No     1233\n",
      "Yes     237\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "IsTest\n",
      "0    1058\n",
      "1     412\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "BusinessTravel\n",
      "Travel_Rarely        1043\n",
      "Travel_Frequently     277\n",
      "Non-Travel            150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check possible values for specific column\n",
    "# Add other columns if needed\n",
    "print(df['Attrition'].value_counts(dropna = False), '\\n')\n",
    "print(df['IsTest'].value_counts(dropna = False), '\\n')\n",
    "print(df['BusinessTravel'].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace boolean and ordinal values with numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhika\\AppData\\Local\\Temp\\VSCodePortableTemp\\ipykernel_14756\\763175519.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId  Age  Attrition  BusinessTravel  DailyRate  \\\n",
       "0           1   38          0               2       1444   \n",
       "1           2   37          1               1       1141   \n",
       "2           3   51          1               1       1323   \n",
       "3           4   42          0               2        555   \n",
       "4           5   40          0               1       1194   \n",
       "\n",
       "               Department  DistanceFromHome  Education EducationField  \\\n",
       "0         Human Resources                 1          4          Other   \n",
       "1  Research & Development                11          2        Medical   \n",
       "2  Research & Development                 4          4  Life Sciences   \n",
       "3                   Sales                26          3      Marketing   \n",
       "4  Research & Development                 2          4        Medical   \n",
       "\n",
       "   EmployeeCount  ...  StandardHours StockOptionLevel  TotalWorkingYears  \\\n",
       "0              1  ...             80                1                  7   \n",
       "1              1  ...             80                0                 15   \n",
       "2              1  ...             80                3                 18   \n",
       "3              1  ...             80                1                 23   \n",
       "4              1  ...             80                3                 20   \n",
       "\n",
       "   TrainingTimesLastYear  WorkLifeBalance YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                      2                3              6                   2   \n",
       "1                      2                1              1                   0   \n",
       "2                      2                4             10                   0   \n",
       "3                      2                4             20                   4   \n",
       "4                      2                3              5                   3   \n",
       "\n",
       "  YearsSinceLastPromotion  YearsWithCurrManager  IsTest  \n",
       "0                       1                     2       1  \n",
       "1                       0                     0       0  \n",
       "2                       2                     7       0  \n",
       "3                       4                     8       0  \n",
       "4                       0                     2       1  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace(\n",
    "    {\n",
    "        'Attrition': {\n",
    "            'Yes': 1,\n",
    "            'No': 0\n",
    "        },\n",
    "        'BusinessTravel': {\n",
    "            'Non-Travel': 0,\n",
    "            'Travel_Rarely': 1,\n",
    "            'Travel_Frequently': 2\n",
    "        },\n",
    "        'Over18': {\n",
    "            'Y': 1,\n",
    "            'N': 0\n",
    "        },\n",
    "        'OverTime': {\n",
    "            'Yes': 1,\n",
    "            'No': 0\n",
    "        }\n",
    "    },\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical values to numeric values (e.g. 0 = single, 1 = married, 2 = divorced)\n",
    "\n",
    "Normally, label encoding is enough for dashboard, but we may use other encoder for ML model later (see comparison [here](https://www.datacamp.com/tutorial/categorical-data) and [there](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_target_encoder.html))\n",
    "\n",
    "**Alternative:** use \"pd.get_dummies\" (one hot encoder) or Sklearn \"TargetEncoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DepartmentN</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>...</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Other</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>Medical</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Medical</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId  Age  Attrition  BusinessTravel  DailyRate  \\\n",
       "0           1   38          0               2       1444   \n",
       "1           2   37          1               1       1141   \n",
       "2           3   51          1               1       1323   \n",
       "3           4   42          0               2        555   \n",
       "4           5   40          0               1       1194   \n",
       "\n",
       "               Department  DepartmentN  DistanceFromHome  Education  \\\n",
       "0         Human Resources            0                 1          4   \n",
       "1  Research & Development            1                11          2   \n",
       "2  Research & Development            1                 4          4   \n",
       "3                   Sales            2                26          3   \n",
       "4  Research & Development            1                 2          4   \n",
       "\n",
       "  EducationField  ...  StandardHours  StockOptionLevel  TotalWorkingYears  \\\n",
       "0          Other  ...             80                 1                  7   \n",
       "1        Medical  ...             80                 0                 15   \n",
       "2  Life Sciences  ...             80                 3                 18   \n",
       "3      Marketing  ...             80                 1                 23   \n",
       "4        Medical  ...             80                 3                 20   \n",
       "\n",
       "  TrainingTimesLastYear  WorkLifeBalance  YearsAtCompany  YearsInCurrentRole  \\\n",
       "0                     2                3               6                   2   \n",
       "1                     2                1               1                   0   \n",
       "2                     2                4              10                   0   \n",
       "3                     2                4              20                   4   \n",
       "4                     2                3               5                   3   \n",
       "\n",
       "   YearsSinceLastPromotion YearsWithCurrManager  IsTest  \n",
       "0                        1                    2       1  \n",
       "1                        0                    0       0  \n",
       "2                        2                    7       0  \n",
       "3                        4                    8       0  \n",
       "4                        0                    2       1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical columns to convert\n",
    "cat_col = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
    "\n",
    "for col in cat_col:\n",
    "    # Add new numeric column (column name + N) for each categorical column\n",
    "    df[col + 'N'] = pd.factorize(df[col])[0]\n",
    "\n",
    "# Sort column names to fix newly added columns\n",
    "df = df.reindex(sorted(df.columns), axis = 1)\n",
    "\n",
    "# Place employee id at beginning and is test at the end\n",
    "temp = [i for i in df.columns if i not in ['EmployeeId', 'IsTest']]\n",
    "df = df[['EmployeeId'] + temp + ['IsTest']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recheck data types\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Remove Useless Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmployeeId is useless (unique values equal row length)\n",
      "EmployeeCount is useless (only 1 possible value)\n",
      "Over18 is useless (only 1 possible value)\n",
      "StandardHours is useless (only 1 possible value)\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].nunique() == 1:\n",
    "        print(i, 'is useless (only 1 possible value)')\n",
    "    if df[i].nunique() == len(df):\n",
    "        print(i, 'is useless (unique values equal row length)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EmployeeId` is useless for dashboard, but still can be used for machine learning prediction (hence not removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['EmployeeCount', 'Over18', 'StandardHours'], axis = 1, inplace = True)\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Other Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Supabase project first or local PostgreSQL server (for Metabase), and fill required URL in `database.txt` to export the CSV table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to Supabase/local SQL server\n",
    "export_database = False\n",
    "\n",
    "with open('database.txt') as f:\n",
    "    database_url = f.readline()\n",
    "    # print(database_url)\n",
    "    engine = sqa.create_engine(url = database_url)\n",
    "\n",
    "if export_database:\n",
    "    df.to_sql('employee', engine, index = False, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix is very useful before analyzing too deep into the data, and can be used as initial hypothesis\n",
    "\n",
    "It will give a quick insight about the data relationship (bi-directional only), especially those that caused attrition\n",
    "\n",
    "**Note 1:** Correlation does not imply causation, and it works both ways (1 and -1 both means strong correlation, but opposite relationship)\n",
    "\n",
    "**Note 2:** Categorical columns should be ignored (non-standard sort order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6ef33_row0_col0 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row1_col0 {\n",
       "  background-color: #b2ccfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6ef33_row2_col0 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_6ef33_row3_col0 {\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row4_col0 {\n",
       "  background-color: #80a3fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row5_col0 {\n",
       "  background-color: #7b9ff9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row6_col0 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row7_col0 {\n",
       "  background-color: #7597f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row8_col0 {\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row9_col0 {\n",
       "  background-color: #6b8df0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row10_col0 {\n",
       "  background-color: #6a8bef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row11_col0 {\n",
       "  background-color: #688aef;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row12_col0 {\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row13_col0 {\n",
       "  background-color: #6485ec;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row14_col0, #T_6ef33_row15_col0, #T_6ef33_row16_col0 {\n",
       "  background-color: #5f7fe8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row17_col0 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row18_col0 {\n",
       "  background-color: #5b7ae5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row19_col0 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row20_col0 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row21_col0 {\n",
       "  background-color: #5673e0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row22_col0, #T_6ef33_row23_col0 {\n",
       "  background-color: #4b64d5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row24_col0, #T_6ef33_row25_col0 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row26_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row27_col0 {\n",
       "  background-color: #3e51c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row28_col0, #T_6ef33_row29_col0, #T_6ef33_row30_col0 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_6ef33_row31_col0, #T_6ef33_row32_col0 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6ef33\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6ef33_level0_col0\" class=\"col_heading level0 col0\" >Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row0\" class=\"row_heading level0 row0\" >Attrition</th>\n",
       "      <td id=\"T_6ef33_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row1\" class=\"row_heading level0 row1\" >OverTime</th>\n",
       "      <td id=\"T_6ef33_row1_col0\" class=\"data row1 col0\" >0.246118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row2\" class=\"row_heading level0 row2\" >BusinessTravel</th>\n",
       "      <td id=\"T_6ef33_row2_col0\" class=\"data row2 col0\" >0.127006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row3\" class=\"row_heading level0 row3\" >EducationFieldN</th>\n",
       "      <td id=\"T_6ef33_row3_col0\" class=\"data row3 col0\" >0.094277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row4\" class=\"row_heading level0 row4\" >DistanceFromHome</th>\n",
       "      <td id=\"T_6ef33_row4_col0\" class=\"data row4 col0\" >0.077924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row5\" class=\"row_heading level0 row5\" >DepartmentN</th>\n",
       "      <td id=\"T_6ef33_row5_col0\" class=\"data row5 col0\" >0.063991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row6\" class=\"row_heading level0 row6\" >JobRoleN</th>\n",
       "      <td id=\"T_6ef33_row6_col0\" class=\"data row6 col0\" >0.047330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row7\" class=\"row_heading level0 row7\" >NumCompaniesWorked</th>\n",
       "      <td id=\"T_6ef33_row7_col0\" class=\"data row7 col0\" >0.043494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row8\" class=\"row_heading level0 row8\" >MonthlyRate</th>\n",
       "      <td id=\"T_6ef33_row8_col0\" class=\"data row8 col0\" >0.015170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row9\" class=\"row_heading level0 row9\" >MaritalStatusN</th>\n",
       "      <td id=\"T_6ef33_row9_col0\" class=\"data row9 col0\" >0.011195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row10\" class=\"row_heading level0 row10\" >PerformanceRating</th>\n",
       "      <td id=\"T_6ef33_row10_col0\" class=\"data row10 col0\" >0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row11\" class=\"row_heading level0 row11\" >EmployeeId</th>\n",
       "      <td id=\"T_6ef33_row11_col0\" class=\"data row11 col0\" >-0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row12\" class=\"row_heading level0 row12\" >HourlyRate</th>\n",
       "      <td id=\"T_6ef33_row12_col0\" class=\"data row12 col0\" >-0.006846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row13\" class=\"row_heading level0 row13\" >PercentSalaryHike</th>\n",
       "      <td id=\"T_6ef33_row13_col0\" class=\"data row13 col0\" >-0.013478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row14\" class=\"row_heading level0 row14\" >GenderN</th>\n",
       "      <td id=\"T_6ef33_row14_col0\" class=\"data row14 col0\" >-0.029453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row15\" class=\"row_heading level0 row15\" >Education</th>\n",
       "      <td id=\"T_6ef33_row15_col0\" class=\"data row15 col0\" >-0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row16\" class=\"row_heading level0 row16\" >YearsSinceLastPromotion</th>\n",
       "      <td id=\"T_6ef33_row16_col0\" class=\"data row16 col0\" >-0.033019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row17\" class=\"row_heading level0 row17\" >IsTest</th>\n",
       "      <td id=\"T_6ef33_row17_col0\" class=\"data row17 col0\" >-0.034699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row18\" class=\"row_heading level0 row18\" >RelationshipSatisfaction</th>\n",
       "      <td id=\"T_6ef33_row18_col0\" class=\"data row18 col0\" >-0.045872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row19\" class=\"row_heading level0 row19\" >DailyRate</th>\n",
       "      <td id=\"T_6ef33_row19_col0\" class=\"data row19 col0\" >-0.056652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row20\" class=\"row_heading level0 row20\" >TrainingTimesLastYear</th>\n",
       "      <td id=\"T_6ef33_row20_col0\" class=\"data row20 col0\" >-0.059478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row21\" class=\"row_heading level0 row21\" >WorkLifeBalance</th>\n",
       "      <td id=\"T_6ef33_row21_col0\" class=\"data row21 col0\" >-0.063939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row22\" class=\"row_heading level0 row22\" >EnvironmentSatisfaction</th>\n",
       "      <td id=\"T_6ef33_row22_col0\" class=\"data row22 col0\" >-0.103369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row23\" class=\"row_heading level0 row23\" >JobSatisfaction</th>\n",
       "      <td id=\"T_6ef33_row23_col0\" class=\"data row23 col0\" >-0.103481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row24\" class=\"row_heading level0 row24\" >JobInvolvement</th>\n",
       "      <td id=\"T_6ef33_row24_col0\" class=\"data row24 col0\" >-0.130016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row25\" class=\"row_heading level0 row25\" >YearsAtCompany</th>\n",
       "      <td id=\"T_6ef33_row25_col0\" class=\"data row25 col0\" >-0.134392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row26\" class=\"row_heading level0 row26\" >StockOptionLevel</th>\n",
       "      <td id=\"T_6ef33_row26_col0\" class=\"data row26 col0\" >-0.137145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row27\" class=\"row_heading level0 row27\" >YearsWithCurrManager</th>\n",
       "      <td id=\"T_6ef33_row27_col0\" class=\"data row27 col0\" >-0.156199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row28\" class=\"row_heading level0 row28\" >Age</th>\n",
       "      <td id=\"T_6ef33_row28_col0\" class=\"data row28 col0\" >-0.159205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row29\" class=\"row_heading level0 row29\" >MonthlyIncome</th>\n",
       "      <td id=\"T_6ef33_row29_col0\" class=\"data row29 col0\" >-0.159840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row30\" class=\"row_heading level0 row30\" >YearsInCurrentRole</th>\n",
       "      <td id=\"T_6ef33_row30_col0\" class=\"data row30 col0\" >-0.160545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row31\" class=\"row_heading level0 row31\" >JobLevel</th>\n",
       "      <td id=\"T_6ef33_row31_col0\" class=\"data row31 col0\" >-0.169105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6ef33_level0_row32\" class=\"row_heading level0 row32\" >TotalWorkingYears</th>\n",
       "      <td id=\"T_6ef33_row32_col0\" class=\"data row32 col0\" >-0.171063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25606666420>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.corr(numeric_only = True)\n",
    "\n",
    "# Focus correlation matrix on certain column\n",
    "col = 'Attrition'\n",
    "\n",
    "temp = temp[[col]]\n",
    "temp = temp.sort_values(by = col, ascending = False)\n",
    "\n",
    "# https://stackoverflow.com/questions/29432629\n",
    "temp.style.background_gradient(cmap = 'coolwarm', axis = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also describe the min, max, mean, etc from each column\n",
    "\n",
    "May be needed for `width_bucket` (e.g. age, salary) or other special SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EmployeeId</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>735.500000</td>\n",
       "      <td>424.496761</td>\n",
       "      <td>1.0</td>\n",
       "      <td>368.25</td>\n",
       "      <td>735.5</td>\n",
       "      <td>1102.75</td>\n",
       "      <td>1470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>36.923810</td>\n",
       "      <td>9.135373</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>43.00</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attrition</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.161224</td>\n",
       "      <td>0.367863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BusinessTravel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.086395</td>\n",
       "      <td>0.532170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DailyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>802.485714</td>\n",
       "      <td>403.509100</td>\n",
       "      <td>102.0</td>\n",
       "      <td>465.00</td>\n",
       "      <td>802.0</td>\n",
       "      <td>1157.00</td>\n",
       "      <td>1499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DepartmentN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.260544</td>\n",
       "      <td>0.527792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>9.192517</td>\n",
       "      <td>8.106864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.912925</td>\n",
       "      <td>1.024165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EducationFieldN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>1.915646</td>\n",
       "      <td>1.079401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.721769</td>\n",
       "      <td>1.093082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GenderN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.490065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HourlyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>65.891156</td>\n",
       "      <td>20.329428</td>\n",
       "      <td>30.0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>83.75</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobInvolvement</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.729932</td>\n",
       "      <td>0.711561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobLevel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.063946</td>\n",
       "      <td>1.106940</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobRoleN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>3.778912</td>\n",
       "      <td>2.126872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.728571</td>\n",
       "      <td>1.102846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaritalStatusN</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.764626</td>\n",
       "      <td>0.790757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>6502.931293</td>\n",
       "      <td>4707.956783</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>2911.00</td>\n",
       "      <td>4919.0</td>\n",
       "      <td>8379.00</td>\n",
       "      <td>19999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthlyRate</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>14313.103401</td>\n",
       "      <td>7117.786044</td>\n",
       "      <td>2094.0</td>\n",
       "      <td>8047.00</td>\n",
       "      <td>14235.5</td>\n",
       "      <td>20461.50</td>\n",
       "      <td>26999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.693197</td>\n",
       "      <td>2.498009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OverTime</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.282993</td>\n",
       "      <td>0.450606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>15.209524</td>\n",
       "      <td>3.659938</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PerformanceRating</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>3.153741</td>\n",
       "      <td>0.360824</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.712245</td>\n",
       "      <td>1.081209</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.793878</td>\n",
       "      <td>0.852077</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>11.279592</td>\n",
       "      <td>7.780782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.799320</td>\n",
       "      <td>1.289271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.761224</td>\n",
       "      <td>0.706476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>7.008163</td>\n",
       "      <td>6.126525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>4.229252</td>\n",
       "      <td>3.623137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>2.187755</td>\n",
       "      <td>3.222430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>4.123129</td>\n",
       "      <td>3.568136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsTest</th>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.280272</td>\n",
       "      <td>0.449285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           count          mean          std     min      25%  \\\n",
       "EmployeeId                1470.0    735.500000   424.496761     1.0   368.25   \n",
       "Age                       1470.0     36.923810     9.135373    18.0    30.00   \n",
       "Attrition                 1470.0      0.161224     0.367863     0.0     0.00   \n",
       "BusinessTravel            1470.0      1.086395     0.532170     0.0     1.00   \n",
       "DailyRate                 1470.0    802.485714   403.509100   102.0   465.00   \n",
       "DepartmentN               1470.0      1.260544     0.527792     0.0     1.00   \n",
       "DistanceFromHome          1470.0      9.192517     8.106864     1.0     2.00   \n",
       "Education                 1470.0      2.912925     1.024165     1.0     2.00   \n",
       "EducationFieldN           1470.0      1.915646     1.079401     0.0     1.00   \n",
       "EnvironmentSatisfaction   1470.0      2.721769     1.093082     1.0     2.00   \n",
       "GenderN                   1470.0      0.400000     0.490065     0.0     0.00   \n",
       "HourlyRate                1470.0     65.891156    20.329428    30.0    48.00   \n",
       "JobInvolvement            1470.0      2.729932     0.711561     1.0     2.00   \n",
       "JobLevel                  1470.0      2.063946     1.106940     1.0     1.00   \n",
       "JobRoleN                  1470.0      3.778912     2.126872     0.0     2.00   \n",
       "JobSatisfaction           1470.0      2.728571     1.102846     1.0     2.00   \n",
       "MaritalStatusN            1470.0      0.764626     0.790757     0.0     0.00   \n",
       "MonthlyIncome             1470.0   6502.931293  4707.956783  1009.0  2911.00   \n",
       "MonthlyRate               1470.0  14313.103401  7117.786044  2094.0  8047.00   \n",
       "NumCompaniesWorked        1470.0      2.693197     2.498009     0.0     1.00   \n",
       "OverTime                  1470.0      0.282993     0.450606     0.0     0.00   \n",
       "PercentSalaryHike         1470.0     15.209524     3.659938    11.0    12.00   \n",
       "PerformanceRating         1470.0      3.153741     0.360824     3.0     3.00   \n",
       "RelationshipSatisfaction  1470.0      2.712245     1.081209     1.0     2.00   \n",
       "StockOptionLevel          1470.0      0.793878     0.852077     0.0     0.00   \n",
       "TotalWorkingYears         1470.0     11.279592     7.780782     0.0     6.00   \n",
       "TrainingTimesLastYear     1470.0      2.799320     1.289271     0.0     2.00   \n",
       "WorkLifeBalance           1470.0      2.761224     0.706476     1.0     2.00   \n",
       "YearsAtCompany            1470.0      7.008163     6.126525     0.0     3.00   \n",
       "YearsInCurrentRole        1470.0      4.229252     3.623137     0.0     2.00   \n",
       "YearsSinceLastPromotion   1470.0      2.187755     3.222430     0.0     0.00   \n",
       "YearsWithCurrManager      1470.0      4.123129     3.568136     0.0     2.00   \n",
       "IsTest                    1470.0      0.280272     0.449285     0.0     0.00   \n",
       "\n",
       "                              50%       75%      max  \n",
       "EmployeeId                  735.5   1102.75   1470.0  \n",
       "Age                          36.0     43.00     60.0  \n",
       "Attrition                     0.0      0.00      1.0  \n",
       "BusinessTravel                1.0      1.00      2.0  \n",
       "DailyRate                   802.0   1157.00   1499.0  \n",
       "DepartmentN                   1.0      2.00      2.0  \n",
       "DistanceFromHome              7.0     14.00     29.0  \n",
       "Education                     3.0      4.00      5.0  \n",
       "EducationFieldN               2.0      2.00      5.0  \n",
       "EnvironmentSatisfaction       3.0      4.00      4.0  \n",
       "GenderN                       0.0      1.00      1.0  \n",
       "HourlyRate                   66.0     83.75    100.0  \n",
       "JobInvolvement                3.0      3.00      4.0  \n",
       "JobLevel                      2.0      3.00      5.0  \n",
       "JobRoleN                      3.0      5.00      8.0  \n",
       "JobSatisfaction               3.0      4.00      4.0  \n",
       "MaritalStatusN                1.0      1.00      2.0  \n",
       "MonthlyIncome              4919.0   8379.00  19999.0  \n",
       "MonthlyRate               14235.5  20461.50  26999.0  \n",
       "NumCompaniesWorked            2.0      4.00      9.0  \n",
       "OverTime                      0.0      1.00      1.0  \n",
       "PercentSalaryHike            14.0     18.00     25.0  \n",
       "PerformanceRating             3.0      3.00      4.0  \n",
       "RelationshipSatisfaction      3.0      4.00      4.0  \n",
       "StockOptionLevel              1.0      1.00      3.0  \n",
       "TotalWorkingYears            10.0     15.00     40.0  \n",
       "TrainingTimesLastYear         3.0      3.00      6.0  \n",
       "WorkLifeBalance               3.0      3.00      4.0  \n",
       "YearsAtCompany                5.0      9.00     40.0  \n",
       "YearsInCurrentRole            3.0      7.00     18.0  \n",
       "YearsSinceLastPromotion       1.0      3.00     15.0  \n",
       "YearsWithCurrManager          3.0      7.00     17.0  \n",
       "IsTest                        0.0      1.00      1.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring Metabase\n",
    "\n",
    "### 2.1. First Time Setup\n",
    "1. Run `run_metabase.sh`\n",
    "2. Open `http://localhost:3000` (Metabase URL)\n",
    "3. Setup username and password with something easy (e.g. `admin@example.com`, `admin456`)\n",
    "3. Fill other things in setup and connect to database (see Supabase - Project Settings - Database)\n",
    "4. Edit more through Metabase admin settings if necessary\n",
    "\n",
    "### 2.2. Add Model as Data Source\n",
    "Metabase model is an derived database table that should be used as main data source for questions and visualizations\n",
    "\n",
    "You can join tables, filter columns, or add engineered columns (e.g. binned age & income) without changing the database itself\n",
    "\n",
    "1. To add a new model, click New - Model - Use a native query\n",
    "2. Select your database, write your SQL query, and save when done\n",
    "3. A single model can (and should) be used by multiple questions and visualizations, try creating a broad one (contains many columns) if possible\n",
    "4. By using a single model in a dashboard, you can create filters (e.g. by age) that can be applied to all visualizations at once\n",
    "5. If you use more than 1 model, the filter won't be applied if there's no common column across models (I learned from my mistake)\n",
    "\n",
    "### 2.3. Add Question/Visualization from a Model\n",
    "1. Click New - Question, and add your previously created model\n",
    "2. Click the metrics you want to see (e.g. count of) and the column(s) to group by (e.g. age and attrition)\n",
    "3. Click Visualize - Visualization, to customize the graph (e.g. line chart)\n",
    "4. If the graph won't show due to ambiguous columns, click the gear icon and customize the axis manually\n",
    "5. Add a dashboard to show the visualization, and create more questions if necessary (e.g. gender and attrition)\n",
    "6. Remember that 1 model can be used by multiple questions, so creating 1 model for each question is very discouraged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For quick check of data and faster mass visualizations\n",
    "\n",
    "# filter = df['Attrition'] == 1\n",
    "\n",
    "# _ = df[filter].hist(\n",
    "#     [\n",
    "#         i for i in df.columns\n",
    "#         if i not in cat_col + ['EmployeeId', 'IsTest', 'Attrition']\n",
    "#         # Remove categorical columns too\n",
    "#         and i[-1] != 'N'\n",
    "#     ],\n",
    "#     figsize = (20, 20)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. ML Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Replace Outlier\n",
    "Outlier can be replaced using IQR method (or just skip this and use robust scaler later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">MonthlyIncome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19859.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19406.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>18711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>17123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>17639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19665.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>19094.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>18824.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>16581.0</td>\n",
       "      <td>16885.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MonthlyIncome         \n",
       "              self    other\n",
       "7          16581.0  19859.0\n",
       "17         16581.0  19406.0\n",
       "19         16581.0  18711.0\n",
       "27         16581.0  17123.0\n",
       "29         16581.0  19658.0\n",
       "...            ...      ...\n",
       "1373       16581.0  17639.0\n",
       "1414       16581.0  19665.0\n",
       "1425       16581.0  19094.0\n",
       "1444       16581.0  18824.0\n",
       "1460       16581.0  16885.0\n",
       "\n",
       "[114 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "# Only normalize rate and income\n",
    "# Normalizing other columns may cause weird issues\n",
    "col = [i for i in df.columns if 'Rate' in i or 'Income' in i]\n",
    "\n",
    "for c in col:\n",
    "    q1 = df[c].quantile(0.25)\n",
    "    q3 = df[c].quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "    iqr1 = q1 - 1.5 * iqr\n",
    "    iqr3 = q3 + 1.5 * iqr\n",
    "    # print(c, q1, q3, iqr, iqr1, iqr3)\n",
    "    \n",
    "    df.loc[ df[c] < iqr1, c ] = 0 if iqr1 < 0 else int(iqr1)\n",
    "    df.loc[ df[c] > iqr3, c ] = int(iqr3)\n",
    "\n",
    "display(df.compare(df_copy))\n",
    "del df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # See more details from Metabase\n",
    "# df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning is must be applied first to group continuous values so that it has limited range (e.g. age, salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pd.cut to use inclusive range\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html\n",
    "bin_arg = {'right': True, 'include_lowest': True}\n",
    "\n",
    "# Bins/groups feature range first\n",
    "# https://stackoverflow.com/questions/45273731\n",
    "\n",
    "temp = [18] + [i * 5 + 20 for i in range(9)] # 18, 20, 25, 30, ..., 60\n",
    "df['Age'] = pd.cut(df['Age'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 5 for i in range(7)] # 0, 5, 10, 15, ..., 30\n",
    "df['DistanceFromHome'] = pd.cut(df['DistanceFromHome'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 2000 for i in range(16)] # 0, 2000, 4000, 6000, ..., 30000\n",
    "df['MonthlyIncome'] = pd.cut(df['MonthlyIncome'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['MonthlyRate'] = pd.cut(df['MonthlyRate'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [0, 1, 2, 5, 10]\n",
    "df['NumCompaniesWorked'] = pd.cut(df['NumCompaniesWorked'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [i * 5 for i in range(6)] # 0, 5, 10, 15, ..., 25\n",
    "df['PercentSalaryHike'] = pd.cut(df['PercentSalaryHike'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "\n",
    "temp = [0, 2] + [ (i + 1) * 5 for i in range(9)] # 0, 2, 5, 10, 15, ..., 45\n",
    "df['YearsAtCompany'] = pd.cut(df['YearsAtCompany'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsInCurrentRole'] = pd.cut(df['YearsInCurrentRole'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsSinceLastPromotion'] = pd.cut(df['YearsSinceLastPromotion'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['YearsWithCurrManager'] = pd.cut(df['YearsWithCurrManager'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)\n",
    "df['TotalWorkingYears'] = pd.cut(df['TotalWorkingYears'], bins = temp, labels = [i for i in range(len(temp) - 1)], **bin_arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some readings, it seems that WOE and target encoder can also be used for non-categorical features\n",
    "\n",
    "This will stimulate relation between those features and target variable (attrition) that may be understood by ML model (see [reference](https://www.datacamp.com/tutorial/categorical-data))\n",
    "\n",
    "For example, we can also apply this on ordinal values (WLB, job satisfaction, etc) or even continuous numerical values (age, salary, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeN</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>BusinessTravelN</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DepartmentN</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>...</th>\n",
       "      <th>WorkLifeBalanceN</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsAtCompanyN</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsInCurrentRoleN</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsSinceLastPromotionN</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "      <th>YearsWithCurrManagerN</th>\n",
       "      <th>IsTest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>1444</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>0.189956</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>2</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1141</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309554</td>\n",
       "      <td>0</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1323</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>2</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>2</td>\n",
       "      <td>0.121966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>555</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0.206156</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>4</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.101520</td>\n",
       "      <td>2</td>\n",
       "      <td>0.121966</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>1194</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>0.138418</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>1</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>1</td>\n",
       "      <td>0.116487</td>\n",
       "      <td>0</td>\n",
       "      <td>0.169545</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeId Age      AgeN  Attrition  BusinessTravel  BusinessTravelN  \\\n",
       "0           1   4  0.090735          0               2         0.248661   \n",
       "1           2   4  0.090735          1               1         0.149579   \n",
       "2           3   7  0.104574          1               1         0.149579   \n",
       "3           4   5  0.093970          0               2         0.248661   \n",
       "4           5   4  0.090735          0               1         0.149579   \n",
       "\n",
       "   DailyRate              Department  DepartmentN DistanceFromHome  ...  \\\n",
       "0       1444         Human Resources     0.189956                0  ...   \n",
       "1       1141  Research & Development     0.138418                2  ...   \n",
       "2       1323  Research & Development     0.138418                0  ...   \n",
       "3        555                   Sales     0.206156                5  ...   \n",
       "4       1194  Research & Development     0.138418                0  ...   \n",
       "\n",
       "   WorkLifeBalanceN  YearsAtCompany YearsAtCompanyN  YearsInCurrentRole  \\\n",
       "0          0.142236               2        0.122836                   0   \n",
       "1          0.309554               0        0.297628                   0   \n",
       "2          0.176364               2        0.122836                   0   \n",
       "3          0.176364               4        0.070050                   1   \n",
       "4          0.142236               1        0.138295                   1   \n",
       "\n",
       "   YearsInCurrentRoleN  YearsSinceLastPromotion  YearsSinceLastPromotionN  \\\n",
       "0             0.225730                        0                  0.169545   \n",
       "1             0.225730                        0                  0.169545   \n",
       "2             0.225730                        0                  0.169545   \n",
       "3             0.116487                        1                  0.101520   \n",
       "4             0.116487                        0                  0.169545   \n",
       "\n",
       "  YearsWithCurrManager  YearsWithCurrManagerN  IsTest  \n",
       "0                    0               0.213667       1  \n",
       "1                    0               0.213667       0  \n",
       "2                    2               0.121966       0  \n",
       "3                    2               0.121966       0  \n",
       "4                    0               0.213667       1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See previous cell\n",
    "bin_col = [\n",
    "    'Age','DistanceFromHome','MonthlyIncome','MonthlyRate','NumCompaniesWorked','PercentSalaryHike',\n",
    "    'YearsAtCompany','YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager','TotalWorkingYears'\n",
    "]\n",
    "\n",
    "# Previously it was encoded only using label encoder\n",
    "cat_col = ['Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus']\n",
    "\n",
    "# Other columns, mostly ordinal (if not all)\n",
    "other_col = [\n",
    "    'BusinessTravel', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction',\n",
    "    'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TrainingTimesLastYear',\n",
    "    'WorkLifeBalance'\n",
    "]\n",
    "\n",
    "# Columns to be encoded\n",
    "enc_col = bin_col + cat_col + other_col\n",
    "cat_encoder = TargetEncoder()\n",
    "\n",
    "new_col = [i + 'N' for i in enc_col]\n",
    "cat_encoder.fit(df[enc_col], df['Attrition'])\n",
    "df[new_col] = cat_encoder.transform(df[enc_col])\n",
    "\n",
    "# Save encoder to use later (without fitting again)\n",
    "with open('model/encoder.lz4', 'wb') as f:\n",
    "        joblib.dump(cat_encoder, f)\n",
    "\n",
    "# Sort column names to fix newly added columns\n",
    "df = df.reindex(sorted(df.columns), axis = 1)\n",
    "\n",
    "# Place employee id at beginning and is test at the end\n",
    "temp = [i for i in df.columns if i not in ['EmployeeId', 'IsTest']]\n",
    "df = df[['EmployeeId'] + temp + ['IsTest']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Feature Removal/Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove original columns that have been converted to numerical or processed using encoder\n",
    "\n",
    "Most ML models can't process non-numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed: ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'EducationField', 'Education', 'EnvironmentSatisfaction', 'Gender', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
      "Kept: ['AgeN', 'BusinessTravelN', 'DepartmentN', 'DistanceFromHomeN', 'EducationFieldN', 'EducationN', 'EnvironmentSatisfactionN', 'GenderN', 'JobInvolvementN', 'JobLevelN', 'JobRoleN', 'JobSatisfactionN', 'MaritalStatusN', 'MonthlyIncomeN', 'MonthlyRateN', 'NumCompaniesWorkedN', 'OverTimeN', 'PercentSalaryHikeN', 'PerformanceRatingN', 'RelationshipSatisfactionN', 'StockOptionLevelN', 'TotalWorkingYearsN', 'TrainingTimesLastYearN', 'WorkLifeBalanceN', 'YearsAtCompanyN', 'YearsInCurrentRoleN', 'YearsSinceLastPromotionN', 'YearsWithCurrManagerN']\n"
     ]
    }
   ],
   "source": [
    "temp = [i[:-1] for i in df.columns if i[-1] == 'N']\n",
    "print('Removed:', temp)\n",
    "print('Kept:', [i + 'N' for i in temp])\n",
    "\n",
    "df.drop(temp, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unimportant features that have little effect or too dependent on each other (e.g. hourly/daily/monthly rate -> monthly income)\n",
    "\n",
    "We should prioritize strong/independent features first, and filter those that are less relevant (see [here](https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80#Feature-Engineering) and [there](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/))\n",
    "\n",
    "**Note 1:** The first reference applied SMOTE on test data, which is a bad practice and inflated the F1 score\n",
    "\n",
    "**Note 2:** Some feature selection methods may only work for numerical/categorical data, not both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0722e_row0_col1 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row1_col1 {\n",
       "  background-color: #b50927;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row2_col1 {\n",
       "  background-color: #ba162b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row3_col1 {\n",
       "  background-color: #bd1f2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row4_col1 {\n",
       "  background-color: #d0473d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row5_col1 {\n",
       "  background-color: #d1493f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row6_col1 {\n",
       "  background-color: #d55042;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row7_col1 {\n",
       "  background-color: #e36b54;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row8_col1 {\n",
       "  background-color: #f29274;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row9_col1 {\n",
       "  background-color: #f6a283;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row10_col1 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row11_col1 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row12_col1 {\n",
       "  background-color: #f2c9b4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row13_col1 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row14_col1, #T_0722e_row15_col1 {\n",
       "  background-color: #e8d6cc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row16_col1 {\n",
       "  background-color: #e5d8d1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row17_col1 {\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row18_col1 {\n",
       "  background-color: #e2dad5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row19_col1 {\n",
       "  background-color: #dcdddd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row20_col1 {\n",
       "  background-color: #d4dbe6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row21_col1 {\n",
       "  background-color: #cdd9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row22_col1 {\n",
       "  background-color: #bfd3f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row23_col1 {\n",
       "  background-color: #bad0f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row24_col1 {\n",
       "  background-color: #abc8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row25_col1 {\n",
       "  background-color: #98b9ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row26_col1 {\n",
       "  background-color: #8fb1fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_0722e_row27_col1 {\n",
       "  background-color: #7a9df8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row28_col1 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row29_col1 {\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row30_col1 {\n",
       "  background-color: #506bda;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_0722e_row31_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0722e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0722e_level0_col0\" class=\"col_heading level0 col0\" >column</th>\n",
       "      <th id=\"T_0722e_level0_col1\" class=\"col_heading level0 col1\" >importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0722e_row0_col0\" class=\"data row0 col0\" >TotalWorkingYearsN</td>\n",
       "      <td id=\"T_0722e_row0_col1\" class=\"data row0 col1\" >0.247316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0722e_row1_col0\" class=\"data row1 col0\" >OverTimeN</td>\n",
       "      <td id=\"T_0722e_row1_col1\" class=\"data row1 col1\" >0.246118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0722e_row2_col0\" class=\"data row2 col0\" >JobRoleN</td>\n",
       "      <td id=\"T_0722e_row2_col1\" class=\"data row2 col1\" >0.242134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0722e_row3_col0\" class=\"data row3 col0\" >MonthlyIncomeN</td>\n",
       "      <td id=\"T_0722e_row3_col1\" class=\"data row3 col1\" >0.239267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0722e_row4_col0\" class=\"data row4 col0\" >AgeN</td>\n",
       "      <td id=\"T_0722e_row4_col1\" class=\"data row4 col1\" >0.223413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0722e_row5_col0\" class=\"data row5 col0\" >JobLevelN</td>\n",
       "      <td id=\"T_0722e_row5_col1\" class=\"data row5 col1\" >0.222125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0722e_row6_col0\" class=\"data row6 col0\" >YearsAtCompanyN</td>\n",
       "      <td id=\"T_0722e_row6_col1\" class=\"data row6 col1\" >0.217988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0722e_row7_col0\" class=\"data row7 col0\" >StockOptionLevelN</td>\n",
       "      <td id=\"T_0722e_row7_col1\" class=\"data row7 col1\" >0.203035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0722e_row8_col0\" class=\"data row8 col0\" >MaritalStatusN</td>\n",
       "      <td id=\"T_0722e_row8_col1\" class=\"data row8 col1\" >0.177211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0722e_row9_col0\" class=\"data row9 col0\" >YearsInCurrentRoleN</td>\n",
       "      <td id=\"T_0722e_row9_col1\" class=\"data row9 col1\" >0.165077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_0722e_row10_col0\" class=\"data row10 col0\" >YearsWithCurrManagerN</td>\n",
       "      <td id=\"T_0722e_row10_col1\" class=\"data row10 col1\" >0.141636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_0722e_row11_col0\" class=\"data row11 col0\" >JobInvolvementN</td>\n",
       "      <td id=\"T_0722e_row11_col1\" class=\"data row11 col1\" >0.139217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_0722e_row12_col0\" class=\"data row12 col0\" >BusinessTravelN</td>\n",
       "      <td id=\"T_0722e_row12_col1\" class=\"data row12 col1\" >0.128260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_0722e_row13_col0\" class=\"data row13 col0\" >EnvironmentSatisfactionN</td>\n",
       "      <td id=\"T_0722e_row13_col1\" class=\"data row13 col1\" >0.123729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_0722e_row14_col0\" class=\"data row14 col0\" >DistanceFromHomeN</td>\n",
       "      <td id=\"T_0722e_row14_col1\" class=\"data row14 col1\" >0.109274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_0722e_row15_col0\" class=\"data row15 col0\" >JobSatisfactionN</td>\n",
       "      <td id=\"T_0722e_row15_col1\" class=\"data row15 col1\" >0.109125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_0722e_row16_col0\" class=\"data row16 col0\" >WorkLifeBalanceN</td>\n",
       "      <td id=\"T_0722e_row16_col1\" class=\"data row16 col1\" >0.105381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_0722e_row17_col0\" class=\"data row17 col0\" >EducationFieldN</td>\n",
       "      <td id=\"T_0722e_row17_col1\" class=\"data row17 col1\" >0.104399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_0722e_row18_col0\" class=\"data row18 col0\" >TrainingTimesLastYearN</td>\n",
       "      <td id=\"T_0722e_row18_col1\" class=\"data row18 col1\" >0.101502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_0722e_row19_col0\" class=\"data row19 col0\" >MonthlyRateN</td>\n",
       "      <td id=\"T_0722e_row19_col1\" class=\"data row19 col1\" >0.094489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_0722e_row20_col0\" class=\"data row20 col0\" >DepartmentN</td>\n",
       "      <td id=\"T_0722e_row20_col1\" class=\"data row20 col1\" >0.085698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_0722e_row21_col0\" class=\"data row21 col0\" >NumCompaniesWorkedN</td>\n",
       "      <td id=\"T_0722e_row21_col1\" class=\"data row21 col1\" >0.077756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_0722e_row22_col0\" class=\"data row22 col0\" >YearsSinceLastPromotionN</td>\n",
       "      <td id=\"T_0722e_row22_col1\" class=\"data row22 col1\" >0.063805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_0722e_row23_col0\" class=\"data row23 col0\" >RelationshipSatisfactionN</td>\n",
       "      <td id=\"T_0722e_row23_col1\" class=\"data row23 col1\" >0.059711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_0722e_row24_col0\" class=\"data row24 col0\" >EducationN</td>\n",
       "      <td id=\"T_0722e_row24_col1\" class=\"data row24 col1\" >0.045728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_0722e_row25_col0\" class=\"data row25 col0\" >GenderN</td>\n",
       "      <td id=\"T_0722e_row25_col1\" class=\"data row25 col1\" >0.029453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_0722e_row26_col0\" class=\"data row26 col0\" >PercentSalaryHikeN</td>\n",
       "      <td id=\"T_0722e_row26_col1\" class=\"data row26 col1\" >0.020810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_0722e_row27_col0\" class=\"data row27 col0\" >PerformanceRatingN</td>\n",
       "      <td id=\"T_0722e_row27_col1\" class=\"data row27 col1\" >0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_0722e_row28_col0\" class=\"data row28 col0\" >EmployeeId</td>\n",
       "      <td id=\"T_0722e_row28_col1\" class=\"data row28 col1\" >-0.000190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_0722e_row29_col0\" class=\"data row29 col0\" >HourlyRate</td>\n",
       "      <td id=\"T_0722e_row29_col1\" class=\"data row29 col1\" >-0.006846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_0722e_row30_col0\" class=\"data row30 col0\" >IsTest</td>\n",
       "      <td id=\"T_0722e_row30_col1\" class=\"data row30 col1\" >-0.034699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0722e_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_0722e_row31_col0\" class=\"data row31 col0\" >DailyRate</td>\n",
       "      <td id=\"T_0722e_row31_col1\" class=\"data row31 col1\" >-0.056652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x25606b1a720>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance = r_regression(\n",
    "    X = df[[i for i in df.columns if i != 'Attrition']],\n",
    "    y = df['Attrition']\n",
    ")\n",
    "\n",
    "temp = pd.DataFrame({\n",
    "    'column': [i for i in df.columns if i != 'Attrition'],\n",
    "    'importance': importance\n",
    "})\n",
    "\n",
    "temp = temp.sort_values(by = 'importance', ascending = False)\n",
    "temp = temp.reset_index(drop = True)\n",
    "temp.style.background_gradient(cmap = 'coolwarm', axis = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that target encoder can change the data relationship, especially those nearing the range -1 to the opposite side aka 1 (see previous correlation matrix for comparison)\n",
    "\n",
    "The affected columns still have strong correlation (either near 1 or -1 originally), but now always move the same way as attrition (more XXX = more attrition chance), as opposed of before, where some column may have this kind of relationship: more XXX = less attrition chance\n",
    "\n",
    "**Note:** Correlation matrix above can't be used directly as conclusion since each possible values are replaced/scored based on the amount of attrition (not sorted in a standard way). It's meant to be used by ML model only unless you know what you're doing\n",
    "\n",
    "Now, we will only keep columns with importance more than 0.1 (or less than -0.1), without removing `EmployeeId`, `IsTest`, and `Attrition` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>TotalWorkingYearsN</th>\n",
       "      <th>OverTimeN</th>\n",
       "      <th>JobRoleN</th>\n",
       "      <th>MonthlyIncomeN</th>\n",
       "      <th>AgeN</th>\n",
       "      <th>JobLevelN</th>\n",
       "      <th>YearsAtCompanyN</th>\n",
       "      <th>StockOptionLevelN</th>\n",
       "      <th>MaritalStatusN</th>\n",
       "      <th>...</th>\n",
       "      <th>JobInvolvementN</th>\n",
       "      <th>BusinessTravelN</th>\n",
       "      <th>EnvironmentSatisfactionN</th>\n",
       "      <th>DistanceFromHomeN</th>\n",
       "      <th>JobSatisfactionN</th>\n",
       "      <th>WorkLifeBalanceN</th>\n",
       "      <th>EducationFieldN</th>\n",
       "      <th>TrainingTimesLastYearN</th>\n",
       "      <th>IsTest</th>\n",
       "      <th>Attrition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.229057</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.134427</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.125805</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.069035</td>\n",
       "      <td>0.113303</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.097456</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.253069</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.309554</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.122836</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.253069</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.082971</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.174802</td>\n",
       "      <td>0.113871</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0.047526</td>\n",
       "      <td>0.070050</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.149551</td>\n",
       "      <td>0.164275</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.219659</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.179123</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1466</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.069267</td>\n",
       "      <td>0.103084</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.146850</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.255039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>1467</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.113871</td>\n",
       "      <td>0.123310</td>\n",
       "      <td>0.146850</td>\n",
       "      <td>0.065213</td>\n",
       "      <td>0.176280</td>\n",
       "      <td>0.101041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189249</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.134581</td>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.228072</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.140567</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1468</td>\n",
       "      <td>0.434971</td>\n",
       "      <td>0.304747</td>\n",
       "      <td>0.238978</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.212685</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.124858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.149579</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.216783</td>\n",
       "      <td>0.113367</td>\n",
       "      <td>0.168582</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1469</td>\n",
       "      <td>0.149935</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.160960</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.090735</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>0.094031</td>\n",
       "      <td>0.101041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144027</td>\n",
       "      <td>0.080294</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.183799</td>\n",
       "      <td>0.165149</td>\n",
       "      <td>0.142236</td>\n",
       "      <td>0.146887</td>\n",
       "      <td>0.274661</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1470</td>\n",
       "      <td>0.434971</td>\n",
       "      <td>0.104402</td>\n",
       "      <td>0.392652</td>\n",
       "      <td>0.233147</td>\n",
       "      <td>0.546509</td>\n",
       "      <td>0.263083</td>\n",
       "      <td>0.297628</td>\n",
       "      <td>0.243878</td>\n",
       "      <td>0.255039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333910</td>\n",
       "      <td>0.248661</td>\n",
       "      <td>0.136912</td>\n",
       "      <td>0.137691</td>\n",
       "      <td>0.228072</td>\n",
       "      <td>0.176364</td>\n",
       "      <td>0.241597</td>\n",
       "      <td>0.117926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1470 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  TotalWorkingYearsN  OverTimeN  JobRoleN  MonthlyIncomeN  \\\n",
       "0              1            0.149935   0.304747  0.229057        0.233147   \n",
       "1              2            0.125805   0.104402  0.069035        0.113303   \n",
       "2              3            0.100942   0.304747  0.160960        0.233147   \n",
       "3              4            0.082971   0.104402  0.174802        0.113871   \n",
       "4              5            0.100942   0.104402  0.160960        0.233147   \n",
       "...          ...                 ...        ...       ...             ...   \n",
       "1465        1466            0.149935   0.304747  0.069267        0.103084   \n",
       "1466        1467            0.100942   0.104402  0.025306        0.113871   \n",
       "1467        1468            0.434971   0.304747  0.238978        0.233147   \n",
       "1468        1469            0.149935   0.104402  0.160960        0.233147   \n",
       "1469        1470            0.434971   0.104402  0.392652        0.233147   \n",
       "\n",
       "          AgeN  JobLevelN  YearsAtCompanyN  StockOptionLevelN  MaritalStatusN  \\\n",
       "0     0.090735   0.263083         0.122836           0.094031        0.124858   \n",
       "1     0.090735   0.097456         0.297628           0.243878        0.124858   \n",
       "2     0.104574   0.263083         0.122836           0.176280        0.124858   \n",
       "3     0.093970   0.047526         0.070050           0.094031        0.124858   \n",
       "4     0.090735   0.263083         0.138295           0.176280        0.124858   \n",
       "...        ...        ...              ...                ...             ...   \n",
       "1465  0.090735   0.146850         0.297628           0.243878        0.255039   \n",
       "1466  0.123310   0.146850         0.065213           0.176280        0.101041   \n",
       "1467  0.212685   0.263083         0.297628           0.243878        0.124858   \n",
       "1468  0.090735   0.263083         0.138295           0.094031        0.101041   \n",
       "1469  0.546509   0.263083         0.297628           0.243878        0.255039   \n",
       "\n",
       "      ...  JobInvolvementN  BusinessTravelN  EnvironmentSatisfactionN  \\\n",
       "0     ...         0.144027         0.248661                  0.134581   \n",
       "1     ...         0.333910         0.149579                  0.253069   \n",
       "2     ...         0.144027         0.149579                  0.253069   \n",
       "3     ...         0.144027         0.248661                  0.136912   \n",
       "4     ...         0.144027         0.149579                  0.136912   \n",
       "...   ...              ...              ...                       ...   \n",
       "1465  ...         0.144027         0.149579                  0.136912   \n",
       "1466  ...         0.189249         0.149579                  0.134581   \n",
       "1467  ...         0.144027         0.149579                  0.136912   \n",
       "1468  ...         0.144027         0.080294                  0.136912   \n",
       "1469  ...         0.333910         0.248661                  0.136912   \n",
       "\n",
       "      DistanceFromHomeN  JobSatisfactionN  WorkLifeBalanceN  EducationFieldN  \\\n",
       "0              0.137691          0.164275          0.142236         0.134427   \n",
       "1              0.216783          0.164275          0.309554         0.135823   \n",
       "2              0.137691          0.165149          0.176364         0.146887   \n",
       "3              0.149551          0.164275          0.176364         0.219659   \n",
       "4              0.137691          0.165149          0.142236         0.135823   \n",
       "...                 ...               ...               ...              ...   \n",
       "1465           0.137691          0.165149          0.176364         0.146887   \n",
       "1466           0.183799          0.228072          0.142236         0.146887   \n",
       "1467           0.216783          0.113367          0.168582         0.146887   \n",
       "1468           0.183799          0.165149          0.142236         0.146887   \n",
       "1469           0.137691          0.228072          0.176364         0.241597   \n",
       "\n",
       "      TrainingTimesLastYearN  IsTest  Attrition  \n",
       "0                   0.179123       1          0  \n",
       "1                   0.179123       0          1  \n",
       "2                   0.179123       0          1  \n",
       "3                   0.179123       0          0  \n",
       "4                   0.179123       1          0  \n",
       "...                      ...     ...        ...  \n",
       "1465                0.210884       0          0  \n",
       "1466                0.140567       1          0  \n",
       "1467                0.210884       0          1  \n",
       "1468                0.274661       0          0  \n",
       "1469                0.117926       0          1  \n",
       "\n",
       "[1470 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = temp[(temp['importance'] >= 0.1) | (temp['importance'] <= -0.1)]['column']\n",
    "col = ['EmployeeId'] + list(col) + ['IsTest', 'Attrition']\n",
    "\n",
    "df = df[col]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Data Scaling/Normalization and Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data using standard/min-max scaler\n",
    "\n",
    "**Note:** Columns that are affected by target encoder can also be ignored (range should already between 0-1)\n",
    "\n",
    "**Alternative:** use robust scaler if not using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [i for i in df.columns if i != 'EmployeeId' and i != 'IsTest']\n",
    "x_col = [i for i in col if i != 'Attrition']\n",
    "y_col = ['Attrition']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Use \"inverse_transform\" to inverse later\n",
    "df[x_col] = scaler.fit_transform(df[x_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for training (x = feature, y = label)\n",
    "x_train = df.loc[df['IsTest'] == 0, x_col]\n",
    "y_train = df.loc[df['IsTest'] == 0, ['Attrition']]\n",
    "\n",
    "# Data for testing\n",
    "x_test = df.loc[df['IsTest'] == 1, x_col]\n",
    "y_test = df.loc[df['IsTest'] == 1, ['Attrition']]\n",
    "\n",
    "len(x_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [SMOTE](https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/) and set class weight to workaround imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components = 2, random_state = 12)\n",
    "# x_pcay = pca.fit_transform(x_train[y_train['Attrition'] == 0])\n",
    "# x_pcan = pca.fit_transform(x_train[y_train['Attrition'] == 1])\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (12, 5))\n",
    "# ax1.scatter(x_pcay[:,0], x_pcay[:,1], label = '0')\n",
    "# ax1.scatter(x_pcan[:,0], x_pcan[:,1], label = '1')\n",
    "# ax1.legend(loc = 'upper left')\n",
    "# ax1.set_title('Before using SMOTE')\n",
    "\n",
    "# #  ====================\n",
    "\n",
    "# smote = SMOTETomek(random_state = 12)\n",
    "# x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "# print('After SMOTE:', y_train.value_counts())\n",
    "\n",
    "# #  ====================\n",
    "\n",
    "# x_pcay = pca.fit_transform(x_train[y_train['Attrition'] == 0])\n",
    "# x_pcan = pca.fit_transform(x_train[y_train['Attrition'] == 1])\n",
    "\n",
    "# ax2.scatter(x_pcay[:,0], x_pcay[:,1], label = '0')\n",
    "# ax2.scatter(x_pcan[:,0], x_pcan[:,1], label = '1')\n",
    "# ax2.legend(loc = 'upper left')\n",
    "# ax2.set_title('After using SMOTE')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, it seems that SMOTE may actually make things worse\n",
    "\n",
    "So we can just skip it and apply only class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 4.910614525139665}\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/66501676\n",
    "# Alternatively, use Sklearn \"compute_class_weight\"\n",
    "\n",
    "class_weights = {}\n",
    "samples = list(y_train.value_counts())\n",
    "sample_max = np.max(samples)\n",
    "\n",
    "for i in range (len(samples)):\n",
    "    class_weights[i] = sample_max / samples[i]\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After multiple testings, I decided to use neural network (NN) [ensembling](https://www.tensorflow.org/decision_forests/tutorials/model_composition_colab) (since TFDF v1.8.1 only support Linux)\n",
    "\n",
    "**Alternative:** use Sklearn [ensembling](https://scikit-learn.org/stable/modules/ensemble.html) (non-NN approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,049</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ -                 │\n",
       "│ batch_normalization │                   │            │                   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ -                 │\n",
       "│ batch_normalizatio… │                   │            │                   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ -                 │\n",
       "│ batch_normalizatio… │                   │            │                   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,153</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_5      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,153</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_8        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_7      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_8      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PReLU</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Average</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ sequential_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │                   │            │ sequential_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m14,049\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,560\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m128\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ -                 │\n",
       "│ batch_normalization │                   │            │                   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m64\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ -                 │\n",
       "│ batch_normalizatio… │                   │            │                   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m32\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m128\u001b[0m │ -                 │\n",
       "│ batch_normalizatio… │                   │            │                   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m13,153\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,560\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m128\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m64\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_5      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m32\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dropout_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │     \u001b[38;5;34m13,153\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_8        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m2,560\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m128\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_7      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m64\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m2,080\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ p_re_lu_8      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m32\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mPReLU\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│    └ dense_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average (\u001b[38;5;33mAverage\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ sequential_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │                   │            │ sequential_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,355</span> (157.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m40,355\u001b[0m (157.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,907</span> (155.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m39,907\u001b[0m (155.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_input = keras.layers.Input(shape = (len(x_col),))\n",
    "\n",
    "model1 = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model2 = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.1),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.1),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dropout(rate = 0.2),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model3 = keras.models.Sequential([\n",
    "    keras.layers.Dense(units = 128),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(units = 64),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(units = 32),\n",
    "    keras.layers.PReLU(),\n",
    "\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model_output = [\n",
    "    model(model_input) for model in [model1, model2, model3]\n",
    "]\n",
    "\n",
    "model_ensemble = keras.models.Model(\n",
    "    inputs = model_input,\n",
    "    # Fix error due to Keras layer passed down to TF function\n",
    "    # See commit before this for the difference\n",
    "    # https://stackoverflow.com/questions/67647843\n",
    "    outputs = keras.layers.Average()(model_output)\n",
    ")\n",
    "\n",
    "model_ensemble.summary(expand_nested = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.5743 - auc: 0.5951 - loss: 1.1157 \n",
      "Epoch 1: val_auc improved from -inf to 0.78597, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step - acc: 0.5806 - auc: 0.6082 - loss: 1.1097 - val_acc: 0.2257 - val_auc: 0.7860 - val_loss: 0.7293 - learning_rate: 0.0100\n",
      "Epoch 2/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - acc: 0.7344 - auc: 0.8667 - loss: 0.9091\n",
      "Epoch 2: val_auc improved from 0.78597 to 0.80672, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.7188 - auc: 0.8622 - loss: 0.9164 - val_acc: 0.4854 - val_auc: 0.8067 - val_loss: 0.6973 - learning_rate: 0.0100\n",
      "Epoch 3/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - acc: 0.7500 - auc: 0.8739 - loss: 0.8433\n",
      "Epoch 3: val_auc improved from 0.80672 to 0.81349, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.7530 - auc: 0.8718 - loss: 0.8908 - val_acc: 0.7451 - val_auc: 0.8135 - val_loss: 0.6425 - learning_rate: 0.0100\n",
      "Epoch 4/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - acc: 0.7031 - auc: 0.9023 - loss: 0.7588\n",
      "Epoch 4: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.7777 - auc: 0.8850 - loss: 0.8706 - val_acc: 0.8422 - val_auc: 0.8135 - val_loss: 0.5918 - learning_rate: 0.0100\n",
      "Epoch 5/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8090 - auc: 0.8937 - loss: 0.8914 \n",
      "Epoch 5: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8087 - auc: 0.8940 - loss: 0.8890 - val_acc: 0.8471 - val_auc: 0.8111 - val_loss: 0.5612 - learning_rate: 0.0100\n",
      "Epoch 6/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8474 - auc: 0.9047 - loss: 0.8066 \n",
      "Epoch 6: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8465 - auc: 0.9047 - loss: 0.8072 - val_acc: 0.8665 - val_auc: 0.8117 - val_loss: 0.5297 - learning_rate: 0.0100\n",
      "Epoch 7/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8362 - auc: 0.9098 - loss: 0.7868 \n",
      "Epoch 7: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8365 - auc: 0.9100 - loss: 0.7874 - val_acc: 0.8714 - val_auc: 0.8076 - val_loss: 0.5026 - learning_rate: 0.0100\n",
      "Epoch 8/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8577 - auc: 0.9298 - loss: 0.7640 \n",
      "Epoch 8: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8569 - auc: 0.9290 - loss: 0.7646 - val_acc: 0.8762 - val_auc: 0.8049 - val_loss: 0.4822 - learning_rate: 0.0100\n",
      "Epoch 9/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8645 - auc: 0.9295 - loss: 0.7469 \n",
      "Epoch 9: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8634 - auc: 0.9268 - loss: 0.7491 - val_acc: 0.8786 - val_auc: 0.8017 - val_loss: 0.4621 - learning_rate: 0.0100\n",
      "Epoch 10/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8693 - auc: 0.9307 - loss: 0.7180 \n",
      "Epoch 10: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8642 - auc: 0.9280 - loss: 0.7227 - val_acc: 0.8762 - val_auc: 0.8009 - val_loss: 0.4551 - learning_rate: 0.0100\n",
      "Epoch 11/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8650 - auc: 0.9230 - loss: 0.6957 \n",
      "Epoch 11: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8659 - auc: 0.9227 - loss: 0.7033 - val_acc: 0.8762 - val_auc: 0.8002 - val_loss: 0.4415 - learning_rate: 0.0100\n",
      "Epoch 12/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8719 - auc: 0.9204 - loss: 0.6982 \n",
      "Epoch 12: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8699 - auc: 0.9220 - loss: 0.6997 - val_acc: 0.8811 - val_auc: 0.7982 - val_loss: 0.4332 - learning_rate: 0.0100\n",
      "Epoch 13/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9017 - auc: 0.9455 - loss: 0.6471  \n",
      "Epoch 13: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8995 - auc: 0.9435 - loss: 0.6536 - val_acc: 0.8738 - val_auc: 0.7977 - val_loss: 0.4307 - learning_rate: 0.0100\n",
      "Epoch 14/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8708 - auc: 0.9423 - loss: 0.6309 \n",
      "Epoch 14: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8716 - auc: 0.9419 - loss: 0.6327 - val_acc: 0.8689 - val_auc: 0.7999 - val_loss: 0.4122 - learning_rate: 0.0100\n",
      "Epoch 15/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8597 - auc: 0.9290 - loss: 0.6474 \n",
      "Epoch 15: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8623 - auc: 0.9299 - loss: 0.6482 - val_acc: 0.8592 - val_auc: 0.8032 - val_loss: 0.4243 - learning_rate: 0.0100\n",
      "Epoch 16/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8535 - auc: 0.9338 - loss: 0.6549 \n",
      "Epoch 16: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8574 - auc: 0.9348 - loss: 0.6483 - val_acc: 0.8689 - val_auc: 0.8052 - val_loss: 0.3999 - learning_rate: 0.0100\n",
      "Epoch 17/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8714 - auc: 0.9406 - loss: 0.6151 \n",
      "Epoch 17: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8735 - auc: 0.9411 - loss: 0.6152 - val_acc: 0.8568 - val_auc: 0.8060 - val_loss: 0.4025 - learning_rate: 0.0100\n",
      "Epoch 18/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8798 - auc: 0.9502 - loss: 0.5713\n",
      "Epoch 18: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.8797 - auc: 0.9499 - loss: 0.5729 - val_acc: 0.8617 - val_auc: 0.8077 - val_loss: 0.3890 - learning_rate: 0.0100\n",
      "Epoch 19/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8876 - auc: 0.9460 - loss: 0.5670 \n",
      "Epoch 19: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8883 - auc: 0.9448 - loss: 0.5789 - val_acc: 0.8689 - val_auc: 0.8067 - val_loss: 0.4160 - learning_rate: 0.0100\n",
      "Epoch 20/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8671 - auc: 0.9422 - loss: 0.5926 \n",
      "Epoch 20: val_auc did not improve from 0.81349\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8702 - auc: 0.9449 - loss: 0.5833 - val_acc: 0.8641 - val_auc: 0.8096 - val_loss: 0.3752 - learning_rate: 0.0100\n",
      "Epoch 21/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8811 - auc: 0.9502 - loss: 0.5662 \n",
      "Epoch 21: val_auc improved from 0.81349 to 0.81480, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - acc: 0.8807 - auc: 0.9498 - loss: 0.5668 - val_acc: 0.8665 - val_auc: 0.8148 - val_loss: 0.3855 - learning_rate: 0.0100\n",
      "Epoch 22/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8785 - auc: 0.9515 - loss: 0.5695 \n",
      "Epoch 22: val_auc did not improve from 0.81480\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8787 - auc: 0.9514 - loss: 0.5657 - val_acc: 0.8689 - val_auc: 0.8111 - val_loss: 0.3578 - learning_rate: 0.0100\n",
      "Epoch 23/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9139 - auc: 0.9620 - loss: 0.5264 \n",
      "Epoch 23: val_auc improved from 0.81480 to 0.81524, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - acc: 0.9131 - auc: 0.9618 - loss: 0.5265 - val_acc: 0.8689 - val_auc: 0.8152 - val_loss: 0.3732 - learning_rate: 0.0100\n",
      "Epoch 24/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8862 - auc: 0.9562 - loss: 0.5598 \n",
      "Epoch 24: val_auc did not improve from 0.81524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8870 - auc: 0.9571 - loss: 0.5492 - val_acc: 0.8665 - val_auc: 0.8151 - val_loss: 0.3685 - learning_rate: 0.0100\n",
      "Epoch 25/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8918 - auc: 0.9596 - loss: 0.5273 \n",
      "Epoch 25: val_auc improved from 0.81524 to 0.81892, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.8898 - auc: 0.9591 - loss: 0.5236 - val_acc: 0.8738 - val_auc: 0.8189 - val_loss: 0.3644 - learning_rate: 0.0100\n",
      "Epoch 26/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - acc: 0.9219 - auc: 0.9655 - loss: 0.4334\n",
      "Epoch 26: val_auc improved from 0.81892 to 0.82152, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9030 - auc: 0.9603 - loss: 0.4920 - val_acc: 0.8568 - val_auc: 0.8215 - val_loss: 0.4296 - learning_rate: 0.0100\n",
      "Epoch 27/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8767 - auc: 0.9525 - loss: 0.5119 \n",
      "Epoch 27: val_auc improved from 0.82152 to 0.82381, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.8773 - auc: 0.9526 - loss: 0.5128 - val_acc: 0.8641 - val_auc: 0.8238 - val_loss: 0.4060 - learning_rate: 0.0100\n",
      "Epoch 28/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8718 - auc: 0.9574 - loss: 0.4973 \n",
      "Epoch 28: val_auc did not improve from 0.82381\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8748 - auc: 0.9580 - loss: 0.4955 - val_acc: 0.8689 - val_auc: 0.8222 - val_loss: 0.3815 - learning_rate: 0.0100\n",
      "Epoch 29/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8823 - auc: 0.9590 - loss: 0.4855 \n",
      "Epoch 29: val_auc improved from 0.82381 to 0.82493, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.8832 - auc: 0.9592 - loss: 0.4849 - val_acc: 0.8592 - val_auc: 0.8249 - val_loss: 0.3888 - learning_rate: 0.0100\n",
      "Epoch 30/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8928 - auc: 0.9492 - loss: 0.5020 \n",
      "Epoch 30: val_auc did not improve from 0.82493\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8926 - auc: 0.9500 - loss: 0.5014 - val_acc: 0.8519 - val_auc: 0.8219 - val_loss: 0.4041 - learning_rate: 0.0100\n",
      "Epoch 31/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8843 - auc: 0.9681 - loss: 0.4498 \n",
      "Epoch 31: val_auc did not improve from 0.82493\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8888 - auc: 0.9664 - loss: 0.4574 - val_acc: 0.8471 - val_auc: 0.8243 - val_loss: 0.3956 - learning_rate: 0.0100\n",
      "Epoch 32/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8965 - auc: 0.9690 - loss: 0.4264 \n",
      "Epoch 32: val_auc improved from 0.82493 to 0.82861, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.8962 - auc: 0.9671 - loss: 0.4361 - val_acc: 0.8568 - val_auc: 0.8286 - val_loss: 0.3725 - learning_rate: 0.0100\n",
      "Epoch 33/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9003 - auc: 0.9622 - loss: 0.4870 \n",
      "Epoch 33: val_auc improved from 0.82861 to 0.83221, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.8993 - auc: 0.9631 - loss: 0.4791 - val_acc: 0.8495 - val_auc: 0.8322 - val_loss: 0.3889 - learning_rate: 0.0100\n",
      "Epoch 34/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9004 - auc: 0.9679 - loss: 0.4385 \n",
      "Epoch 34: val_auc did not improve from 0.83221\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9002 - auc: 0.9676 - loss: 0.4380 - val_acc: 0.8592 - val_auc: 0.8288 - val_loss: 0.3780 - learning_rate: 0.0100\n",
      "Epoch 35/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.9128 - auc: 0.9656 - loss: 0.4421\n",
      "Epoch 35: val_auc improved from 0.83221 to 0.83835, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - acc: 0.9055 - auc: 0.9649 - loss: 0.4431 - val_acc: 0.8568 - val_auc: 0.8383 - val_loss: 0.4065 - learning_rate: 0.0100\n",
      "Epoch 36/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9015 - auc: 0.9663 - loss: 0.4750 \n",
      "Epoch 36: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9013 - auc: 0.9662 - loss: 0.4681 - val_acc: 0.8592 - val_auc: 0.8301 - val_loss: 0.4116 - learning_rate: 0.0100\n",
      "Epoch 37/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8865 - auc: 0.9621 - loss: 0.4388 \n",
      "Epoch 37: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8884 - auc: 0.9630 - loss: 0.4360 - val_acc: 0.8665 - val_auc: 0.8296 - val_loss: 0.3796 - learning_rate: 0.0100\n",
      "Epoch 38/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9086 - auc: 0.9691 - loss: 0.4240 \n",
      "Epoch 38: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9089 - auc: 0.9687 - loss: 0.4240 - val_acc: 0.8592 - val_auc: 0.8274 - val_loss: 0.3880 - learning_rate: 0.0100\n",
      "Epoch 39/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9014 - auc: 0.9667 - loss: 0.4380 \n",
      "Epoch 39: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9016 - auc: 0.9671 - loss: 0.4355 - val_acc: 0.8617 - val_auc: 0.8235 - val_loss: 0.4005 - learning_rate: 0.0100\n",
      "Epoch 40/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9080 - auc: 0.9663 - loss: 0.4192 \n",
      "Epoch 40: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9054 - auc: 0.9663 - loss: 0.4215 - val_acc: 0.8641 - val_auc: 0.8264 - val_loss: 0.4080 - learning_rate: 0.0100\n",
      "Epoch 41/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8991 - auc: 0.9748 - loss: 0.3667 \n",
      "Epoch 41: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9003 - auc: 0.9723 - loss: 0.3831 - val_acc: 0.8471 - val_auc: 0.8290 - val_loss: 0.4315 - learning_rate: 0.0100\n",
      "Epoch 42/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8887 - auc: 0.9666 - loss: 0.3959 \n",
      "Epoch 42: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8903 - auc: 0.9641 - loss: 0.4158 - val_acc: 0.8374 - val_auc: 0.8278 - val_loss: 0.4603 - learning_rate: 0.0100\n",
      "Epoch 43/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8776 - auc: 0.9531 - loss: 0.4478 \n",
      "Epoch 43: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8793 - auc: 0.9548 - loss: 0.4466 - val_acc: 0.8447 - val_auc: 0.8223 - val_loss: 0.4422 - learning_rate: 0.0100\n",
      "Epoch 44/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8842 - auc: 0.9584 - loss: 0.4435 \n",
      "Epoch 44: val_auc did not improve from 0.83835\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8829 - auc: 0.9580 - loss: 0.4453 - val_acc: 0.8228 - val_auc: 0.8218 - val_loss: 0.4961 - learning_rate: 0.0100\n",
      "Epoch 45/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8934 - auc: 0.9647 - loss: 0.4427 \n",
      "Epoch 45: val_auc improved from 0.83835 to 0.83940, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.8932 - auc: 0.9646 - loss: 0.4408 - val_acc: 0.8374 - val_auc: 0.8394 - val_loss: 0.4125 - learning_rate: 0.0100\n",
      "Epoch 46/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8994 - auc: 0.9726 - loss: 0.4004 \n",
      "Epoch 46: val_auc did not improve from 0.83940\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8982 - auc: 0.9717 - loss: 0.4036 - val_acc: 0.8180 - val_auc: 0.8357 - val_loss: 0.4623 - learning_rate: 0.0100\n",
      "Epoch 47/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8987 - auc: 0.9503 - loss: 0.4626  \n",
      "Epoch 47: val_auc did not improve from 0.83940\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8989 - auc: 0.9510 - loss: 0.4608 - val_acc: 0.8252 - val_auc: 0.8196 - val_loss: 0.4897 - learning_rate: 0.0100\n",
      "Epoch 48/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8801 - auc: 0.9668 - loss: 0.4105 \n",
      "Epoch 48: val_auc did not improve from 0.83940\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8853 - auc: 0.9662 - loss: 0.4135 - val_acc: 0.8325 - val_auc: 0.8354 - val_loss: 0.4466 - learning_rate: 0.0100\n",
      "Epoch 49/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8961 - auc: 0.9644 - loss: 0.4018 \n",
      "Epoch 49: val_auc improved from 0.83940 to 0.84010, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - acc: 0.8976 - auc: 0.9654 - loss: 0.4027 - val_acc: 0.8519 - val_auc: 0.8401 - val_loss: 0.4287 - learning_rate: 0.0100\n",
      "Epoch 50/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8991 - auc: 0.9739 - loss: 0.3674 \n",
      "Epoch 50: val_auc did not improve from 0.84010\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9002 - auc: 0.9726 - loss: 0.3755 - val_acc: 0.8277 - val_auc: 0.8386 - val_loss: 0.4233 - learning_rate: 0.0100\n",
      "Epoch 51/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8876 - auc: 0.9617 - loss: 0.4248 \n",
      "Epoch 51: val_auc did not improve from 0.84010\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8875 - auc: 0.9641 - loss: 0.4165 - val_acc: 0.8374 - val_auc: 0.8377 - val_loss: 0.4550 - learning_rate: 0.0100\n",
      "Epoch 52/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8783 - auc: 0.9652 - loss: 0.4192 \n",
      "Epoch 52: val_auc improved from 0.84010 to 0.84057, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.8793 - auc: 0.9655 - loss: 0.4172 - val_acc: 0.8641 - val_auc: 0.8406 - val_loss: 0.3938 - learning_rate: 0.0100\n",
      "Epoch 53/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8986 - auc: 0.9655 - loss: 0.4353 \n",
      "Epoch 53: val_auc did not improve from 0.84057\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9013 - auc: 0.9665 - loss: 0.4230 - val_acc: 0.8495 - val_auc: 0.8389 - val_loss: 0.4365 - learning_rate: 0.0100\n",
      "Epoch 54/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8882 - auc: 0.9695 - loss: 0.3782 \n",
      "Epoch 54: val_auc did not improve from 0.84057\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8923 - auc: 0.9689 - loss: 0.3849 - val_acc: 0.8252 - val_auc: 0.8362 - val_loss: 0.5034 - learning_rate: 0.0100\n",
      "Epoch 55/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8691 - auc: 0.9671 - loss: 0.3962 \n",
      "Epoch 55: val_auc improved from 0.84057 to 0.84738, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.8717 - auc: 0.9672 - loss: 0.3956 - val_acc: 0.8447 - val_auc: 0.8474 - val_loss: 0.4316 - learning_rate: 0.0100\n",
      "Epoch 56/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9135 - auc: 0.9679 - loss: 0.3889 \n",
      "Epoch 56: val_auc improved from 0.84738 to 0.85652, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.9118 - auc: 0.9687 - loss: 0.3866 - val_acc: 0.8617 - val_auc: 0.8565 - val_loss: 0.3845 - learning_rate: 0.0100\n",
      "Epoch 57/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9185 - auc: 0.9763 - loss: 0.3737 \n",
      "Epoch 57: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9158 - auc: 0.9752 - loss: 0.3757 - val_acc: 0.8374 - val_auc: 0.8435 - val_loss: 0.4452 - learning_rate: 0.0100\n",
      "Epoch 58/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8985 - auc: 0.9739 - loss: 0.3652 \n",
      "Epoch 58: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8984 - auc: 0.9735 - loss: 0.3664 - val_acc: 0.8471 - val_auc: 0.8354 - val_loss: 0.4111 - learning_rate: 0.0100\n",
      "Epoch 59/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9086 - auc: 0.9775 - loss: 0.3461 \n",
      "Epoch 59: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9074 - auc: 0.9769 - loss: 0.3472 - val_acc: 0.8398 - val_auc: 0.8425 - val_loss: 0.4561 - learning_rate: 0.0100\n",
      "Epoch 60/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8978 - auc: 0.9759 - loss: 0.3448 \n",
      "Epoch 60: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8981 - auc: 0.9755 - loss: 0.3479 - val_acc: 0.8350 - val_auc: 0.8505 - val_loss: 0.4702 - learning_rate: 0.0100\n",
      "Epoch 61/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9096 - auc: 0.9750 - loss: 0.3493 \n",
      "Epoch 61: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9083 - auc: 0.9747 - loss: 0.3508 - val_acc: 0.8568 - val_auc: 0.8440 - val_loss: 0.4203 - learning_rate: 0.0100\n",
      "Epoch 62/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9131 - auc: 0.9777 - loss: 0.3641\n",
      "Epoch 62: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9109 - auc: 0.9767 - loss: 0.3595 - val_acc: 0.8447 - val_auc: 0.8385 - val_loss: 0.4577 - learning_rate: 0.0100\n",
      "Epoch 63/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9045 - auc: 0.9716 - loss: 0.3600 \n",
      "Epoch 63: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9036 - auc: 0.9711 - loss: 0.3658 - val_acc: 0.8350 - val_auc: 0.8420 - val_loss: 0.4516 - learning_rate: 0.0100\n",
      "Epoch 64/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8552 - auc: 0.9695 - loss: 0.3930\n",
      "Epoch 64: val_auc did not improve from 0.85652\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8585 - auc: 0.9695 - loss: 0.3913 - val_acc: 0.8495 - val_auc: 0.8502 - val_loss: 0.4255 - learning_rate: 0.0100\n",
      "Epoch 65/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9046 - auc: 0.9745 - loss: 0.3457 \n",
      "Epoch 65: val_auc improved from 0.85652 to 0.85910, saving model to model/model_ensemble.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.9051 - auc: 0.9740 - loss: 0.3493 - val_acc: 0.8228 - val_auc: 0.8591 - val_loss: 0.5069 - learning_rate: 0.0100\n",
      "Epoch 66/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8783 - auc: 0.9673 - loss: 0.3769 \n",
      "Epoch 66: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8792 - auc: 0.9675 - loss: 0.3795 - val_acc: 0.8180 - val_auc: 0.8464 - val_loss: 0.5238 - learning_rate: 0.0100\n",
      "Epoch 67/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8866 - auc: 0.9664 - loss: 0.4084  \n",
      "Epoch 67: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8907 - auc: 0.9677 - loss: 0.3976 - val_acc: 0.8252 - val_auc: 0.8519 - val_loss: 0.5036 - learning_rate: 0.0100\n",
      "Epoch 68/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8961 - auc: 0.9632 - loss: 0.3942 \n",
      "Epoch 68: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8970 - auc: 0.9657 - loss: 0.3855 - val_acc: 0.8374 - val_auc: 0.8431 - val_loss: 0.5125 - learning_rate: 0.0100\n",
      "Epoch 69/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9065 - auc: 0.9716 - loss: 0.3439 \n",
      "Epoch 69: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9060 - auc: 0.9724 - loss: 0.3442 - val_acc: 0.8325 - val_auc: 0.8333 - val_loss: 0.5575 - learning_rate: 0.0100\n",
      "Epoch 70/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9065 - auc: 0.9721 - loss: 0.3303  \n",
      "Epoch 70: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9056 - auc: 0.9721 - loss: 0.3399 - val_acc: 0.8228 - val_auc: 0.8345 - val_loss: 0.5315 - learning_rate: 0.0100\n",
      "Epoch 71/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8945 - auc: 0.9731 - loss: 0.3498  \n",
      "Epoch 71: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8938 - auc: 0.9726 - loss: 0.3552 - val_acc: 0.8228 - val_auc: 0.8384 - val_loss: 0.5570 - learning_rate: 0.0100\n",
      "Epoch 72/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9073 - auc: 0.9661 - loss: 0.3973 \n",
      "Epoch 72: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9100 - auc: 0.9684 - loss: 0.3830 - val_acc: 0.8204 - val_auc: 0.8423 - val_loss: 0.5197 - learning_rate: 0.0100\n",
      "Epoch 73/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8547 - auc: 0.9753 - loss: 0.3548 \n",
      "Epoch 73: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8672 - auc: 0.9741 - loss: 0.3576 - val_acc: 0.8180 - val_auc: 0.8354 - val_loss: 0.5312 - learning_rate: 0.0100\n",
      "Epoch 74/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.8916 - auc: 0.9744 - loss: 0.3419 \n",
      "Epoch 74: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.8918 - auc: 0.9743 - loss: 0.3424 - val_acc: 0.8398 - val_auc: 0.8193 - val_loss: 0.5132 - learning_rate: 0.0100\n",
      "Epoch 75/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9047 - auc: 0.9703 - loss: 0.3762 \n",
      "Epoch 75: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9047 - auc: 0.9707 - loss: 0.3724 - val_acc: 0.8277 - val_auc: 0.8278 - val_loss: 0.5096 - learning_rate: 0.0100\n",
      "Epoch 76/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9074 - auc: 0.9766 - loss: 0.3500 \n",
      "Epoch 76: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9062 - auc: 0.9760 - loss: 0.3499 - val_acc: 0.8398 - val_auc: 0.8333 - val_loss: 0.5157 - learning_rate: 0.0100\n",
      "Epoch 77/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9065 - auc: 0.9723 - loss: 0.3288 \n",
      "Epoch 77: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9056 - auc: 0.9727 - loss: 0.3389 - val_acc: 0.8422 - val_auc: 0.8295 - val_loss: 0.5371 - learning_rate: 0.0100\n",
      "Epoch 78/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9028 - auc: 0.9761 - loss: 0.3320 \n",
      "Epoch 78: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9028 - auc: 0.9749 - loss: 0.3395 - val_acc: 0.8398 - val_auc: 0.8313 - val_loss: 0.5259 - learning_rate: 0.0100\n",
      "Epoch 79/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8901 - auc: 0.9745 - loss: 0.3460 \n",
      "Epoch 79: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8919 - auc: 0.9732 - loss: 0.3529 - val_acc: 0.8374 - val_auc: 0.8327 - val_loss: 0.5399 - learning_rate: 0.0100\n",
      "Epoch 80/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.8854 - auc: 0.9679 - loss: 0.3627 \n",
      "Epoch 80: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.8863 - auc: 0.9678 - loss: 0.3646 - val_acc: 0.8544 - val_auc: 0.8294 - val_loss: 0.4881 - learning_rate: 0.0100\n",
      "Epoch 81/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8814 - auc: 0.9662 - loss: 0.4081 \n",
      "Epoch 81: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8831 - auc: 0.9662 - loss: 0.4054 - val_acc: 0.8301 - val_auc: 0.8264 - val_loss: 0.5053 - learning_rate: 0.0100\n",
      "Epoch 82/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8976 - auc: 0.9775 - loss: 0.3327 \n",
      "Epoch 82: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8990 - auc: 0.9750 - loss: 0.3412 - val_acc: 0.8447 - val_auc: 0.8290 - val_loss: 0.4950 - learning_rate: 0.0100\n",
      "Epoch 83/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.8802 - auc: 0.9643 - loss: 0.3753 \n",
      "Epoch 83: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.8895 - auc: 0.9679 - loss: 0.3692 - val_acc: 0.8325 - val_auc: 0.8365 - val_loss: 0.5495 - learning_rate: 0.0100\n",
      "Epoch 84/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8985 - auc: 0.9805 - loss: 0.3287 \n",
      "Epoch 84: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9006 - auc: 0.9792 - loss: 0.3306 - val_acc: 0.8471 - val_auc: 0.8293 - val_loss: 0.5230 - learning_rate: 0.0100\n",
      "Epoch 85/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9004 - auc: 0.9738 - loss: 0.3343\n",
      "Epoch 85: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.9005 - auc: 0.9744 - loss: 0.3324 - val_acc: 0.8495 - val_auc: 0.8283 - val_loss: 0.5100 - learning_rate: 0.0100\n",
      "Epoch 86/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9193 - auc: 0.9832 - loss: 0.2730 \n",
      "Epoch 86: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9199 - auc: 0.9822 - loss: 0.2854 - val_acc: 0.8252 - val_auc: 0.8349 - val_loss: 0.6037 - learning_rate: 0.0100\n",
      "Epoch 87/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8960 - auc: 0.9767 - loss: 0.3254  \n",
      "Epoch 87: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8988 - auc: 0.9772 - loss: 0.3198 - val_acc: 0.8447 - val_auc: 0.8278 - val_loss: 0.5104 - learning_rate: 0.0100\n",
      "Epoch 88/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8939 - auc: 0.9789 - loss: 0.3093 \n",
      "Epoch 88: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8935 - auc: 0.9779 - loss: 0.3147 - val_acc: 0.8592 - val_auc: 0.8148 - val_loss: 0.5215 - learning_rate: 0.0100\n",
      "Epoch 89/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9125 - auc: 0.9713 - loss: 0.3803 \n",
      "Epoch 89: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9116 - auc: 0.9725 - loss: 0.3666 - val_acc: 0.8519 - val_auc: 0.8218 - val_loss: 0.5069 - learning_rate: 0.0100\n",
      "Epoch 90/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9206 - auc: 0.9766 - loss: 0.3114 \n",
      "Epoch 90: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9161 - auc: 0.9769 - loss: 0.3124 - val_acc: 0.8325 - val_auc: 0.8240 - val_loss: 0.5566 - learning_rate: 0.0100\n",
      "Epoch 91/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9195 - auc: 0.9810 - loss: 0.3138 \n",
      "Epoch 91: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9167 - auc: 0.9800 - loss: 0.3147 - val_acc: 0.8398 - val_auc: 0.8314 - val_loss: 0.5136 - learning_rate: 0.0100\n",
      "Epoch 92/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9171 - auc: 0.9856 - loss: 0.2913 \n",
      "Epoch 92: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9165 - auc: 0.9842 - loss: 0.2936 - val_acc: 0.8519 - val_auc: 0.8358 - val_loss: 0.4945 - learning_rate: 0.0100\n",
      "Epoch 93/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.8961 - auc: 0.9724 - loss: 0.3682  \n",
      "Epoch 93: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.8994 - auc: 0.9738 - loss: 0.3538 - val_acc: 0.8350 - val_auc: 0.8384 - val_loss: 0.5319 - learning_rate: 0.0100\n",
      "Epoch 94/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9160 - auc: 0.9780 - loss: 0.2982\n",
      "Epoch 94: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9158 - auc: 0.9779 - loss: 0.2993 - val_acc: 0.8447 - val_auc: 0.8291 - val_loss: 0.5385 - learning_rate: 0.0100\n",
      "Epoch 95/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9035 - auc: 0.9849 - loss: 0.2557 \n",
      "Epoch 95: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9055 - auc: 0.9837 - loss: 0.2678 - val_acc: 0.8495 - val_auc: 0.8371 - val_loss: 0.5101 - learning_rate: 0.0100\n",
      "Epoch 96/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9119 - auc: 0.9811 - loss: 0.2842\n",
      "Epoch 96: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - acc: 0.9120 - auc: 0.9810 - loss: 0.2847 - val_acc: 0.8544 - val_auc: 0.8287 - val_loss: 0.5071 - learning_rate: 0.0100\n",
      "Epoch 97/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9087 - auc: 0.9777 - loss: 0.3188 \n",
      "Epoch 97: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9103 - auc: 0.9785 - loss: 0.3132 - val_acc: 0.8398 - val_auc: 0.8268 - val_loss: 0.5227 - learning_rate: 0.0100\n",
      "Epoch 98/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9057 - auc: 0.9807 - loss: 0.2959  \n",
      "Epoch 98: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9077 - auc: 0.9804 - loss: 0.2963 - val_acc: 0.8374 - val_auc: 0.8216 - val_loss: 0.5727 - learning_rate: 0.0100\n",
      "Epoch 99/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9096 - auc: 0.9793 - loss: 0.3069 \n",
      "Epoch 99: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9107 - auc: 0.9795 - loss: 0.3048 - val_acc: 0.8471 - val_auc: 0.8277 - val_loss: 0.5246 - learning_rate: 0.0100\n",
      "Epoch 100/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9320 - auc: 0.9859 - loss: 0.2591 \n",
      "Epoch 100: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9242 - auc: 0.9840 - loss: 0.2722 - val_acc: 0.8519 - val_auc: 0.8239 - val_loss: 0.5078 - learning_rate: 0.0100\n",
      "Epoch 101/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9282 - auc: 0.9845 - loss: 0.2748 \n",
      "Epoch 101: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9259 - auc: 0.9833 - loss: 0.2789 - val_acc: 0.8350 - val_auc: 0.8311 - val_loss: 0.5653 - learning_rate: 0.0100\n",
      "Epoch 102/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.8950 - auc: 0.9659 - loss: 0.4020 \n",
      "Epoch 102: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9029 - auc: 0.9707 - loss: 0.3670 - val_acc: 0.8325 - val_auc: 0.8203 - val_loss: 0.5733 - learning_rate: 0.0100\n",
      "Epoch 103/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9378 - auc: 0.9873 - loss: 0.2547 \n",
      "Epoch 103: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9324 - auc: 0.9856 - loss: 0.2640 - val_acc: 0.8447 - val_auc: 0.8224 - val_loss: 0.5307 - learning_rate: 0.0100\n",
      "Epoch 104/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9321 - auc: 0.9835 - loss: 0.2927 \n",
      "Epoch 104: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9288 - auc: 0.9819 - loss: 0.2978 - val_acc: 0.8350 - val_auc: 0.8194 - val_loss: 0.5717 - learning_rate: 0.0100\n",
      "Epoch 105/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9223 - auc: 0.9844 - loss: 0.2698 \n",
      "Epoch 105: val_auc did not improve from 0.85910\n",
      "\n",
      "Epoch 105: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9181 - auc: 0.9828 - loss: 0.2779 - val_acc: 0.8422 - val_auc: 0.8295 - val_loss: 0.5541 - learning_rate: 0.0100\n",
      "Epoch 106/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9266 - auc: 0.9814 - loss: 0.2919 \n",
      "Epoch 106: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9245 - auc: 0.9809 - loss: 0.2960 - val_acc: 0.8519 - val_auc: 0.8240 - val_loss: 0.5180 - learning_rate: 0.0050\n",
      "Epoch 107/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9364 - auc: 0.9847 - loss: 0.2773 \n",
      "Epoch 107: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9349 - auc: 0.9844 - loss: 0.2777 - val_acc: 0.8471 - val_auc: 0.8263 - val_loss: 0.5784 - learning_rate: 0.0050\n",
      "Epoch 108/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9181 - auc: 0.9834 - loss: 0.3102 \n",
      "Epoch 108: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9214 - auc: 0.9835 - loss: 0.2969 - val_acc: 0.8325 - val_auc: 0.8259 - val_loss: 0.6100 - learning_rate: 0.0050\n",
      "Epoch 109/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9312 - auc: 0.9845 - loss: 0.2581 \n",
      "Epoch 109: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9296 - auc: 0.9843 - loss: 0.2598 - val_acc: 0.8350 - val_auc: 0.8262 - val_loss: 0.5813 - learning_rate: 0.0050\n",
      "Epoch 110/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9343 - auc: 0.9878 - loss: 0.2286 \n",
      "Epoch 110: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9301 - auc: 0.9862 - loss: 0.2397 - val_acc: 0.8447 - val_auc: 0.8288 - val_loss: 0.5656 - learning_rate: 0.0050\n",
      "Epoch 111/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9145 - auc: 0.9797 - loss: 0.2919 \n",
      "Epoch 111: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9166 - auc: 0.9804 - loss: 0.2871 - val_acc: 0.8325 - val_auc: 0.8280 - val_loss: 0.6167 - learning_rate: 0.0050\n",
      "Epoch 112/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9024 - auc: 0.9822 - loss: 0.2687 \n",
      "Epoch 112: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9092 - auc: 0.9828 - loss: 0.2639 - val_acc: 0.8495 - val_auc: 0.8298 - val_loss: 0.5783 - learning_rate: 0.0050\n",
      "Epoch 113/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9370 - auc: 0.9880 - loss: 0.2362 \n",
      "Epoch 113: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9324 - auc: 0.9865 - loss: 0.2431 - val_acc: 0.8544 - val_auc: 0.8245 - val_loss: 0.5684 - learning_rate: 0.0050\n",
      "Epoch 114/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9195 - auc: 0.9847 - loss: 0.2670 \n",
      "Epoch 114: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9212 - auc: 0.9845 - loss: 0.2651 - val_acc: 0.8568 - val_auc: 0.8290 - val_loss: 0.5688 - learning_rate: 0.0050\n",
      "Epoch 115/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9358 - auc: 0.9890 - loss: 0.2245 \n",
      "Epoch 115: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9314 - auc: 0.9872 - loss: 0.2374 - val_acc: 0.8447 - val_auc: 0.8214 - val_loss: 0.5970 - learning_rate: 0.0050\n",
      "Epoch 116/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9264 - auc: 0.9848 - loss: 0.2547 \n",
      "Epoch 116: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9274 - auc: 0.9851 - loss: 0.2506 - val_acc: 0.8544 - val_auc: 0.8293 - val_loss: 0.5534 - learning_rate: 0.0050\n",
      "Epoch 117/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9268 - auc: 0.9839 - loss: 0.2663 \n",
      "Epoch 117: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9273 - auc: 0.9839 - loss: 0.2655 - val_acc: 0.8398 - val_auc: 0.8287 - val_loss: 0.5975 - learning_rate: 0.0050\n",
      "Epoch 118/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9272 - auc: 0.9853 - loss: 0.2354 \n",
      "Epoch 118: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9269 - auc: 0.9852 - loss: 0.2377 - val_acc: 0.8519 - val_auc: 0.8290 - val_loss: 0.5866 - learning_rate: 0.0050\n",
      "Epoch 119/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9273 - auc: 0.9818 - loss: 0.2666 \n",
      "Epoch 119: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9280 - auc: 0.9822 - loss: 0.2646 - val_acc: 0.8519 - val_auc: 0.8307 - val_loss: 0.5773 - learning_rate: 0.0050\n",
      "Epoch 120/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9317 - auc: 0.9850 - loss: 0.2465 \n",
      "Epoch 120: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9313 - auc: 0.9849 - loss: 0.2486 - val_acc: 0.8495 - val_auc: 0.8304 - val_loss: 0.5818 - learning_rate: 0.0050\n",
      "Epoch 121/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9240 - auc: 0.9774 - loss: 0.3018 \n",
      "Epoch 121: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9281 - auc: 0.9803 - loss: 0.2801 - val_acc: 0.8519 - val_auc: 0.8285 - val_loss: 0.5919 - learning_rate: 0.0050\n",
      "Epoch 122/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9308 - auc: 0.9818 - loss: 0.2572 \n",
      "Epoch 122: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9296 - auc: 0.9822 - loss: 0.2564 - val_acc: 0.8447 - val_auc: 0.8251 - val_loss: 0.5948 - learning_rate: 0.0050\n",
      "Epoch 123/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9224 - auc: 0.9866 - loss: 0.2450 \n",
      "Epoch 123: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9226 - auc: 0.9860 - loss: 0.2469 - val_acc: 0.8544 - val_auc: 0.8254 - val_loss: 0.5742 - learning_rate: 0.0050\n",
      "Epoch 124/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9342 - auc: 0.9842 - loss: 0.2680 \n",
      "Epoch 124: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9340 - auc: 0.9842 - loss: 0.2632 - val_acc: 0.8471 - val_auc: 0.8229 - val_loss: 0.5790 - learning_rate: 0.0050\n",
      "Epoch 125/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9243 - auc: 0.9783 - loss: 0.2710 \n",
      "Epoch 125: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9245 - auc: 0.9805 - loss: 0.2642 - val_acc: 0.8471 - val_auc: 0.8245 - val_loss: 0.5804 - learning_rate: 0.0050\n",
      "Epoch 126/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9476 - auc: 0.9907 - loss: 0.2053 \n",
      "Epoch 126: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9439 - auc: 0.9892 - loss: 0.2141 - val_acc: 0.8471 - val_auc: 0.8228 - val_loss: 0.5768 - learning_rate: 0.0050\n",
      "Epoch 127/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9347 - auc: 0.9892 - loss: 0.2145 \n",
      "Epoch 127: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9338 - auc: 0.9883 - loss: 0.2219 - val_acc: 0.8544 - val_auc: 0.8260 - val_loss: 0.5872 - learning_rate: 0.0050\n",
      "Epoch 128/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9345 - auc: 0.9832 - loss: 0.2686 \n",
      "Epoch 128: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9345 - auc: 0.9836 - loss: 0.2649 - val_acc: 0.8495 - val_auc: 0.8227 - val_loss: 0.5939 - learning_rate: 0.0050\n",
      "Epoch 129/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9273 - auc: 0.9847 - loss: 0.2493 \n",
      "Epoch 129: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9277 - auc: 0.9847 - loss: 0.2493 - val_acc: 0.8398 - val_auc: 0.8202 - val_loss: 0.5998 - learning_rate: 0.0050\n",
      "Epoch 130/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9260 - auc: 0.9807 - loss: 0.2695 \n",
      "Epoch 130: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9277 - auc: 0.9824 - loss: 0.2580 - val_acc: 0.8495 - val_auc: 0.8249 - val_loss: 0.5974 - learning_rate: 0.0050\n",
      "Epoch 131/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9350 - auc: 0.9884 - loss: 0.2097\n",
      "Epoch 131: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9346 - auc: 0.9877 - loss: 0.2166 - val_acc: 0.8568 - val_auc: 0.8218 - val_loss: 0.6044 - learning_rate: 0.0050\n",
      "Epoch 132/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9295 - auc: 0.9865 - loss: 0.2205 \n",
      "Epoch 132: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9313 - auc: 0.9859 - loss: 0.2281 - val_acc: 0.8471 - val_auc: 0.8260 - val_loss: 0.6092 - learning_rate: 0.0050\n",
      "Epoch 133/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9067 - auc: 0.9780 - loss: 0.2923 \n",
      "Epoch 133: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9130 - auc: 0.9796 - loss: 0.2801 - val_acc: 0.8495 - val_auc: 0.8233 - val_loss: 0.5789 - learning_rate: 0.0050\n",
      "Epoch 134/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9426 - auc: 0.9904 - loss: 0.2072 \n",
      "Epoch 134: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9403 - auc: 0.9889 - loss: 0.2179 - val_acc: 0.8495 - val_auc: 0.8289 - val_loss: 0.6034 - learning_rate: 0.0050\n",
      "Epoch 135/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9241 - auc: 0.9822 - loss: 0.2408  \n",
      "Epoch 135: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9272 - auc: 0.9839 - loss: 0.2370 - val_acc: 0.8422 - val_auc: 0.8306 - val_loss: 0.6366 - learning_rate: 0.0050\n",
      "Epoch 136/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9342 - auc: 0.9870 - loss: 0.2432 \n",
      "Epoch 136: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9338 - auc: 0.9862 - loss: 0.2455 - val_acc: 0.8495 - val_auc: 0.8320 - val_loss: 0.6041 - learning_rate: 0.0050\n",
      "Epoch 137/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9301 - auc: 0.9838 - loss: 0.2531 \n",
      "Epoch 137: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9302 - auc: 0.9839 - loss: 0.2519 - val_acc: 0.8495 - val_auc: 0.8251 - val_loss: 0.5925 - learning_rate: 0.0050\n",
      "Epoch 138/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9396 - auc: 0.9886 - loss: 0.2262 \n",
      "Epoch 138: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9381 - auc: 0.9880 - loss: 0.2289 - val_acc: 0.8447 - val_auc: 0.8252 - val_loss: 0.6154 - learning_rate: 0.0050\n",
      "Epoch 139/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9458 - auc: 0.9892 - loss: 0.2117 \n",
      "Epoch 139: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9435 - auc: 0.9887 - loss: 0.2155 - val_acc: 0.8422 - val_auc: 0.8259 - val_loss: 0.6270 - learning_rate: 0.0050\n",
      "Epoch 140/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9327 - auc: 0.9843 - loss: 0.2525 \n",
      "Epoch 140: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9326 - auc: 0.9845 - loss: 0.2495 - val_acc: 0.8471 - val_auc: 0.8241 - val_loss: 0.6364 - learning_rate: 0.0050\n",
      "Epoch 141/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9366 - auc: 0.9894 - loss: 0.1972 \n",
      "Epoch 141: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9342 - auc: 0.9886 - loss: 0.2061 - val_acc: 0.8471 - val_auc: 0.8243 - val_loss: 0.6098 - learning_rate: 0.0050\n",
      "Epoch 142/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9383 - auc: 0.9823 - loss: 0.2761 \n",
      "Epoch 142: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9378 - auc: 0.9838 - loss: 0.2631 - val_acc: 0.8471 - val_auc: 0.8271 - val_loss: 0.6350 - learning_rate: 0.0050\n",
      "Epoch 143/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9377 - auc: 0.9815 - loss: 0.2634 \n",
      "Epoch 143: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9359 - auc: 0.9824 - loss: 0.2572 - val_acc: 0.8519 - val_auc: 0.8219 - val_loss: 0.6149 - learning_rate: 0.0050\n",
      "Epoch 144/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9523 - auc: 0.9888 - loss: 0.2161  \n",
      "Epoch 144: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9439 - auc: 0.9870 - loss: 0.2277 - val_acc: 0.8495 - val_auc: 0.8218 - val_loss: 0.6265 - learning_rate: 0.0050\n",
      "Epoch 145/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9395 - auc: 0.9896 - loss: 0.2141 \n",
      "Epoch 145: val_auc did not improve from 0.85910\n",
      "\n",
      "Epoch 145: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9373 - auc: 0.9878 - loss: 0.2223 - val_acc: 0.8568 - val_auc: 0.8183 - val_loss: 0.6015 - learning_rate: 0.0050\n",
      "Epoch 146/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9371 - auc: 0.9872 - loss: 0.2378 \n",
      "Epoch 146: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9369 - auc: 0.9868 - loss: 0.2394 - val_acc: 0.8519 - val_auc: 0.8234 - val_loss: 0.6398 - learning_rate: 0.0025\n",
      "Epoch 147/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9283 - auc: 0.9895 - loss: 0.2011 \n",
      "Epoch 147: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9290 - auc: 0.9884 - loss: 0.2093 - val_acc: 0.8519 - val_auc: 0.8187 - val_loss: 0.6357 - learning_rate: 0.0025\n",
      "Epoch 148/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9411 - auc: 0.9906 - loss: 0.1953 \n",
      "Epoch 148: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9401 - auc: 0.9899 - loss: 0.2008 - val_acc: 0.8519 - val_auc: 0.8136 - val_loss: 0.6370 - learning_rate: 0.0025\n",
      "Epoch 149/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9326 - auc: 0.9824 - loss: 0.2449 \n",
      "Epoch 149: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9335 - auc: 0.9834 - loss: 0.2433 - val_acc: 0.8495 - val_auc: 0.8244 - val_loss: 0.6477 - learning_rate: 0.0025\n",
      "Epoch 150/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9159 - auc: 0.9822 - loss: 0.2355 \n",
      "Epoch 150: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9218 - auc: 0.9836 - loss: 0.2313 - val_acc: 0.8519 - val_auc: 0.8200 - val_loss: 0.6391 - learning_rate: 0.0025\n",
      "Epoch 151/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9313 - auc: 0.9874 - loss: 0.2163 \n",
      "Epoch 151: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9317 - auc: 0.9870 - loss: 0.2192 - val_acc: 0.8544 - val_auc: 0.8204 - val_loss: 0.6325 - learning_rate: 0.0025\n",
      "Epoch 152/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9404 - auc: 0.9929 - loss: 0.1734\n",
      "Epoch 152: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - acc: 0.9402 - auc: 0.9926 - loss: 0.1765 - val_acc: 0.8519 - val_auc: 0.8180 - val_loss: 0.6485 - learning_rate: 0.0025\n",
      "Epoch 153/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9452 - auc: 0.9870 - loss: 0.2111 \n",
      "Epoch 153: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9445 - auc: 0.9869 - loss: 0.2126 - val_acc: 0.8544 - val_auc: 0.8205 - val_loss: 0.6427 - learning_rate: 0.0025\n",
      "Epoch 154/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9441 - auc: 0.9875 - loss: 0.2240 \n",
      "Epoch 154: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9427 - auc: 0.9871 - loss: 0.2260 - val_acc: 0.8495 - val_auc: 0.8179 - val_loss: 0.6586 - learning_rate: 0.0025\n",
      "Epoch 155/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9337 - auc: 0.9814 - loss: 0.2470 \n",
      "Epoch 155: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9344 - auc: 0.9827 - loss: 0.2415 - val_acc: 0.8544 - val_auc: 0.8198 - val_loss: 0.6466 - learning_rate: 0.0025\n",
      "Epoch 156/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9214 - auc: 0.9876 - loss: 0.2400 \n",
      "Epoch 156: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9270 - auc: 0.9871 - loss: 0.2360 - val_acc: 0.8519 - val_auc: 0.8175 - val_loss: 0.6557 - learning_rate: 0.0025\n",
      "Epoch 157/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9281 - auc: 0.9869 - loss: 0.2135 \n",
      "Epoch 157: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.9307 - auc: 0.9868 - loss: 0.2155 - val_acc: 0.8519 - val_auc: 0.8180 - val_loss: 0.6578 - learning_rate: 0.0025\n",
      "Epoch 158/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9325 - auc: 0.9880 - loss: 0.2006 \n",
      "Epoch 158: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9352 - auc: 0.9872 - loss: 0.2121 - val_acc: 0.8495 - val_auc: 0.8194 - val_loss: 0.6594 - learning_rate: 0.0025\n",
      "Epoch 159/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9280 - auc: 0.9871 - loss: 0.2230 \n",
      "Epoch 159: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9290 - auc: 0.9869 - loss: 0.2238 - val_acc: 0.8471 - val_auc: 0.8205 - val_loss: 0.6446 - learning_rate: 0.0025\n",
      "Epoch 160/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - acc: 0.9219 - auc: 0.9918 - loss: 0.2175\n",
      "Epoch 160: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.9336 - auc: 0.9856 - loss: 0.2364 - val_acc: 0.8544 - val_auc: 0.8221 - val_loss: 0.6487 - learning_rate: 0.0025\n",
      "Epoch 161/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9351 - auc: 0.9846 - loss: 0.2280 \n",
      "Epoch 161: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9351 - auc: 0.9847 - loss: 0.2280 - val_acc: 0.8471 - val_auc: 0.8198 - val_loss: 0.6688 - learning_rate: 0.0025\n",
      "Epoch 162/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9291 - auc: 0.9822 - loss: 0.2575 \n",
      "Epoch 162: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9305 - auc: 0.9834 - loss: 0.2483 - val_acc: 0.8544 - val_auc: 0.8143 - val_loss: 0.6564 - learning_rate: 0.0025\n",
      "Epoch 163/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9573 - auc: 0.9910 - loss: 0.1967 \n",
      "Epoch 163: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9511 - auc: 0.9896 - loss: 0.2082 - val_acc: 0.8568 - val_auc: 0.8170 - val_loss: 0.6429 - learning_rate: 0.0025\n",
      "Epoch 164/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9419 - auc: 0.9852 - loss: 0.2185 \n",
      "Epoch 164: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9420 - auc: 0.9861 - loss: 0.2165 - val_acc: 0.8495 - val_auc: 0.8158 - val_loss: 0.6704 - learning_rate: 0.0025\n",
      "Epoch 165/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9183 - auc: 0.9821 - loss: 0.2686 \n",
      "Epoch 165: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9221 - auc: 0.9830 - loss: 0.2590 - val_acc: 0.8495 - val_auc: 0.8151 - val_loss: 0.6495 - learning_rate: 0.0025\n",
      "Epoch 166/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9282 - auc: 0.9837 - loss: 0.2568 \n",
      "Epoch 166: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9319 - auc: 0.9849 - loss: 0.2409 - val_acc: 0.8544 - val_auc: 0.8149 - val_loss: 0.6499 - learning_rate: 0.0025\n",
      "Epoch 167/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9371 - auc: 0.9856 - loss: 0.2236 \n",
      "Epoch 167: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9370 - auc: 0.9856 - loss: 0.2252 - val_acc: 0.8544 - val_auc: 0.8170 - val_loss: 0.6500 - learning_rate: 0.0025\n",
      "Epoch 168/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9333 - auc: 0.9861 - loss: 0.2287\n",
      "Epoch 168: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.9337 - auc: 0.9860 - loss: 0.2285 - val_acc: 0.8519 - val_auc: 0.8197 - val_loss: 0.6246 - learning_rate: 0.0025\n",
      "Epoch 169/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9365 - auc: 0.9808 - loss: 0.2556 \n",
      "Epoch 169: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9364 - auc: 0.9814 - loss: 0.2523 - val_acc: 0.8519 - val_auc: 0.8137 - val_loss: 0.6386 - learning_rate: 0.0025\n",
      "Epoch 170/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9239 - auc: 0.9829 - loss: 0.2666 \n",
      "Epoch 170: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9268 - auc: 0.9837 - loss: 0.2546 - val_acc: 0.8519 - val_auc: 0.8141 - val_loss: 0.6463 - learning_rate: 0.0025\n",
      "Epoch 171/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9253 - auc: 0.9807 - loss: 0.2653 \n",
      "Epoch 171: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9287 - auc: 0.9823 - loss: 0.2556 - val_acc: 0.8495 - val_auc: 0.8155 - val_loss: 0.6498 - learning_rate: 0.0025\n",
      "Epoch 172/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9321 - auc: 0.9781 - loss: 0.2671 \n",
      "Epoch 172: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9355 - auc: 0.9819 - loss: 0.2487 - val_acc: 0.8447 - val_auc: 0.8150 - val_loss: 0.6672 - learning_rate: 0.0025\n",
      "Epoch 173/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9362 - auc: 0.9879 - loss: 0.1994 \n",
      "Epoch 173: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9357 - auc: 0.9873 - loss: 0.2041 - val_acc: 0.8544 - val_auc: 0.8166 - val_loss: 0.6597 - learning_rate: 0.0025\n",
      "Epoch 174/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9390 - auc: 0.9846 - loss: 0.2339 \n",
      "Epoch 174: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9373 - auc: 0.9850 - loss: 0.2310 - val_acc: 0.8544 - val_auc: 0.8169 - val_loss: 0.6522 - learning_rate: 0.0025\n",
      "Epoch 175/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9369 - auc: 0.9840 - loss: 0.2313 \n",
      "Epoch 175: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9368 - auc: 0.9844 - loss: 0.2297 - val_acc: 0.8519 - val_auc: 0.8164 - val_loss: 0.6430 - learning_rate: 0.0025\n",
      "Epoch 176/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9503 - auc: 0.9908 - loss: 0.1862  \n",
      "Epoch 176: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9437 - auc: 0.9884 - loss: 0.2054 - val_acc: 0.8519 - val_auc: 0.8146 - val_loss: 0.6388 - learning_rate: 0.0025\n",
      "Epoch 177/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9429 - auc: 0.9872 - loss: 0.2356 \n",
      "Epoch 177: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9419 - auc: 0.9868 - loss: 0.2344 - val_acc: 0.8544 - val_auc: 0.8155 - val_loss: 0.6528 - learning_rate: 0.0025\n",
      "Epoch 178/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9608 - auc: 0.9912 - loss: 0.1768 \n",
      "Epoch 178: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9541 - auc: 0.9899 - loss: 0.1908 - val_acc: 0.8544 - val_auc: 0.8147 - val_loss: 0.6633 - learning_rate: 0.0025\n",
      "Epoch 179/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9478 - auc: 0.9920 - loss: 0.1677 \n",
      "Epoch 179: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9444 - auc: 0.9896 - loss: 0.1919 - val_acc: 0.8544 - val_auc: 0.8153 - val_loss: 0.6510 - learning_rate: 0.0025\n",
      "Epoch 180/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9471 - auc: 0.9904 - loss: 0.2080 \n",
      "Epoch 180: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9447 - auc: 0.9897 - loss: 0.2104 - val_acc: 0.8568 - val_auc: 0.8162 - val_loss: 0.6474 - learning_rate: 0.0025\n",
      "Epoch 181/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9329 - auc: 0.9808 - loss: 0.2511 \n",
      "Epoch 181: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9369 - auc: 0.9832 - loss: 0.2392 - val_acc: 0.8568 - val_auc: 0.8145 - val_loss: 0.6632 - learning_rate: 0.0025\n",
      "Epoch 182/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9538 - auc: 0.9893 - loss: 0.1865 \n",
      "Epoch 182: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9491 - auc: 0.9885 - loss: 0.1975 - val_acc: 0.8519 - val_auc: 0.8136 - val_loss: 0.6633 - learning_rate: 0.0025\n",
      "Epoch 183/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9396 - auc: 0.9834 - loss: 0.2478\n",
      "Epoch 183: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - acc: 0.9395 - auc: 0.9838 - loss: 0.2453 - val_acc: 0.8568 - val_auc: 0.8066 - val_loss: 0.6536 - learning_rate: 0.0025\n",
      "Epoch 184/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9262 - auc: 0.9870 - loss: 0.2130 \n",
      "Epoch 184: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9290 - auc: 0.9869 - loss: 0.2130 - val_acc: 0.8544 - val_auc: 0.8131 - val_loss: 0.6623 - learning_rate: 0.0025\n",
      "Epoch 185/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9275 - auc: 0.9856 - loss: 0.2309 \n",
      "Epoch 185: val_auc did not improve from 0.85910\n",
      "\n",
      "Epoch 185: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9312 - auc: 0.9862 - loss: 0.2278 - val_acc: 0.8568 - val_auc: 0.8131 - val_loss: 0.6676 - learning_rate: 0.0025\n",
      "Epoch 186/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - acc: 0.9688 - auc: 0.9985 - loss: 0.1302\n",
      "Epoch 186: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9427 - auc: 0.9884 - loss: 0.2077 - val_acc: 0.8568 - val_auc: 0.8138 - val_loss: 0.6680 - learning_rate: 0.0012\n",
      "Epoch 187/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9315 - auc: 0.9833 - loss: 0.2495 \n",
      "Epoch 187: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9319 - auc: 0.9835 - loss: 0.2480 - val_acc: 0.8568 - val_auc: 0.8104 - val_loss: 0.6637 - learning_rate: 0.0012\n",
      "Epoch 188/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9513 - auc: 0.9912 - loss: 0.1896 \n",
      "Epoch 188: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9496 - auc: 0.9906 - loss: 0.1938 - val_acc: 0.8544 - val_auc: 0.8152 - val_loss: 0.6632 - learning_rate: 0.0012\n",
      "Epoch 189/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9323 - auc: 0.9882 - loss: 0.2506 \n",
      "Epoch 189: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9337 - auc: 0.9880 - loss: 0.2415 - val_acc: 0.8544 - val_auc: 0.8150 - val_loss: 0.6577 - learning_rate: 0.0012\n",
      "Epoch 190/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9624 - auc: 0.9927 - loss: 0.1701 \n",
      "Epoch 190: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - acc: 0.9571 - auc: 0.9913 - loss: 0.1836 - val_acc: 0.8544 - val_auc: 0.8187 - val_loss: 0.6658 - learning_rate: 0.0012\n",
      "Epoch 191/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9514 - auc: 0.9877 - loss: 0.2151\n",
      "Epoch 191: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9507 - auc: 0.9876 - loss: 0.2156 - val_acc: 0.8544 - val_auc: 0.8193 - val_loss: 0.6666 - learning_rate: 0.0012\n",
      "Epoch 192/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9274 - auc: 0.9897 - loss: 0.1986 \n",
      "Epoch 192: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9322 - auc: 0.9888 - loss: 0.2044 - val_acc: 0.8519 - val_auc: 0.8096 - val_loss: 0.6648 - learning_rate: 0.0012\n",
      "Epoch 193/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 175ms/step - acc: 0.9531 - auc: 0.9776 - loss: 0.3364\n",
      "Epoch 193: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9390 - auc: 0.9838 - loss: 0.2405 - val_acc: 0.8544 - val_auc: 0.8152 - val_loss: 0.6685 - learning_rate: 0.0012\n",
      "Epoch 194/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9424 - auc: 0.9863 - loss: 0.2275 \n",
      "Epoch 194: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9417 - auc: 0.9865 - loss: 0.2240 - val_acc: 0.8544 - val_auc: 0.8091 - val_loss: 0.6692 - learning_rate: 0.0012\n",
      "Epoch 195/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9547 - auc: 0.9912 - loss: 0.1903 \n",
      "Epoch 195: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9492 - auc: 0.9897 - loss: 0.2006 - val_acc: 0.8519 - val_auc: 0.8109 - val_loss: 0.6613 - learning_rate: 0.0012\n",
      "Epoch 196/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9452 - auc: 0.9908 - loss: 0.1913 \n",
      "Epoch 196: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9418 - auc: 0.9892 - loss: 0.2016 - val_acc: 0.8544 - val_auc: 0.8095 - val_loss: 0.6633 - learning_rate: 0.0012\n",
      "Epoch 197/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9495 - auc: 0.9904 - loss: 0.2018\n",
      "Epoch 197: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9475 - auc: 0.9896 - loss: 0.2080 - val_acc: 0.8544 - val_auc: 0.8092 - val_loss: 0.6613 - learning_rate: 0.0012\n",
      "Epoch 198/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9543 - auc: 0.9901 - loss: 0.2022  \n",
      "Epoch 198: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9466 - auc: 0.9879 - loss: 0.2126 - val_acc: 0.8544 - val_auc: 0.8092 - val_loss: 0.6578 - learning_rate: 0.0012\n",
      "Epoch 199/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9434 - auc: 0.9879 - loss: 0.2192 \n",
      "Epoch 199: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9431 - auc: 0.9878 - loss: 0.2195 - val_acc: 0.8519 - val_auc: 0.8089 - val_loss: 0.6638 - learning_rate: 0.0012\n",
      "Epoch 200/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9258 - auc: 0.9810 - loss: 0.2587 \n",
      "Epoch 200: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9286 - auc: 0.9826 - loss: 0.2480 - val_acc: 0.8544 - val_auc: 0.8116 - val_loss: 0.6601 - learning_rate: 0.0012\n",
      "Epoch 201/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9575 - auc: 0.9903 - loss: 0.1848 \n",
      "Epoch 201: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9528 - auc: 0.9893 - loss: 0.1957 - val_acc: 0.8568 - val_auc: 0.8088 - val_loss: 0.6705 - learning_rate: 0.0012\n",
      "Epoch 202/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9333 - auc: 0.9920 - loss: 0.1756 \n",
      "Epoch 202: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9346 - auc: 0.9905 - loss: 0.1871 - val_acc: 0.8544 - val_auc: 0.8109 - val_loss: 0.6651 - learning_rate: 0.0012\n",
      "Epoch 203/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9250 - auc: 0.9900 - loss: 0.1895 \n",
      "Epoch 203: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9301 - auc: 0.9892 - loss: 0.1982 - val_acc: 0.8519 - val_auc: 0.8113 - val_loss: 0.6593 - learning_rate: 0.0012\n",
      "Epoch 204/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - acc: 0.9531 - auc: 0.9797 - loss: 0.1944\n",
      "Epoch 204: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.9520 - auc: 0.9875 - loss: 0.2078 - val_acc: 0.8519 - val_auc: 0.8045 - val_loss: 0.6452 - learning_rate: 0.0012\n",
      "Epoch 205/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9446 - auc: 0.9883 - loss: 0.2189 \n",
      "Epoch 205: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9443 - auc: 0.9880 - loss: 0.2181 - val_acc: 0.8544 - val_auc: 0.8102 - val_loss: 0.6522 - learning_rate: 0.0012\n",
      "Epoch 206/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9597 - auc: 0.9929 - loss: 0.1625 \n",
      "Epoch 206: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9538 - auc: 0.9914 - loss: 0.1790 - val_acc: 0.8568 - val_auc: 0.8111 - val_loss: 0.6592 - learning_rate: 0.0012\n",
      "Epoch 207/500\n",
      "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9428 - auc: 0.9848 - loss: 0.2323\n",
      "Epoch 207: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9425 - auc: 0.9850 - loss: 0.2311 - val_acc: 0.8519 - val_auc: 0.8142 - val_loss: 0.6704 - learning_rate: 0.0012\n",
      "Epoch 208/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9454 - auc: 0.9874 - loss: 0.2112 \n",
      "Epoch 208: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9444 - auc: 0.9873 - loss: 0.2122 - val_acc: 0.8544 - val_auc: 0.8107 - val_loss: 0.6702 - learning_rate: 0.0012\n",
      "Epoch 209/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9278 - auc: 0.9845 - loss: 0.2670 \n",
      "Epoch 209: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9320 - auc: 0.9855 - loss: 0.2517 - val_acc: 0.8568 - val_auc: 0.8120 - val_loss: 0.6696 - learning_rate: 0.0012\n",
      "Epoch 210/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9365 - auc: 0.9894 - loss: 0.2008\n",
      "Epoch 210: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - acc: 0.9371 - auc: 0.9892 - loss: 0.2037 - val_acc: 0.8568 - val_auc: 0.8109 - val_loss: 0.6773 - learning_rate: 0.0012\n",
      "Epoch 211/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9320 - auc: 0.9837 - loss: 0.2442 \n",
      "Epoch 211: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9337 - auc: 0.9843 - loss: 0.2380 - val_acc: 0.8568 - val_auc: 0.8124 - val_loss: 0.6722 - learning_rate: 0.0012\n",
      "Epoch 212/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9367 - auc: 0.9875 - loss: 0.2313 \n",
      "Epoch 212: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9378 - auc: 0.9874 - loss: 0.2293 - val_acc: 0.8544 - val_auc: 0.8111 - val_loss: 0.6821 - learning_rate: 0.0012\n",
      "Epoch 213/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9361 - auc: 0.9834 - loss: 0.2601 \n",
      "Epoch 213: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9372 - auc: 0.9841 - loss: 0.2504 - val_acc: 0.8544 - val_auc: 0.8139 - val_loss: 0.6741 - learning_rate: 0.0012\n",
      "Epoch 214/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9470 - auc: 0.9886 - loss: 0.1903 \n",
      "Epoch 214: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9460 - auc: 0.9888 - loss: 0.1944 - val_acc: 0.8544 - val_auc: 0.8141 - val_loss: 0.6739 - learning_rate: 0.0012\n",
      "Epoch 215/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9423 - auc: 0.9886 - loss: 0.2279 \n",
      "Epoch 215: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9416 - auc: 0.9886 - loss: 0.2240 - val_acc: 0.8544 - val_auc: 0.8105 - val_loss: 0.6831 - learning_rate: 0.0012\n",
      "Epoch 216/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - acc: 0.9515 - auc: 0.9906 - loss: 0.1976 \n",
      "Epoch 216: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9510 - auc: 0.9904 - loss: 0.1986 - val_acc: 0.8544 - val_auc: 0.8121 - val_loss: 0.6793 - learning_rate: 0.0012\n",
      "Epoch 217/500\n",
      "\u001b[1m 1/17\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 183ms/step - acc: 1.0000 - auc: 1.0000 - loss: 0.1263\n",
      "Epoch 217: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9491 - auc: 0.9894 - loss: 0.2046 - val_acc: 0.8519 - val_auc: 0.8131 - val_loss: 0.6814 - learning_rate: 0.0012\n",
      "Epoch 218/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9530 - auc: 0.9931 - loss: 0.1719 \n",
      "Epoch 218: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9487 - auc: 0.9909 - loss: 0.1876 - val_acc: 0.8544 - val_auc: 0.8121 - val_loss: 0.6806 - learning_rate: 0.0012\n",
      "Epoch 219/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9265 - auc: 0.9853 - loss: 0.2325 \n",
      "Epoch 219: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9319 - auc: 0.9860 - loss: 0.2322 - val_acc: 0.8568 - val_auc: 0.8118 - val_loss: 0.6843 - learning_rate: 0.0012\n",
      "Epoch 220/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9429 - auc: 0.9884 - loss: 0.2026 \n",
      "Epoch 220: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9431 - auc: 0.9883 - loss: 0.2047 - val_acc: 0.8544 - val_auc: 0.8128 - val_loss: 0.6814 - learning_rate: 0.0012\n",
      "Epoch 221/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9346 - auc: 0.9871 - loss: 0.2042 \n",
      "Epoch 221: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9374 - auc: 0.9874 - loss: 0.2041 - val_acc: 0.8544 - val_auc: 0.8127 - val_loss: 0.6805 - learning_rate: 0.0012\n",
      "Epoch 222/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9210 - auc: 0.9856 - loss: 0.2284 \n",
      "Epoch 222: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9266 - auc: 0.9860 - loss: 0.2244 - val_acc: 0.8544 - val_auc: 0.8122 - val_loss: 0.6789 - learning_rate: 0.0012\n",
      "Epoch 223/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9397 - auc: 0.9844 - loss: 0.2237  \n",
      "Epoch 223: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9408 - auc: 0.9853 - loss: 0.2209 - val_acc: 0.8544 - val_auc: 0.8119 - val_loss: 0.6820 - learning_rate: 0.0012\n",
      "Epoch 224/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9509 - auc: 0.9894 - loss: 0.1930 \n",
      "Epoch 224: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.9492 - auc: 0.9888 - loss: 0.1983 - val_acc: 0.8568 - val_auc: 0.8136 - val_loss: 0.6794 - learning_rate: 0.0012\n",
      "Epoch 225/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9472 - auc: 0.9937 - loss: 0.1638 \n",
      "Epoch 225: val_auc did not improve from 0.85910\n",
      "\n",
      "Epoch 225: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9443 - auc: 0.9913 - loss: 0.1837 - val_acc: 0.8568 - val_auc: 0.8137 - val_loss: 0.6785 - learning_rate: 0.0012\n",
      "Epoch 226/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9409 - auc: 0.9870 - loss: 0.2078 \n",
      "Epoch 226: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9411 - auc: 0.9870 - loss: 0.2094 - val_acc: 0.8568 - val_auc: 0.8127 - val_loss: 0.6834 - learning_rate: 6.2500e-04\n",
      "Epoch 227/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9410 - auc: 0.9885 - loss: 0.2137 \n",
      "Epoch 227: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9403 - auc: 0.9880 - loss: 0.2149 - val_acc: 0.8544 - val_auc: 0.8119 - val_loss: 0.6862 - learning_rate: 6.2500e-04\n",
      "Epoch 228/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9397 - auc: 0.9843 - loss: 0.2103 \n",
      "Epoch 228: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9387 - auc: 0.9846 - loss: 0.2162 - val_acc: 0.8544 - val_auc: 0.8117 - val_loss: 0.6888 - learning_rate: 6.2500e-04\n",
      "Epoch 229/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9439 - auc: 0.9921 - loss: 0.1830 \n",
      "Epoch 229: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9436 - auc: 0.9911 - loss: 0.1903 - val_acc: 0.8544 - val_auc: 0.8116 - val_loss: 0.6920 - learning_rate: 6.2500e-04\n",
      "Epoch 230/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9290 - auc: 0.9875 - loss: 0.2257  \n",
      "Epoch 230: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9343 - auc: 0.9877 - loss: 0.2202 - val_acc: 0.8544 - val_auc: 0.8130 - val_loss: 0.6871 - learning_rate: 6.2500e-04\n",
      "Epoch 231/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9553 - auc: 0.9919 - loss: 0.1678 \n",
      "Epoch 231: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9511 - auc: 0.9904 - loss: 0.1814 - val_acc: 0.8544 - val_auc: 0.8133 - val_loss: 0.6869 - learning_rate: 6.2500e-04\n",
      "Epoch 232/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9580 - auc: 0.9922 - loss: 0.1732 \n",
      "Epoch 232: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9541 - auc: 0.9911 - loss: 0.1820 - val_acc: 0.8544 - val_auc: 0.8144 - val_loss: 0.6826 - learning_rate: 6.2500e-04\n",
      "Epoch 233/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9420 - auc: 0.9883 - loss: 0.2164 \n",
      "Epoch 233: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9414 - auc: 0.9879 - loss: 0.2166 - val_acc: 0.8544 - val_auc: 0.8139 - val_loss: 0.6858 - learning_rate: 6.2500e-04\n",
      "Epoch 234/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9467 - auc: 0.9918 - loss: 0.1754 \n",
      "Epoch 234: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9460 - auc: 0.9909 - loss: 0.1829 - val_acc: 0.8519 - val_auc: 0.8119 - val_loss: 0.6880 - learning_rate: 6.2500e-04\n",
      "Epoch 235/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9409 - auc: 0.9880 - loss: 0.2011 \n",
      "Epoch 235: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9407 - auc: 0.9878 - loss: 0.2038 - val_acc: 0.8519 - val_auc: 0.8112 - val_loss: 0.6895 - learning_rate: 6.2500e-04\n",
      "Epoch 236/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9450 - auc: 0.9900 - loss: 0.1898\n",
      "Epoch 236: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - acc: 0.9429 - auc: 0.9895 - loss: 0.1956 - val_acc: 0.8519 - val_auc: 0.8113 - val_loss: 0.6895 - learning_rate: 6.2500e-04\n",
      "Epoch 237/500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9331 - auc: 0.9852 - loss: 0.2436 \n",
      "Epoch 237: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9335 - auc: 0.9853 - loss: 0.2421 - val_acc: 0.8544 - val_auc: 0.8118 - val_loss: 0.6875 - learning_rate: 6.2500e-04\n",
      "Epoch 238/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9248 - auc: 0.9786 - loss: 0.2710 \n",
      "Epoch 238: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9315 - auc: 0.9829 - loss: 0.2426 - val_acc: 0.8544 - val_auc: 0.8116 - val_loss: 0.6870 - learning_rate: 6.2500e-04\n",
      "Epoch 239/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9495 - auc: 0.9930 - loss: 0.1584 \n",
      "Epoch 239: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9481 - auc: 0.9912 - loss: 0.1761 - val_acc: 0.8519 - val_auc: 0.8125 - val_loss: 0.6851 - learning_rate: 6.2500e-04\n",
      "Epoch 240/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9339 - auc: 0.9882 - loss: 0.2046\n",
      "Epoch 240: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.9350 - auc: 0.9883 - loss: 0.2048 - val_acc: 0.8519 - val_auc: 0.8131 - val_loss: 0.6841 - learning_rate: 6.2500e-04\n",
      "Epoch 241/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9577 - auc: 0.9931 - loss: 0.1767 \n",
      "Epoch 241: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9521 - auc: 0.9906 - loss: 0.1943 - val_acc: 0.8519 - val_auc: 0.8119 - val_loss: 0.6872 - learning_rate: 6.2500e-04\n",
      "Epoch 242/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - acc: 0.9295 - auc: 0.9842 - loss: 0.2304\n",
      "Epoch 242: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - acc: 0.9338 - auc: 0.9856 - loss: 0.2222 - val_acc: 0.8519 - val_auc: 0.8120 - val_loss: 0.6875 - learning_rate: 6.2500e-04\n",
      "Epoch 243/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9382 - auc: 0.9879 - loss: 0.2172 \n",
      "Epoch 243: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9392 - auc: 0.9881 - loss: 0.2152 - val_acc: 0.8519 - val_auc: 0.8132 - val_loss: 0.6871 - learning_rate: 6.2500e-04\n",
      "Epoch 244/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9343 - auc: 0.9857 - loss: 0.2314\n",
      "Epoch 244: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9355 - auc: 0.9861 - loss: 0.2247 - val_acc: 0.8519 - val_auc: 0.8123 - val_loss: 0.6873 - learning_rate: 6.2500e-04\n",
      "Epoch 245/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9465 - auc: 0.9805 - loss: 0.2507 \n",
      "Epoch 245: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9451 - auc: 0.9834 - loss: 0.2346 - val_acc: 0.8544 - val_auc: 0.8121 - val_loss: 0.6879 - learning_rate: 6.2500e-04\n",
      "Epoch 246/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9226 - auc: 0.9865 - loss: 0.2288 \n",
      "Epoch 246: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - acc: 0.9277 - auc: 0.9868 - loss: 0.2239 - val_acc: 0.8519 - val_auc: 0.8116 - val_loss: 0.6905 - learning_rate: 6.2500e-04\n",
      "Epoch 247/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9349 - auc: 0.9837 - loss: 0.2177 \n",
      "Epoch 247: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9365 - auc: 0.9846 - loss: 0.2178 - val_acc: 0.8519 - val_auc: 0.8114 - val_loss: 0.6891 - learning_rate: 6.2500e-04\n",
      "Epoch 248/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9425 - auc: 0.9866 - loss: 0.2044\n",
      "Epoch 248: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9414 - auc: 0.9866 - loss: 0.2064 - val_acc: 0.8519 - val_auc: 0.8112 - val_loss: 0.6889 - learning_rate: 6.2500e-04\n",
      "Epoch 249/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9421 - auc: 0.9860 - loss: 0.2255 \n",
      "Epoch 249: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9416 - auc: 0.9863 - loss: 0.2212 - val_acc: 0.8519 - val_auc: 0.8131 - val_loss: 0.6833 - learning_rate: 6.2500e-04\n",
      "Epoch 250/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9512 - auc: 0.9909 - loss: 0.1796 \n",
      "Epoch 250: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9490 - auc: 0.9903 - loss: 0.1867 - val_acc: 0.8519 - val_auc: 0.8135 - val_loss: 0.6835 - learning_rate: 6.2500e-04\n",
      "Epoch 251/500\n",
      "\u001b[1m 9/17\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9532 - auc: 0.9907 - loss: 0.1915 \n",
      "Epoch 251: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - acc: 0.9511 - auc: 0.9897 - loss: 0.1977 - val_acc: 0.8544 - val_auc: 0.8117 - val_loss: 0.6869 - learning_rate: 6.2500e-04\n",
      "Epoch 252/500\n",
      "\u001b[1m11/17\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9536 - auc: 0.9897 - loss: 0.2033 \n",
      "Epoch 252: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9485 - auc: 0.9887 - loss: 0.2082 - val_acc: 0.8544 - val_auc: 0.8125 - val_loss: 0.6878 - learning_rate: 6.2500e-04\n",
      "Epoch 253/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9414 - auc: 0.9918 - loss: 0.1850 \n",
      "Epoch 253: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9412 - auc: 0.9911 - loss: 0.1912 - val_acc: 0.8519 - val_auc: 0.8130 - val_loss: 0.6880 - learning_rate: 6.2500e-04\n",
      "Epoch 254/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9484 - auc: 0.9892 - loss: 0.2138 \n",
      "Epoch 254: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9460 - auc: 0.9886 - loss: 0.2148 - val_acc: 0.8519 - val_auc: 0.8129 - val_loss: 0.6890 - learning_rate: 6.2500e-04\n",
      "Epoch 255/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9434 - auc: 0.9875 - loss: 0.2176 \n",
      "Epoch 255: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9425 - auc: 0.9873 - loss: 0.2183 - val_acc: 0.8544 - val_auc: 0.8126 - val_loss: 0.6883 - learning_rate: 6.2500e-04\n",
      "Epoch 256/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - acc: 0.9312 - auc: 0.9889 - loss: 0.2033 \n",
      "Epoch 256: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - acc: 0.9334 - auc: 0.9885 - loss: 0.2073 - val_acc: 0.8544 - val_auc: 0.8125 - val_loss: 0.6894 - learning_rate: 6.2500e-04\n",
      "Epoch 257/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9253 - auc: 0.9817 - loss: 0.2348 \n",
      "Epoch 257: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9300 - auc: 0.9834 - loss: 0.2290 - val_acc: 0.8544 - val_auc: 0.8116 - val_loss: 0.6918 - learning_rate: 6.2500e-04\n",
      "Epoch 258/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9415 - auc: 0.9907 - loss: 0.1918 \n",
      "Epoch 258: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9411 - auc: 0.9899 - loss: 0.1977 - val_acc: 0.8544 - val_auc: 0.8122 - val_loss: 0.6914 - learning_rate: 6.2500e-04\n",
      "Epoch 259/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9335 - auc: 0.9894 - loss: 0.1965 \n",
      "Epoch 259: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9350 - auc: 0.9891 - loss: 0.1990 - val_acc: 0.8544 - val_auc: 0.8130 - val_loss: 0.6878 - learning_rate: 6.2500e-04\n",
      "Epoch 260/500\n",
      "\u001b[1m14/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9311 - auc: 0.9831 - loss: 0.2434 \n",
      "Epoch 260: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9333 - auc: 0.9840 - loss: 0.2376 - val_acc: 0.8544 - val_auc: 0.8131 - val_loss: 0.6883 - learning_rate: 6.2500e-04\n",
      "Epoch 261/500\n",
      "\u001b[1m15/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9438 - auc: 0.9906 - loss: 0.1782 \n",
      "Epoch 261: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9434 - auc: 0.9900 - loss: 0.1836 - val_acc: 0.8519 - val_auc: 0.8141 - val_loss: 0.6880 - learning_rate: 6.2500e-04\n",
      "Epoch 262/500\n",
      "\u001b[1m12/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - acc: 0.9433 - auc: 0.9901 - loss: 0.1935 \n",
      "Epoch 262: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - acc: 0.9420 - auc: 0.9892 - loss: 0.2006 - val_acc: 0.8544 - val_auc: 0.8125 - val_loss: 0.6913 - learning_rate: 6.2500e-04\n",
      "Epoch 263/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9362 - auc: 0.9900 - loss: 0.1863 \n",
      "Epoch 263: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9375 - auc: 0.9893 - loss: 0.1926 - val_acc: 0.8544 - val_auc: 0.8122 - val_loss: 0.6931 - learning_rate: 6.2500e-04\n",
      "Epoch 264/500\n",
      "\u001b[1m13/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - acc: 0.9362 - auc: 0.9846 - loss: 0.2396 \n",
      "Epoch 264: val_auc did not improve from 0.85910\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - acc: 0.9376 - auc: 0.9853 - loss: 0.2323 - val_acc: 0.8544 - val_auc: 0.8124 - val_loss: 0.6918 - learning_rate: 6.2500e-04\n",
      "Epoch 265/500\n",
      "\u001b[1m10/17\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - acc: 0.9195 - auc: 0.9749 - loss: 0.2999 \n",
      "Epoch 265: val_auc did not improve from 0.85910\n",
      "\n",
      "Epoch 265: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - acc: 0.9293 - auc: 0.9799 - loss: 0.2631 - val_acc: 0.8544 - val_auc: 0.8123 - val_loss: 0.6928 - learning_rate: 6.2500e-04\n",
      "Epoch 265: early stopping\n",
      "Restoring model weights from the end of the best epoch: 65.\n"
     ]
    }
   ],
   "source": [
    "model_ensemble.compile(\n",
    "    # SGD may generalize better than Adam, especially on CNN\n",
    "    # https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer\n",
    "    optimizer = keras.optimizers.SGD(momentum = 0.9, nesterov = True),\n",
    "    # False if already using activation (e.g. sigmoid), true if no activation\n",
    "    # May call sigmoid activation internally if true\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "    metrics = [\n",
    "        keras.metrics.BinaryAccuracy(name = 'acc'),\n",
    "        # Often used in binary classification since accuracy is a flawed metric\n",
    "        keras.metrics.AUC(name = 'auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "#  ====================\n",
    "\n",
    "# Reduce LR is usually not needed for Adam (but useful for SGD)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    factor = 0.5,\n",
    "    patience = 40,\n",
    "    min_lr = 0.0001,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    patience = 200,\n",
    "    min_delta = 0,\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    restore_best_weights = True,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "logger = keras.callbacks.CSVLogger(\n",
    "    filename = 'model/model_training_hist.csv',\n",
    "    append = False\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = f'model/model_ensemble.keras',\n",
    "    monitor = 'val_auc',\n",
    "    mode = 'max',\n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "history = model_ensemble.fit(\n",
    "    x_train,\n",
    "    y_train.values,\n",
    "    epochs = 500,\n",
    "    # Bigger size to include more minority class for the model to learn\n",
    "    batch_size = 64,\n",
    "    class_weight = class_weights,\n",
    "    validation_data = (x_test, y_test),\n",
    "    callbacks = [checkpoint, logger, reduce_lr, early_stop],\n",
    ")\n",
    "\n",
    "# Save also individual models that composed ensemble model\n",
    "for i, model in enumerate([model1, model2, model3]):\n",
    "    model.save(f'model/model{i + 1}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Visualize layer output to debug model performance (see [reference](https://www.kaggle.com/code/kirillka95/keras-nn-layer-activations-ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TotalWorkingYearsN', 'OverTimeN', 'JobRoleN', 'MonthlyIncomeN', 'AgeN',\n",
       "       'JobLevelN', 'YearsAtCompanyN', 'StockOptionLevelN', 'MaritalStatusN',\n",
       "       'YearsInCurrentRoleN', 'YearsWithCurrManagerN', 'JobInvolvementN',\n",
       "       'BusinessTravelN', 'EnvironmentSatisfactionN', 'DistanceFromHomeN',\n",
       "       'JobSatisfactionN', 'WorkLifeBalanceN', 'EducationFieldN',\n",
       "       'TrainingTimesLastYearN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeId</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>Pred_Attrition</th>\n",
       "      <th>Pred_Percent</th>\n",
       "      <th>Pred_IsTrue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.978718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.085854</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.966809</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.330511</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>946</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.474686</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>548</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.126595</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>1104</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80.166626</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>95.934441</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34.129616</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>698</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99.989555</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EmployeeId  Attrition  Pred_Attrition  Pred_Percent  Pred_IsTrue\n",
       "0              1          0               0     25.978718         True\n",
       "953          954          0               0      1.085854         True\n",
       "952          953          0               0      8.966809         True\n",
       "949          950          0               0      0.330511         True\n",
       "945          946          0               0      0.474686         True\n",
       "...          ...        ...             ...           ...          ...\n",
       "547          548          1               0     27.126595        False\n",
       "1103        1104          1               1     80.166626         True\n",
       "540          541          1               1     95.934441         True\n",
       "864          865          1               0     34.129616        False\n",
       "697          698          1               1     99.989555         True\n",
       "\n",
       "[412 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_ensemble.predict(x_test)\n",
    "\n",
    "df_pred = y_test.copy()\n",
    "df_pred['EmployeeId'] = df.loc[df['IsTest'] == 1, 'EmployeeId']\n",
    "df_pred['Pred_Percent'] = y_pred * 100\n",
    "\n",
    "# If more than 50% chance, it's most likely attrition = true\n",
    "df_pred.loc[df_pred['Pred_Percent'] > 50, 'Pred_Attrition'] = 1\n",
    "df_pred.loc[df_pred['Pred_Percent'] <= 50, 'Pred_Attrition'] = 0\n",
    "\n",
    "# https://stackoverflow.com/a/71797755\n",
    "df_pred['Pred_IsTrue'] = df_pred['Attrition'] == df_pred['Pred_Attrition']\n",
    "\n",
    "df_pred['Pred_Attrition'] = df_pred['Pred_Attrition'].astype('int')\n",
    "df_pred = df_pred[['EmployeeId', 'Attrition', 'Pred_Attrition', 'Pred_Percent', 'Pred_IsTrue']]\n",
    "df_pred.sort_values('Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition\n",
      "0    354\n",
      "1     58\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Pred_IsTrue\n",
      "True     339\n",
      "False     73\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_pred['Attrition'].value_counts(), '\\n')\n",
    "print(df_pred['Pred_IsTrue'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check F1 score for both attrition 0 and 1 (usually score will be quite bad on minority class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.84      0.89       354\n",
      "           1       0.43      0.74      0.54        58\n",
      "\n",
      "    accuracy                           0.82       412\n",
      "   macro avg       0.69      0.79      0.72       412\n",
      "weighted avg       0.88      0.82      0.84       412\n",
      "\n",
      "Confusion matrix:\n",
      " [[296  58]\n",
      " [ 15  43]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_pred['Attrition'], df_pred['Pred_Attrition']))\n",
    "print('Confusion matrix:\\n', confusion_matrix(df_pred['Attrition'], df_pred['Pred_Attrition']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "Changelog:\n",
    "- v1: Initial\n",
    "- v2: Switch from decision tree to neural network. Around 80-90% accuracy but bad F1 score for attrition = true (0.51)\n",
    "- v3: Use SMOTE, F1 score varies but generally worse than without using it. Also, SMOTE random seed doesn't work correctly\n",
    "- v4: Remove SMOTE. Changed label encoder to target encoder, added feature selection using Pearson method. F1 score doesn't seem to improve much even after following this [reference](https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80) (reference has data leakage)\n",
    "- v5: Use model ensembling (3 NN models), F1 score is still the same but prediction is somewhat better (more true positives)\n",
    "- v6: Used label encoder again before uploading data to Supabase (it will still be encoded using target encoder, but in later steps)\n",
    "\n",
    "Reference and/or todo list:\n",
    "- https://www.kaggle.com/competitions/playground-series-s3e3 (main competition)\n",
    "- https://www.kaggle.com/competitions/cat-in-the-dat-ii (categorical feature encoding)\n",
    "- https://www.kaggle.com/code/tanmay111999/hr-analytics-data-leakage-eda-f1-score-80 (good EDA, recommended)\n",
    "- https://www.kaggle.com/code/kirillka95/keras-nn-layer-activations-ensemble (ensemble NN, NN visualization, recommended)\n",
    "- https://www.kaggle.com/code/robertobonilla/explained-100-advanced-classification-beginners (multiple normalization/pre-processing)\n",
    "- https://www.kaggle.com/code/ashishkumarak/employee-retention-workforce-reduction-prediction (calculating feature importance NN)\n",
    "- https://www.kaggle.com/code/samuelcortinhas/ps-s3e3-hill-climbing-like-a-gm (hill climbing)\n",
    "- https://www.kaggle.com/code/zuberrr/feature-engineering-w-xgboost-87-auc-in-10-mins (feature engineering)\n",
    "- https://www.kaggle.com/code/cv13j0/pss-3-episode-3-lama (feature engineering)\n",
    "- https://www.kaggle.com/code/kirillka95/ps-s03e03-eda-16-models-test-0-94 (ensemble + model test)\n",
    "- https://www.kaggle.com/code/chunweishen/ps-s03e03-ensembling (ensemble + feature importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
